{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeVectorStore():\n",
    "    # docs = load_docs(directory)\n",
    "    # #print(docs)\n",
    "    # chunk_size = 1000\n",
    "    # chunk_overlap = 100\n",
    "    # text_splitter = RecursiveCharacterTextSplitter(\n",
    "    #     chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    # )\n",
    "    # split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "    # vectorstore = PineconeVectorStore.from_documents(\n",
    "    #     split_docs, embeddings, index_name=index_name\n",
    "    # )\n",
    "    vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "docs = load_docs(directory)\n",
    "\n",
    "chunk_size = 200\n",
    "chunk_overlap = 20\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# vectorstore = PineconeVectorStore.from_documents(\n",
    "#     split_docs, embeddings, index_name=index_name\n",
    "# )\n",
    "#vectorstore = PineconeVectorStore(index_name=index_name, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# keyword = \"\"\" \"\"\"\n",
    "query = \" \"\n",
    "\n",
    "\n",
    "# retriever.get_relevant_documents(keyword)\n",
    "# retriever.invoke(keyword)\n",
    "# retriever\n",
    "\n",
    "\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.5)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "\n",
    "# qa = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     chain_type=\"stuff\",\n",
    "#     retriever=vectorstore.as_retriever(),\n",
    "#     return_source_documents=True,\n",
    "# )\n",
    "\n",
    "\n",
    "class PineconeConnectionPool:\n",
    "    def __init__(self, index_name: str):\n",
    "        self.index_name = index_name\n",
    "        self.index = pinecone.Index(\n",
    "            index_name, host=\"https://test-3-unx28qm.svc.aped-4627-b74a.pinecone.io\"\n",
    "        )\n",
    "\n",
    "    def get_index(self):\n",
    "        return self.index\n",
    "\n",
    "\n",
    "# Creating a connection pool for Pinecone\n",
    "connection_pool = PineconeConnectionPool(index_name=\"test-3\")\n",
    "\n",
    "\n",
    "class tenantId(BaseModel):\n",
    "    tenantId: str\n",
    "\n",
    "\n",
    "# retriever = vectorstore.similarity_search(\n",
    "#     query=\"Vitafy\",\n",
    "#     k=1,\n",
    "#     filter={\"tenantId\": tenantId}\n",
    "# )\n",
    "\n",
    "# retriever = vectorstore.as_retriever(\n",
    "#     search_kwargs={\"k\": 3},\n",
    "#     filter={\"tenantId\": tenantId},\n",
    "# )\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "template = \"\"\"You are an expert LLM assistant specialized in answering questions based solely on the information provided in the uploaded documents (PDF, DOCX, or TXT formats). Use only the information from the documents to respond accurately and clearly to each question.\n",
    "\n",
    "Guidelines:\n",
    "1. Avoid using outside knowledge or assumptions. Stick strictly to the content in the documents.\n",
    "2. If the answer is not found in the uploaded documents, state, \"The answer is not specifically mentioned in the provided documents.\"\n",
    "3. Avoid using outside knowledge or assumptions. Stick strictly to the content in the documents.\n",
    "4. Maintain a professional and helpful tone thinking you are giving service to the customer for their documents.\n",
    "5. Answer for normal conversation questions like \"Hi\", \"Hey\", \"Hello\", \"How are you?\", and many others with the answer: \"Hello, How can I assist you?\".\n",
    "6. If the question is on \"summarize\" or \"summarization\", then summarize the documents to (1/4)th the size of the original documents.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "custom_rag_template = PromptTemplate.from_template(template)\n",
    "# vectorstore.similarity_search(\n",
    "#     query, k=3  # our search query  # return 3 most relevant docs\n",
    "# )\n",
    "# retriever = vectorstore.similarity_search(\n",
    "#     search_kwargs={\"k\": 3, \"filter\": {\"tenantId\": tenantId}}\n",
    "# )\n",
    "# My_rag_chain = (\n",
    "#     RunnableParallel(\n",
    "#         {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "#     )\n",
    "#     | custom_rag_template\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "\n",
    "## My chain : Retriever(Pinecone) | custom_rag_template(prompt) | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "# $$$$$$$$$$ DOCS $$$$$$$$$\n",
    "@app.get(\"/api/artificial-intelligence\")\n",
    "async def redirect_root_to_docs():\n",
    "    return RedirectResponse(\"/docs\")\n",
    "\n",
    "\n",
    "# class tenantId(BaseModel):\n",
    "#     tenantId: str\n",
    "\n",
    "\n",
    "# def process_and_upsert_documents(\n",
    "#     ids, texts, metadata, tenantId, text_splitter, embeddings, index\n",
    "# ):\n",
    "#     vectors = []\n",
    "\n",
    "#     # Split into chunks, generate vectors, and add metadata\n",
    "#     for doc_id, text, meta in zip(ids, texts, metadata):\n",
    "#         chunks = text_splitter.split_text(text)  # Split document into chunks\n",
    "#         for i, chunk in enumerate(chunks):\n",
    "#             chunk_id = f\"{doc_id}#chunk{i+1}\"  # Create a unique ID for each chunk\n",
    "#             vector = embeddings.embed_documents([chunk])[\n",
    "#                 0\n",
    "#             ]  # Generate embeddings for each chunk\n",
    "\n",
    "#             # Prepare metadata with tenantId and fileName\n",
    "#             chunk_metadata = {\n",
    "#                 \"fileName\": meta[\"fileName\"],\n",
    "#                 \"tenantId\": tenantId,\n",
    "#             }\n",
    "\n",
    "#             # Preparing the vector for upsert\n",
    "#             vectors.append(\n",
    "#                 {\"id\": chunk_id, \"values\": vector, \"metadata\": chunk_metadata}\n",
    "#             )\n",
    "\n",
    "#     # Upsert vectors into Pinecone\n",
    "#     if vectors:\n",
    "#         index.upsert(vectors)\n",
    "\n",
    "#     # Structured list of documents with IDs and names for the response\n",
    "#     documents = [\n",
    "#         {\n",
    "#             \"embeddedId\": doc_id,\n",
    "#             \"fileName\": meta[\"fileName\"],\n",
    "#             \"tenantId\": tenantId,\n",
    "#         }\n",
    "#         for doc_id, meta in zip(ids, metadata)\n",
    "#     ]\n",
    "\n",
    "#     return documents\n",
    "\n",
    "\n",
    "# @app.post(\"/api/artificial-intelligence/upload\")\n",
    "# async def upload_files(\n",
    "#     tenantId: str = Form(...),\n",
    "#     files: List[UploadFile] = File(...),\n",
    "#     background_task: BackgroundTasks = BackgroundTasks(),\n",
    "# ):\n",
    "#     \"\"\"Upload multiple PDF, DOCX, and TXT files\"\"\"\n",
    "\n",
    "#     # Create a directory for the tenant if it doesn't exist\n",
    "#     tenant_directory = os.path.join(base_directory, tenantId)\n",
    "#     os.makedirs(tenant_directory, exist_ok=True)\n",
    "\n",
    "#     # Allowed file extensions\n",
    "#     allowed_extensions = {\".pdf\", \".docx\", \".txt\"}\n",
    "\n",
    "#     # Tracking file names to check for duplicates\n",
    "#     fileName = set()\n",
    "\n",
    "#     # Saving each uploaded file\n",
    "#     for file in files:\n",
    "#         if file.filename in fileName:\n",
    "#             raise HTTPException(\n",
    "#                 status_code=400, detail=f\"Duplicate file detected: {file.filename}\"\n",
    "#             )\n",
    "\n",
    "#         fileName.add(file.filename)\n",
    "\n",
    "#         # Check for file extension\n",
    "#         _, extension = os.path.splitext(file.filename)\n",
    "#         if extension.lower() not in allowed_extensions:\n",
    "#             raise HTTPException(\n",
    "#                 status_code=400,\n",
    "#                 detail=f\"Invalid file type: {file.filename}. Only PDF, DOCX, and TXT files are allowed.\",\n",
    "#             )\n",
    "\n",
    "#         # Define the destination path for the uploaded file\n",
    "#         destination = os.path.join(tenant_directory, file.filename)\n",
    "\n",
    "#         # Save the uploaded file to the tenant's directory\n",
    "#         with open(destination, \"wb\") as buffer:\n",
    "#             shutil.copyfileobj(file.file, buffer)\n",
    "\n",
    "#     # Load the documents from the tenant's directory\n",
    "#     docs = load_docs(tenant_directory)\n",
    "\n",
    "#     # Extract texts and metadata for each document\n",
    "#     texts = [\n",
    "#         doc.page_content for doc in docs\n",
    "#     ]  # Assuming docs have a page_content attribute\n",
    "#     metadata = [\n",
    "#         {\"fileName\": os.path.basename(doc.metadata[\"source\"]), \"tenantId\": tenantId}\n",
    "#         for doc in docs\n",
    "#     ]\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "#     # Generate unique IDs for each document\n",
    "#     ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "\n",
    "#     # Call the function to process and upsert the documents\n",
    "#     background_task.add_task(\n",
    "#         process_and_upsert_documents,\n",
    "#         ids,\n",
    "#         texts,\n",
    "#         metadata,\n",
    "#         tenantId,\n",
    "#         text_splitter,\n",
    "#         embeddings,\n",
    "#         index,\n",
    "#     )\n",
    "\n",
    "#     # Return JSON response with document details\n",
    "#     return JSONResponse(\n",
    "#         content={\"message\": \"Files uploaded successfully.\"},\n",
    "#         status_code=200,\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############### UPLOAD###########\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from uuid import uuid4\n",
    "from fastapi import BackgroundTasks, UploadFile, File, HTTPException, Form\n",
    "from fastapi.responses import JSONResponse\n",
    "import os\n",
    "import shutil\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "class tenantId(BaseModel):\n",
    "    tenantId: str\n",
    "\n",
    "\n",
    "def process_and_upsert_documents(\n",
    "    ids, texts, metadata, tenantId, text_splitter, embeddings, index\n",
    "):\n",
    "    vectors = []\n",
    "\n",
    "    # Split into chunks, generate vectors, and add metadata\n",
    "    for doc_id, text, meta in zip(ids, texts, metadata):\n",
    "        chunks = text_splitter.split_text(text)  # Split document into chunks\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_id = f\"{doc_id}#chunk{i+1}\"  # Create a unique ID for each chunk\n",
    "            vector = embeddings.embed_documents([chunk])[0]  # Generate embeddings for each chunk\n",
    "\n",
    "            # Prepare metadata with tenantId, fileName, source, and text\n",
    "            chunk_metadata = {\n",
    "                \"fileName\": meta[\"fileName\"],\n",
    "                \"source\": meta.get(\"source\", \"unknown\"),\n",
    "                \"tenantId\": tenantId,\n",
    "                \"text\": chunk  # Store the chunk text in metadata\n",
    "            }\n",
    "\n",
    "            # Preparing the vector for upsert\n",
    "            vectors.append(\n",
    "                {\"id\": chunk_id, \"values\": vector, \"metadata\": chunk_metadata}\n",
    "            )\n",
    "\n",
    "    # Upsert vectors into Pinecone\n",
    "    if vectors:\n",
    "        index.upsert(vectors)\n",
    "\n",
    "    # Structured list of documents with IDs and names for the response\n",
    "    documents = [\n",
    "        {\n",
    "            \"embeddedId\": doc_id,\n",
    "            \"fileName\": meta[\"fileName\"],\n",
    "            \"tenantId\": tenantId,\n",
    "        }\n",
    "        for doc_id, meta in zip(ids, metadata)\n",
    "    ]\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "@app.post(\"/api/artificial-intelligence/upload\")\n",
    "async def upload_files(\n",
    "    tenantId: str = Form(...),\n",
    "    files: List[UploadFile] = File(...),\n",
    "    background_task: BackgroundTasks = BackgroundTasks(),\n",
    "):\n",
    "    \"\"\"Upload multiple PDF, DOCX, and TXT files\"\"\"\n",
    "\n",
    "    # Create a directory for the tenant if it doesn't exist\n",
    "    tenant_directory = os.path.join(base_directory, tenantId)\n",
    "    os.makedirs(tenant_directory, exist_ok=True)\n",
    "\n",
    "    # Allowed file extensions\n",
    "    allowed_extensions = {\".pdf\", \".docx\", \".txt\"}\n",
    "\n",
    "    # Tracking file names to check for duplicates\n",
    "    fileName = set()\n",
    "\n",
    "    # Saving each uploaded file\n",
    "    for file in files:\n",
    "        if file.filename in fileName:\n",
    "            raise HTTPException(\n",
    "                status_code=400, detail=f\"Duplicate file detected: {file.filename}\"\n",
    "            )\n",
    "\n",
    "        fileName.add(file.filename)\n",
    "\n",
    "        # Check for file extension\n",
    "        _, extension = os.path.splitext(file.filename)\n",
    "        if extension.lower() not in allowed_extensions:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=f\"Invalid file type: {file.filename}. Only PDF, DOCX, and TXT files are allowed.\",\n",
    "            )\n",
    "\n",
    "        # Define the destination path for the uploaded file\n",
    "        destination = os.path.join(tenant_directory, file.filename)\n",
    "\n",
    "        # Save the uploaded file to the tenant's directory\n",
    "        with open(destination, \"wb\") as buffer:\n",
    "            shutil.copyfileobj(file.file, buffer)\n",
    "\n",
    "    # Load the documents from the tenant's directory\n",
    "    docs = load_docs(tenant_directory)\n",
    "\n",
    "    # Extract texts and metadata for each document\n",
    "    texts = [doc.page_content for doc in docs]  # Assuming docs have a page_content attribute\n",
    "    metadata = [\n",
    "        {\n",
    "            \"fileName\": os.path.basename(doc.metadata[\"source\"]),\n",
    "            \"source\": doc.metadata[\"source\"],\n",
    "            \"tenantId\": tenantId\n",
    "        }\n",
    "        for doc in docs\n",
    "    ]\n",
    "\n",
    "    # Generate unique IDs for each document\n",
    "    ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "\n",
    "    # Call the function to process and upsert the documents\n",
    "    background_task.add_task(\n",
    "        process_and_upsert_documents,\n",
    "        ids,\n",
    "        texts,\n",
    "        metadata,\n",
    "        tenantId,\n",
    "        text_splitter,\n",
    "        embeddings,\n",
    "        index,\n",
    "    )\n",
    "\n",
    "    # Return JSON response with document details\n",
    "    return JSONResponse(\n",
    "        content={\"message\": \"Files uploaded successfully.\"},\n",
    "        status_code=200,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@app.get(\"/api/artificial-intelligence/tenant_files\")\n",
    "async def get_tenant_files(tenantId: str = Query(...)):\n",
    "    \"\"\"Retrieve a list of uploaded files for a specific tenant\"\"\"\n",
    "\n",
    "    # Tenant's directory\n",
    "    tenant_directory = os.path.join(base_directory, tenantId)\n",
    "\n",
    "    #  if tenant's directory exists\n",
    "    if not os.path.exists(tenant_directory):\n",
    "        raise HTTPException(\n",
    "            status_code=404, detail=f\"No documents found for tenantId: {tenantId}\"\n",
    "        )\n",
    "\n",
    "    # all the files in the tenant's directory\n",
    "    uploaded_files = os.listdir(tenant_directory)\n",
    "\n",
    "    if not uploaded_files:\n",
    "        return {\"message\": f\"No files uploaded for tenantId: {tenantId}\"}\n",
    "\n",
    "    return {\"tenantId\": tenantId, \"uploaded_files\": uploaded_files}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def run_rag_chain(query, tenantId):\n",
    "#     # Perform a similarity search to retrieve relevant documents\n",
    "#     retrieved_docs = vectorstore.similarity_search(\n",
    "#         query, k=3, search_kwargs={\"filter\": {\"tenantId\": tenantId}}\n",
    "#     )\n",
    "    \n",
    "#     # Format the retrieved documents\n",
    "#     formatted_docs = format_docs(retrieved_docs)\n",
    "    \n",
    "#     # Prepare the context and question for the RAG chain\n",
    "#     context = formatted_docs\n",
    "#     question = query\n",
    "    \n",
    "#     # Run the RAG chain\n",
    "#     response = (\n",
    "#         RunnableParallel(\n",
    "#             {\"context\": context, \"question\": RunnablePassthrough()}\n",
    "#         )\n",
    "#         | custom_rag_template\n",
    "#         | llm\n",
    "#         | StrOutputParser()\n",
    "#     )\n",
    "    \n",
    "#     return response\n",
    "\n",
    "\n",
    "\n",
    "# @app.get(\"/api/artificial-intelligence/prompts\")\n",
    "# async def prompts_keyword(tenantId: str = Query(...), keyword: str = Query(...)):\n",
    "#     try:\n",
    "#         # Retrieving the tenant directory based on the tenantId\n",
    "#         tenant_directory = os.path.join(base_directory, tenantId)\n",
    "\n",
    "#         # Checking if the tenant directory exists\n",
    "#         if not os.path.exists(tenant_directory):\n",
    "#             raise HTTPException(\n",
    "#                 status_code=404,\n",
    "#                 detail=f\"No documents available for tenant ID {tenantId}.\",\n",
    "#             )\n",
    "\n",
    "#         # Setting up the retriever to filter documents based on tenantId\n",
    "#         retriever = vectorstore.as_retriever(\n",
    "#             search_kwargs={\"k\": 1},\n",
    "#             filter={\"tenantId\": tenantId},\n",
    "#         )\n",
    "\n",
    "#         # Retrieve relevant documents based on the keyword\n",
    "#         retrieved_docs = retriever.get_relevant_documents(keyword)\n",
    "\n",
    "#         # Formatting the retrieved documents into context text\n",
    "#         # context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "#         # Use the RAG (Retrieval-Augmented Generation) chain to extract answers\n",
    "#         # answer = qa.invoke(keyword)\n",
    "#         answer = qa.invoke({\"query\": keyword})\n",
    "\n",
    "#         # Extract relevant documents and metadata from the retrieved response\n",
    "#         source_documents = [\n",
    "#             {\n",
    "#                 # \"fileName\": doc.metadata.get(\"fileName\", \"unknown\"),\n",
    "#                 # \"tenantId\": doc.metadata.get(\"tenantId\", \"unknown\"),\n",
    "#                 # \"embeddedId\": doc.metadata.get(\"embeddedId\", \"unknown\"),\n",
    "#                 # \"content\": doc.page_content,\n",
    "#                 \"answer\": answer,\n",
    "#             }\n",
    "#             for doc in retrieved_docs\n",
    "#         ]\n",
    "\n",
    "#     except HTTPException as http_exc:\n",
    "#         # Handle the tenant-specific exception\n",
    "#         return {\"error\": http_exc.detail}\n",
    "\n",
    "#     except Exception as e:\n",
    "#         # Catch other exceptions and return the error message\n",
    "#         return {\"error\": str(e)}\n",
    "\n",
    "#     # Returning the source documents with metadata, content, and AI-generated answers\n",
    "#     # return {\"source_documents\": answer}\n",
    "#     return {\"answer\": answer}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################ CHAT ###############\n",
    "\n",
    "\n",
    "@app.get(\"/api/artificial-intelligence/prompts\")\n",
    "async def prompts_keyword(tenantId: str = Query(...), keyword: str = Query(...)):\n",
    "    try:\n",
    "        print(tenantId, keyword)\n",
    "        vectorStore = initializeVectorStore()\n",
    "        #retriever = vectorStore.as_retriever()\n",
    "        retriever = vectorStore.as_retriever(\n",
    "    \n",
    "    #search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 1,\n",
    "            \"filter\" : {\n",
    "        'tenantId': {'$eq': tenantId}  \n",
    "    \n",
    "    },\n",
    "            }\n",
    ")\n",
    "        newQa = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            #retriever=vectorStore.as_retriever(),\n",
    "            return_source_documents=True,\n",
    "        )\n",
    "\n",
    "        #answer = newQa.invoke({\"query\": keyword})\n",
    "        answer = newQa({\"query\": keyword})\n",
    "\n",
    "\n",
    "        return {\"answer\": answer}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############## DELETE ##############\n",
    "\n",
    "class DeleteRequest(BaseModel):\n",
    "    prefix: str\n",
    "\n",
    "@app.delete(\"/api/artificial-intelligence/delete\", summary=\"Delete documents by prefix\", description=\"Delete all documents in the Pinecone index that match the given prefix.\")\n",
    "async def delete_documents(request: DeleteRequest):\n",
    "    \"\"\"\n",
    "    Deletes all document IDs in the Pinecone index that start with the given prefix.\n",
    "\n",
    "    Parameters:\n",
    "    - **prefix**: The prefix string to filter and delete documents by.\n",
    "\n",
    "    Returns:\n",
    "    - A JSON response with a success message or an error if no IDs were found with the given prefix.\n",
    "    \"\"\"\n",
    "    prefix = request.prefix\n",
    "    # List all IDs with the given prefix\n",
    "    ids_to_delete = [id for id in index.list(prefix=prefix)]\n",
    "\n",
    "    if not ids_to_delete:\n",
    "        raise HTTPException(status_code=404, detail=\"No documents found with the given prefix.\")\n",
    "\n",
    "    # Delete the IDs from the index\n",
    "    index.delete(ids=ids_to_delete)\n",
    "    return {\"message\": f\"Deleted {len(ids_to_delete)} documents with prefix '{prefix}'.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################# Add Q&A #############################\n",
    "\n",
    "############################# Add Q&A #############################\n",
    "\n",
    "############################# Add Q&A #############################\n",
    "# @app.post(\"/api/artificial-intelligence/Add Q&A\")\n",
    "# async def upload_text(\n",
    "#     tenantId: str = Form(...),\n",
    "#     input_text: str = Form(...),\n",
    "#     background_task: BackgroundTasks = BackgroundTasks(),\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Upload and process raw text input and store it in Pinecone.\n",
    "#     \"\"\"\n",
    "#     embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "#     vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "\n",
    "#     chunk_size = 1000\n",
    "#     chunk_overlap = 20\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "#     split_docs = []\n",
    "#     chunk_ids = []\n",
    "\n",
    "#     uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "#     if not os.path.exists(uploaded_documents_path):\n",
    "#         with open(uploaded_documents_path, \"w\") as f:\n",
    "#             json.dump({}, f)\n",
    "\n",
    "#     with open(uploaded_documents_path, \"r\") as f:\n",
    "#         upload_documents = json.load(f)\n",
    "\n",
    "#     try:\n",
    "#         document_id = str(uuid.uuid4())\n",
    "\n",
    "#         metadata = {\n",
    "#             \"tenantId\": tenantId,\n",
    "#             \"fileName\": f\"text_input_{document_id}.txt\",\n",
    "#         }\n",
    "\n",
    "#         document = Document(page_content=input_text, metadata=metadata)\n",
    "\n",
    "#         curr_split_docs = text_splitter.split_documents([document])\n",
    "\n",
    "#         curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "\n",
    "#         split_docs += curr_split_docs\n",
    "#         chunk_ids += curr_chunk_ids\n",
    "\n",
    "#         upload_documents[document_id] = metadata\n",
    "\n",
    "#         vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "\n",
    "#         with open(uploaded_documents_path, \"w\") as f:\n",
    "#             json.dump(upload_documents, f, indent=4)\n",
    "\n",
    "#         return {\"status\": \"success\", \"message\": \"Text processed and stored successfully.\"}\n",
    "\n",
    "#     except Exception as e:\n",
    "#         raise HTTPException(status_code=500, detail=f\"Error processing the text: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langserve import add_routes\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "from fastapi import UploadFile, File, HTTPException\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from typing import Any\n",
    "import uuid\n",
    "import pylibmagic\n",
    "from fastapi import Form, BackgroundTasks\n",
    "from fastapi import Query\n",
    "import io\n",
    "import pinecone\n",
    "from PyPDF2 import PdfReader\n",
    "import docx\n",
    "import shutil\n",
    "import uvicorn\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.retrievers import PineconeHybridSearchRetriever\n",
    "from pinecone import ServerlessSpec\n",
    "# from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "# from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import Runnable\n",
    "from pydantic import BaseModel, Field, validator, ValidationError\n",
    "from tqdm.auto import tqdm\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv\n",
    "from fastapi.responses import RedirectResponse, JSONResponse\n",
    "import pinecone\n",
    "import glob\n",
    "from fastapi import FastAPI, File, UploadFile, HTTPException, Form\n",
    "from fastapi.responses import JSONResponse\n",
    "from uuid import uuid4\n",
    "import time\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi import FastAPI, BackgroundTasks, UploadFile, File, Form, HTTPException\n",
    "import getpass\n",
    "import os\n",
    "import concurrent.futures\n",
    "from fastapi import FastAPI, UploadFile, File, Form\n",
    "from fastapi.responses import JSONResponse\n",
    "from PyPDF2 import PdfReader  # For reading PDF files\n",
    "import docx\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "\n",
    "class MyModel(BaseModel):\n",
    "    message: BaseMessage\n",
    "\n",
    "\n",
    "class Config:\n",
    "    arbitrary_types_allowed = True\n",
    "\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"o1\",\n",
    "    description=\"\",\n",
    ")\n",
    "# Set all CORS enabled origins\n",
    "origins = [\"*\"]\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=origins,\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def add_process_time_header(request: Request, call_next):\n",
    "    start_time = time.perf_counter()\n",
    "    response = await call_next(request)\n",
    "    process_time = time.perf_counter() - start_time\n",
    "    response.headers[\"Processing-Time\"] = str(process_time)\n",
    "    return response\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "directory = os.getenv(\"directory\")\n",
    "base_directory = os.getenv(\"base_directory\")\n",
    "\n",
    "\n",
    "if openai_api_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "if pinecone_api_key:\n",
    "    os.environ[\"PINECONE_API_KEY\"] = pinecone_api_key\n",
    "\n",
    "# Verify that the keys are loaded\n",
    "# print(f\"OpenAI API Key: {os.environ.get('OPENAI_API_KEY')}\")\n",
    "# print(f\"Pinecone API Key: {os.environ.get('PINECONE_API_KEY')}\")\n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "import time\n",
    "\n",
    "index_name = \"test-3\"  # change if desired\n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=3072,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@app.post(\"/api/artificial-intelligence/upload\")\n",
    "async def upload_files(\n",
    "    tenantId: str = Form(...),\n",
    "    files: List[UploadFile] = File(...),\n",
    "    background_task: BackgroundTasks = BackgroundTasks(),\n",
    "):\n",
    "    \"\"\"Upload multiple PDF, DOCX, and TXT files\"\"\"\n",
    "    dir_name = str(uuid4())\n",
    "    tenant_directory = os.path.join(base_directory, dir_name)\n",
    "    os.makedirs(tenant_directory, exist_ok=True)\n",
    "\n",
    "    # Allowed file extensions\n",
    "    allowed_extensions = {\".pdf\", \".docx\", \".txt\"}\n",
    "\n",
    "    # Tracking file names to check for duplicates\n",
    "    fileName = set()\n",
    "\n",
    "    # Saving each uploaded file\n",
    "    for file in files:\n",
    "        if file.filename in fileName:\n",
    "            raise HTTPException(\n",
    "                status_code=400, detail=f\"Duplicate file detected: {file.filename}\"\n",
    "            )\n",
    "\n",
    "        fileName.add(file.filename)\n",
    "\n",
    "        # Check for file extension\n",
    "        _, extension = os.path.splitext(file.filename)\n",
    "        if extension.lower() not in allowed_extensions:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=f\"Invalid file type: {file.filename}. Only PDF, DOCX, and TXT files are allowed.\",\n",
    "            )\n",
    "\n",
    "        # Define the destination path for the uploaded file\n",
    "        destination = os.path.join(tenant_directory, file.filename)\n",
    "\n",
    "        print('creating doc ' + destination)\n",
    "        # Save the uploaded file to the tenant's directory\n",
    "        with open(destination, \"wb\") as buffer:\n",
    "            shutil.copyfileobj(file.file, buffer)\n",
    "\n",
    "\n",
    "\n",
    "    # Load the documents from the tenant's directory\n",
    "    docs = load_docs(tenant_directory, tenantId)\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\", #response time is 9s  #infloat/e5-base-V2 has 3.53sec response time.\n",
    "    )\n",
    "    vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "\n",
    "    chunk_size = 5000 \n",
    "    chunk_overlap = 20\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    for doc in docs:\n",
    "        curr_split_docs = text_splitter.split_documents([doc])\n",
    "\n",
    "         # Generate a unique document ID\n",
    "        document_id = str(uuid4())\n",
    "\n",
    "        # Create unique IDs for each chunk with the document ID as a prefix\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "\n",
    "        split_docs = split_docs + curr_split_docs\n",
    "        chunk_ids = chunk_ids + curr_chunk_ids\n",
    "\n",
    "        upload_documents[document_id] =  {\"fileName\": doc.metadata['filename'], \"id\": document_id, \"tenantId\": tenantId}\n",
    "\n",
    "    # Add documents to vector store with unique chunk IDs\n",
    "    vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f)\n",
    "\n",
    "    shutil.rmtree(tenant_directory)\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "def load_docs(directory, tenantId):\n",
    "    loader = DirectoryLoader(directory)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc.metadata['tenantId'] = tenantId\n",
    "\n",
    "        # Extract the filename from the 'source' path\n",
    "        doc.metadata['filename'] = os.path.basename(doc.metadata['source'])\n",
    "        print(\"ooooooooo\" + doc.metadata['filename'])\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "@app.get(\"/api/artificial-intelligence/tenant_files\")\n",
    "async def retrieve_files(tenantId: str = Query(...)):\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    tenantFiles = []\n",
    "    for item in list(upload_documents.values()):\n",
    "        if item.get('tenantId') == tenantId:\n",
    "            tenantFiles.append(item)\n",
    "            \n",
    "    return {\"data\": tenantFiles}\n",
    "\n",
    "\n",
    "\n",
    "def initializeVectorStore():\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",                             #response time is 9s  #infloat/e5-base-V2 has 3.53sec response time.\n",
    "    )\n",
    "    vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    #model=\"gpt-4o\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "@app.get(\"/api/artificial-intelligence/prompts\")\n",
    "async def prompts_keyword(tenantId: str = Query(...), keyword: str = Query(...)):\n",
    "    try:\n",
    "        print(tenantId, keyword)\n",
    "        vectorStore = initializeVectorStore()\n",
    "        #retriever = vectorStore.as_retriever()\n",
    "        retriever = vectorStore.as_retriever(\n",
    "    \n",
    "    #search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 1,\n",
    "            \"filter\" : {\n",
    "        'tenantId': {'$eq': tenantId}  \n",
    "    \n",
    "    },\n",
    "            }\n",
    ")\n",
    "        newQa = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            #retriever=vectorStore.as_retriever(),\n",
    "            return_source_documents=True,\n",
    "        )\n",
    "\n",
    "        #answer = newQa.invoke({\"query\": keyword})\n",
    "        answer = newQa({\"query\": keyword})\n",
    "\n",
    "\n",
    "        return {\"answer\": answer}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DeleteRequest(BaseModel):\n",
    "    prefix: str\n",
    "@app.delete(\"/api/artificial-intelligence/delete\", summary=\"Delete documents by prefix\", description=\"Delete all documents in the Pinecone index that match the given prefix.\")\n",
    "async def delete_documents(request: DeleteRequest):\n",
    "    \"\"\"\n",
    "    Deletes all document IDs in the Pinecone index that start with the given prefix.\n",
    "\n",
    "    Parameters:\n",
    "    - **prefix**: The prefix string to filter and delete documents by.\n",
    "\n",
    "    Returns:\n",
    "    - A JSON response with a success message or an error if no IDs were found with the given prefix.\n",
    "    \"\"\"\n",
    "    prefix = request.prefix\n",
    "    # List all IDs with the given prefix\n",
    "    ids_to_delete = [id for id in index.list(prefix=prefix)]\n",
    "    \n",
    "    print(ids_to_delete)\n",
    "\n",
    "    if not ids_to_delete:\n",
    "        raise HTTPException(status_code=404, detail=\"No documents found with the given prefix.\")\n",
    "\n",
    "    # Delete the IDs from the index\n",
    "    index.delete(ids=ids_to_delete)\n",
    "    return {\"message\": f\"Deleted {len(ids_to_delete)} documents with prefix '{prefix}'.\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeVectorStore():\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",                            \n",
    "    )\n",
    "    vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    #model=\"gpt-4o\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.2,\n",
    ")\n",
    "@app.get(\"/api/artificial-intelligence/prompts\")\n",
    "async def prompts_keyword(tenantId: str = Query(...), keyword: str = Query(...)):\n",
    "    try:\n",
    "        print(tenantId, keyword)\n",
    "        vectorStore = initializeVectorStore()\n",
    "        #retriever = vectorStore.as_retriever()\n",
    "        retriever = vectorStore.as_retriever(\n",
    "    \n",
    "    #search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 1,\n",
    "            \"filter\" : {\n",
    "        'tenantId': {'$eq': tenantId}  \n",
    "    \n",
    "    },\n",
    "            }\n",
    ")\n",
    "        newQa = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            #retriever=vectorStore.as_retriever(),\n",
    "            return_source_documents=True,\n",
    "        )\n",
    "\n",
    "        answer = newQa({\"query\": keyword})\n",
    "\n",
    "    #For JSON Output\n",
    "    #     return {\"answer\": answer}\n",
    "    # except Exception as e:\n",
    "    #     return {\"error\": str(e)}\n",
    "    \n",
    "    #For Plain Text Output\n",
    "        return PlainTextResponse(answer[\"result\"]) \n",
    "\n",
    "    except Exception as e:\n",
    "        return PlainTextResponse(\n",
    "            f\"Error: {str(e)}. Please contact the support team for assistance.\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "#from langserve import add_routes\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "from fastapi import UploadFile, File, HTTPException\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from typing import Any\n",
    "import uuid\n",
    "import pylibmagic\n",
    "from fastapi import Form, BackgroundTasks\n",
    "from fastapi import Query\n",
    "import io\n",
    "import pinecone\n",
    "from PyPDF2 import PdfReader\n",
    "import docx\n",
    "import shutil\n",
    "import uvicorn\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.retrievers import PineconeHybridSearchRetriever\n",
    "from fastapi.responses import PlainTextResponse\n",
    "from pinecone import ServerlessSpec\n",
    "# from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "# from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import Runnable\n",
    "from pydantic import BaseModel, Field, validator, ValidationError\n",
    "from tqdm.auto import tqdm\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv\n",
    "from fastapi.responses import RedirectResponse, JSONResponse\n",
    "import pinecone\n",
    "import glob\n",
    "from fastapi import FastAPI, File, UploadFile, HTTPException, Form\n",
    "from fastapi.responses import JSONResponse\n",
    "from uuid import uuid4\n",
    "import time\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi import FastAPI, BackgroundTasks, UploadFile, File, Form, HTTPException\n",
    "import getpass\n",
    "import os\n",
    "import concurrent.futures\n",
    "from fastapi import FastAPI, UploadFile, File, Form\n",
    "from fastapi.responses import JSONResponse\n",
    "from PyPDF2 import PdfReader  # For reading PDF files\n",
    "import docx\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "\n",
    "class MyModel(BaseModel):\n",
    "    message: BaseMessage\n",
    "\n",
    "\n",
    "class Config:\n",
    "    arbitrary_types_allowed = True\n",
    "\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"o1\",\n",
    "    description=\"\",\n",
    ")\n",
    "# Set all CORS enabled origins\n",
    "origins = [\"*\"]\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=origins,\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def add_process_time_header(request: Request, call_next):\n",
    "    start_time = time.perf_counter()\n",
    "    response = await call_next(request)\n",
    "    process_time = time.perf_counter() - start_time\n",
    "    response.headers[\"Processing-Time\"] = str(process_time)\n",
    "    return response\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pinecone_index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if openai_api_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "if pinecone_api_key:\n",
    "    os.environ[\"PINECONE_API_KEY\"] = pinecone_api_key\n",
    "if pinecone_index_name:\n",
    "    os.environ[\"PINECONE_INDEX_NAME\"] = pinecone_index_name\n",
    "\n",
    "# Verify that the keys are loaded\n",
    "# print(f\"OpenAI API Key: {os.environ.get('OPENAI_API_KEY')}\")\n",
    "# print(f\"Pinecone API Key: {os.environ.get('PINECONE_API_KEY')}\")\n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "index_name = os.environ.get(\"PINECONE_INDEX_NAME\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "import time\n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=3072,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "@app.post(\"/api/artificial-intelligence/upload\")\n",
    "async def upload_files(\n",
    "    tenantId: str = Form(...),\n",
    "    files: List[UploadFile] = File(...),\n",
    "    background_task: BackgroundTasks = BackgroundTasks(),\n",
    "):\n",
    "    \"\"\"Upload multiple PDF, DOCX, and TXT files\"\"\"\n",
    "    dir_name = str(uuid4())\n",
    "\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "    # Allowed file extensions\n",
    "    allowed_extensions = {\".pdf\", \".docx\", \".txt\"}\n",
    "\n",
    "    # Tracking file names to check for duplicates\n",
    "    fileName = set()\n",
    "\n",
    "    # Saving each uploaded file\n",
    "    for file in files:\n",
    "        if file.filename in fileName:\n",
    "            raise HTTPException(\n",
    "                status_code=400, detail=f\"Duplicate file detected: {file.filename}\"\n",
    "            )\n",
    "\n",
    "        fileName.add(file.filename)\n",
    "\n",
    "        # Check for file extension\n",
    "        _, extension = os.path.splitext(file.filename)\n",
    "        if extension.lower() not in allowed_extensions:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=f\"Invalid file type: {file.filename}. Only PDF, DOCX, and TXT files are allowed.\",\n",
    "            )\n",
    "\n",
    "        # Define the destination path for the uploaded file\n",
    "        destination = os.path.join(dir_name, file.filename)\n",
    "\n",
    "        print('creating doc ' + destination)\n",
    "        # Save the uploaded file to the tenant's directory\n",
    "        with open(destination, \"wb\") as buffer:\n",
    "            shutil.copyfileobj(file.file, buffer)\n",
    "\n",
    "    # Load the documents from the tenant's directory\n",
    "    docs = load_docs(dir_name, tenantId)\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\", #response time is 9s  #infloat/e5-base-V2 has 3.53sec response time.\n",
    "    )\n",
    "    vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "\n",
    "    chunk_size = 1000 \n",
    "    chunk_overlap = 20\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    for doc in docs:\n",
    "        curr_split_docs = text_splitter.split_documents([doc])\n",
    "\n",
    "         # Generate a unique document ID\n",
    "        document_id = str(uuid4())\n",
    "\n",
    "        # Create unique IDs for each chunk with the document ID as a prefix\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "\n",
    "        split_docs = split_docs + curr_split_docs\n",
    "        chunk_ids = chunk_ids + curr_chunk_ids\n",
    "\n",
    "        upload_documents[document_id] =  {\"fileName\": doc.metadata['filename'], \"id\": document_id, \"tenantId\": tenantId}\n",
    "\n",
    "    # Add documents to vector store with unique chunk IDs\n",
    "    vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "\n",
    "    shutil.rmtree(dir_name)\n",
    "\n",
    "    return {\"status\": \"success\", \"message\": \"Files uploaded successfully.\"}\n",
    "\n",
    "def load_docs(directory, tenantId):\n",
    "    loader = DirectoryLoader(directory)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc.metadata['tenantId'] = tenantId\n",
    "\n",
    "        # Extract the filename from the 'source' path\n",
    "        doc.metadata['filename'] = os.path.basename(doc.metadata['source'])\n",
    "        print(\"ooooooooo\" + doc.metadata['filename'])\n",
    "\n",
    "    return docs\n",
    "\n",
    "############################# Web-URL #############################\n",
    "\n",
    "############################# Web-URL #############################\n",
    "\n",
    "############################# Web-URL #############################\n",
    "@app.post(\"/api/artificial-intelligence/links\")\n",
    "async def upload_web_urls(\n",
    "    tenantId: str = Form(...),\n",
    "    urls: List[str] = Form(...),\n",
    "):\n",
    "    \"\"\"\n",
    "    Upload and process web URLs, then store content in Pinecone.\n",
    "    \"\"\"\n",
    "    all_docs = []  \n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            loader = WebBaseLoader(url)\n",
    "            docs = loader.load()  \n",
    "            \n",
    "            for doc in docs:\n",
    "                doc.metadata[\"tenantId\"] = tenantId\n",
    "                doc.metadata[\"source\"] = url\n",
    "            all_docs.extend(docs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading URL {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not all_docs:\n",
    "        raise HTTPException(status_code=400, detail=\"No valid URLs provided or failed to fetch content.\")\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "\n",
    "    chunk_size = 1000\n",
    "    chunk_overlap = 20\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    for doc in all_docs:\n",
    "        curr_split_docs = text_splitter.split_documents([doc])\n",
    "\n",
    "        # Generate a unique document ID\n",
    "        document_id = str(uuid.uuid4())\n",
    "\n",
    "        # Create unique IDs for each chunk with the document ID as a prefix\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "\n",
    "        split_docs.extend(curr_split_docs)\n",
    "        chunk_ids.extend(curr_chunk_ids)\n",
    "\n",
    "        upload_documents[document_id] = {\"source\": doc.metadata[\"source\"], \"id\": document_id, \"tenantId\": tenantId}\n",
    "\n",
    "    # Add documents to vector store with unique chunk IDs\n",
    "    vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "\n",
    "    return {\"status\": \"success\", \"message\": \"URLs processed and stored successfully.\"}\n",
    "\n",
    "############################# qa #############################\n",
    "\n",
    "############################# qa #############################\n",
    "\n",
    "############################# qa #############################\n",
    "@app.post(\"/api/artificial-intelligence/qa\")\n",
    "async def upload_question_answer(\n",
    "    tenantId: str = Form(...),\n",
    "    question: List[str] = Form(...),\n",
    "    answer: List[str] = Form(...),\n",
    "    background_task: BackgroundTasks = BackgroundTasks(),\n",
    "):\n",
    "    \"\"\"\n",
    "    Upload and process a question and answer pair and store it in Pinecone.\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "    chunk_size = 1000\n",
    "    chunk_overlap = 20\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    try:\n",
    "        document_id = str(uuid.uuid4())\n",
    "\n",
    "        combined_text = f\"Question: {question}\\nAnswer: {answer}\"\n",
    "\n",
    "        #document_id = str(uuid.uuid4())\n",
    "\n",
    "        metadata = {\n",
    "            \"tenantId\": tenantId,\n",
    "            \"fileName\": f\"Q&A_input_{document_id}.txt\",\n",
    "            \"id\": document_id,\n",
    "            \"question\": question[0],  \n",
    "            \"answer\": answer[0], \n",
    "\n",
    "        }\n",
    "\n",
    "        document = Document(page_content=combined_text, metadata=metadata)\n",
    "\n",
    "        curr_split_docs = text_splitter.split_documents([document])\n",
    "\n",
    "\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "\n",
    "        split_docs += curr_split_docs\n",
    "        chunk_ids += curr_chunk_ids\n",
    "\n",
    "        upload_documents[document_id] = metadata\n",
    "        \n",
    "\n",
    "        # Add to Pinecone vector store\n",
    "        vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump(upload_documents, f, indent=4)\n",
    "\n",
    "        #return {\"status\": \"success\", \"message\": \"Question and Answer processed and stored successfully.\"}\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"Question and Answer processed  successfully.\",\n",
    "            \"id\": document_id,\n",
    "            \"question\": question[0],  \n",
    "            \"answer\": answer[0]       \n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error processing the question and answer: {str(e)}\")\n",
    "\n",
    "############################# edit-qa #############################\n",
    "\n",
    "############################# edit-qa #############################\n",
    "\n",
    "############################# edit-qa #############################\n",
    "@app.put(\"/api/artificial-intelligence/qa_edit\")\n",
    "async def edit_question_answer(\n",
    "    tenantId: str = Form(...),\n",
    "    embeddedId: str = Form(...),\n",
    "    question: List[str] = Form(...),\n",
    "    answer: List[str] = Form(...),\n",
    "    background_task: BackgroundTasks = BackgroundTasks(),\n",
    "):\n",
    "    \"\"\"\n",
    "    Edit a question and answer pair by replacing the content in the same embeddedId.\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "    chunk_size = 1000\n",
    "    chunk_overlap = 20\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    if embeddedId not in upload_documents:\n",
    "        raise HTTPException(status_code=404, detail=\"The embeddedId does not exist.\")\n",
    "\n",
    "    try:\n",
    "        # Delete existing documents with the prefix\n",
    "        ids_to_delete = [id for id in index.list(prefix=embeddedId)]\n",
    "        index.delete(ids=ids_to_delete)\n",
    "\n",
    "        # Update the content\n",
    "        combined_text = f\"Question: {question}\\nAnswer: {answer}\"\n",
    "\n",
    "        metadata = {\n",
    "            \"tenantId\": tenantId,\n",
    "            \"fileName\": f\"Q&A_input_{embeddedId}.txt\",\n",
    "            \"id\": embeddedId,\n",
    "            \"question\": question[0],\n",
    "            \"answer\": answer[0],\n",
    "        }\n",
    "\n",
    "        document = Document(page_content=combined_text, metadata=metadata)\n",
    "\n",
    "        curr_split_docs = text_splitter.split_documents([document])\n",
    "        curr_chunk_ids = [f\"{embeddedId}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "\n",
    "        # Add updated documents to Pinecone\n",
    "        vectorstore.add_documents(documents=curr_split_docs, ids=curr_chunk_ids)\n",
    "\n",
    "        # Update metadata file\n",
    "        upload_documents[embeddedId] = metadata\n",
    "\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump(upload_documents, f, indent=4)\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"Question and Answer edited successfully.\",\n",
    "            \"id\": embeddedId,\n",
    "            \"question\": question[0],\n",
    "            \"answer\": answer[0],\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error editing the question and answer: {str(e)}\")\n",
    "############################# tenant_files #############################\n",
    "\n",
    "############################# tenant_files #############################\n",
    "\n",
    "############################# tenant_files #############################\n",
    "\n",
    "@app.get(\"/api/artificial-intelligence/tenant_files\")\n",
    "async def retrieve_files(tenantId: str = Query(...)):\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    tenantFiles = []\n",
    "    for item in list(upload_documents.values()):\n",
    "        if item.get('tenantId') == tenantId:\n",
    "            tenantFiles.append(item)\n",
    "\n",
    "    return {\"data\": tenantFiles}\n",
    "\n",
    "############################# prompts #############################\n",
    "\n",
    "############################# prompts #############################\n",
    "\n",
    "############################# prompts #############################\n",
    "\n",
    "def initializeVectorStore():\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",                             #response time is 9s  #infloat/e5-base-V2 has 3.53sec response time.\n",
    "    )\n",
    "    vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    #model=\"gpt-4o\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "@app.get(\"/api/artificial-intelligence/prompts\")\n",
    "async def prompts_keyword(tenantId: str = Query(...), keyword: str = Query(...)):\n",
    "    try:\n",
    "        print(tenantId, keyword)\n",
    "        vectorStore = initializeVectorStore()\n",
    "        #retriever = vectorStore.as_retriever()\n",
    "        retriever = vectorStore.as_retriever(\n",
    "    \n",
    "    #search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 1,\n",
    "            \"filter\" : {\n",
    "        'tenantId': {'$eq': tenantId}  \n",
    "    \n",
    "    },\n",
    "            }\n",
    ")\n",
    "        newQa = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            #retriever=vectorStore.as_retriever(),\n",
    "            return_source_documents=False,\n",
    "        )\n",
    "\n",
    "        #answer = newQa.invoke({\"query\": keyword})\n",
    "        answer = newQa({\"query\": keyword})\n",
    "\n",
    "\n",
    "           #For JSON Output\n",
    "    #     return {\"answer\": answer}\n",
    "    # except Exception as e:\n",
    "    #     return {\"error\": str(e)}\n",
    "    \n",
    "    #For Plain Text Output\n",
    "        return PlainTextResponse(answer[\"result\"]) \n",
    "\n",
    "    except Exception as e:\n",
    "        return PlainTextResponse(\n",
    "            f\"Error: {str(e)}. Please contact the support team for assistance.\"\n",
    "        )\n",
    "\n",
    "\n",
    "####################################### Delete ##########################\n",
    "\n",
    "####################################### Delete ##########################\n",
    "\n",
    "####################################### Delete ##########################\n",
    "class DeleteRequest(BaseModel):\n",
    "    prefix: str\n",
    "@app.delete(\"/api/artificial-intelligence/delete\", summary=\"Delete documents by prefix\", description=\"Delete all documents in the Pinecone index that match the given prefix.\")\n",
    "async def delete_documents(request: DeleteRequest):\n",
    "    \"\"\"\n",
    "    Deletes all document IDs in the Pinecone index that start with the given prefix.\n",
    "\n",
    "    Parameters:\n",
    "    - **prefix**: The prefix string to filter and delete documents by.\n",
    "\n",
    "    Returns:\n",
    "    - A JSON response with a success message or an error if no IDs were found with the given prefix.\n",
    "    \"\"\"\n",
    "    prefix = request.prefix\n",
    "    # List all IDs with the given prefix\n",
    "    ids_to_delete = [id for id in index.list(prefix=prefix)]\n",
    "    \n",
    "    print(ids_to_delete)\n",
    "\n",
    "    if not ids_to_delete:\n",
    "        raise HTTPException(status_code=404, detail=\"No documents found with the given prefix.\")\n",
    "\n",
    "    # Delete the IDs from the index\n",
    "    index.delete(ids=ids_to_delete)\n",
    "\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    if prefix in upload_documents:\n",
    "        del upload_documents[prefix]\n",
    "    \n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "\n",
    "    return {\"message\": f\"Deleted {len(ids_to_delete)} documents with prefix '{prefix}'.\"}\n",
    "\n",
    "####################################### Delete qa ##########################\n",
    "\n",
    "####################################### Delete qa##########################\n",
    "\n",
    "####################################### Delete qa##########################\n",
    "\n",
    "class DeleteRequest(BaseModel):\n",
    "    prefix: str\n",
    "@app.delete(\"/api/artificial-intelligence/qa\", summary=\"Delete QA by prefix\", description=\"Delete all documents in the Pinecone index that match the given prefix.\")\n",
    "async def delete_documents(request: DeleteRequest):\n",
    "    \"\"\"\n",
    "    Deletes all QA IDs in the Pinecone index that start with the given prefix.\n",
    "\n",
    "    Parameters:\n",
    "    - **prefix**: The prefix string to filter and delete QA by.\n",
    "\n",
    "    Returns:\n",
    "    - A JSON response with a success message or an error if no IDs were found with the given prefix.\n",
    "    \"\"\"\n",
    "    prefix = request.prefix\n",
    "    # List all IDs with the given prefix\n",
    "    ids_to_delete = [id for id in index.list(prefix=prefix)]\n",
    "    \n",
    "    print(ids_to_delete)\n",
    "\n",
    "    if not ids_to_delete:\n",
    "        raise HTTPException(status_code=404, detail=\"No QA found with the given prefix.\")\n",
    "\n",
    "    # Delete the IDs from the index\n",
    "    index.delete(ids=ids_to_delete)\n",
    "\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    if prefix in upload_documents:\n",
    "        del upload_documents[prefix]\n",
    "    \n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "\n",
    "    return {\"message\": f\"Deleted {len(ids_to_delete)} QA with prefix '{prefix}'.\"}\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "#from langserve import add_routes\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "from fastapi import UploadFile, File, HTTPException\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from typing import Any\n",
    "import uuid\n",
    "import pylibmagic\n",
    "from fastapi import Query\n",
    "import io\n",
    "import docx\n",
    "import shutil\n",
    "import uvicorn\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.retrievers import PineconeHybridSearchRetriever\n",
    "from fastapi.responses import PlainTextResponse\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import Runnable\n",
    "from pydantic import BaseModel, Field, validator, ValidationError\n",
    "from tqdm.auto import tqdm\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv\n",
    "from fastapi.responses import RedirectResponse\n",
    "import glob\n",
    "from fastapi import  UploadFile\n",
    "from fastapi.responses import JSONResponse\n",
    "from uuid import uuid4\n",
    "import time\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi import  Request\n",
    "from fastapi import  BackgroundTasks, UploadFile, Form\n",
    "import getpass\n",
    "import os\n",
    "import concurrent.futures\n",
    "from fastapi import  UploadFile\n",
    "from PyPDF2 import PdfReader  \n",
    "import docx\n",
    "import json\n",
    "\n",
    "class MyModel(BaseModel):\n",
    "    message: BaseMessage\n",
    "class Config:\n",
    "    arbitrary_types_allowed = True\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"version:vac0.1\",\n",
    "    description=\"\")\n",
    "\n",
    "origins = [\"*\"]\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=origins,\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"])\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def add_process_time_header(request: Request, call_next):\n",
    "    start_time = time.perf_counter()\n",
    "    response = await call_next(request)\n",
    "    process_time = time.perf_counter() - start_time\n",
    "    response.headers[\"Processing-Time\"] = str(process_time)\n",
    "    return response\n",
    "\n",
    "\n",
    "######################### keys #############################\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pinecone_index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "if not openai_api_key:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set. Please set it in your environment.\")\n",
    "if not pinecone_api_key:\n",
    "    raise EnvironmentError(\"PINECONE_API_KEY is not set. Please set it in your environment.\")\n",
    "if not pinecone_index_name:\n",
    "    raise EnvironmentError(\"PINECONE_INDEX_NAME is not set. Please set it in your environment.\")\n",
    "if openai_api_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "if pinecone_api_key:\n",
    "    os.environ[\"PINECONE_API_KEY\"] = pinecone_api_key\n",
    "if pinecone_index_name:\n",
    "    os.environ[\"PINECONE_INDEX_NAME\"] = pinecone_index_name\n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "\n",
    "############### vectordatabase initializations ####################\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "pinecone_index_name = os.environ.get(\"PINECONE_INDEX_NAME\")\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "pinecone_client = Pinecone(api_key=pinecone_api_key)\n",
    "def initialize_pinecone_index(client: Pinecone, index_name: str):\n",
    "    existing_indexes = [index_info[\"name\"] for index_info in client.list_indexes()]\n",
    "    if index_name not in existing_indexes:\n",
    "        client.create_index(\n",
    "            name=index_name,\n",
    "            dimension=3072,\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))\n",
    "        while not client.describe_index(index_name).status[\"ready\"]:\n",
    "            time.sleep(1)\n",
    "    return client.Index(index_name)\n",
    "index = initialize_pinecone_index(pinecone_client, pinecone_index_name)\n",
    "\n",
    "\n",
    "############################# upload #############################\n",
    "@app.post(\"/api/artificial-intelligence/upload\")\n",
    "async def upload_files(\n",
    "    tenantId: str = Form(...),\n",
    "    files: List[UploadFile] = File(...),\n",
    "    background_task: BackgroundTasks = BackgroundTasks()):\n",
    "    \"\"\"\n",
    "    Upload and process multiple document files\n",
    "        PDF, DOCX, and TXT files\n",
    "    Args:\n",
    "        tenantId: Identifier for the tenant\n",
    "        files: List of files to upload\n",
    "        background_task: Background task handler\n",
    "    Returns:\n",
    "        JSON response indicating upload status\n",
    "    Raises:\n",
    "        HTTPException: For invalid files or duplicates\n",
    "    \"\"\"\n",
    "    dir_name = str(uuid4())\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    allowed_extensions = {\".pdf\", \".docx\", \".txt\"}\n",
    "    fileName = set()\n",
    "    for file in files:\n",
    "        if file.filename in fileName:\n",
    "            raise HTTPException(\n",
    "                status_code=400, detail=f\"Duplicate file detected: {file.filename}\")\n",
    "        fileName.add(file.filename)\n",
    "        _, extension = os.path.splitext(file.filename)\n",
    "        if extension.lower() not in allowed_extensions:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=f\"Invalid file type: {file.filename}. Only PDF, DOCX, and TXT files are allowed.\",)\n",
    "        destination = os.path.join(dir_name, file.filename)\n",
    "        print('creating doc ' + destination)\n",
    "        with open(destination, \"wb\") as buffer:\n",
    "            shutil.copyfileobj(file.file, buffer)\n",
    "    docs = load_docs(dir_name, tenantId)\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    chunk_size = 500\n",
    "    chunk_overlap = 50\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    for doc in docs:\n",
    "        curr_split_docs = text_splitter.split_documents([doc])\n",
    "        document_id = str(uuid4())\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "        split_docs = split_docs + curr_split_docs\n",
    "        chunk_ids = chunk_ids + curr_chunk_ids\n",
    "        upload_documents[document_id] =  {\"fileName\": doc.metadata['filename'], \"id\": document_id, \"tenantId\": tenantId}\n",
    "    vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    shutil.rmtree(dir_name)\n",
    "    return {\"status\": \"success\", \"message\": \"Files uploaded successfully.\"}\n",
    "def load_docs(directory, tenantId):\n",
    "    loader = DirectoryLoader(directory)\n",
    "    docs = loader.load()\n",
    "    for doc in docs:\n",
    "        doc.metadata['tenantId'] = tenantId\n",
    "        doc.metadata['filename'] = os.path.basename(doc.metadata['source'])\n",
    "        print(\"ooooooooo\" + doc.metadata['filename'])\n",
    "    return docs\n",
    "\n",
    "\n",
    "############################# links #############################\n",
    "@app.post(\"/api/artificial-intelligence/links\")\n",
    "async def upload_web_urls(\n",
    "    tenantId: str = Form(...),\n",
    "    urls: List[str] = Form(...)):\n",
    "    \"\"\"\n",
    "    Process and store web URL content\n",
    "    Args:\n",
    "        tenantId: Identifier for the tenant\n",
    "        urls: List of URLs to process   \n",
    "    Returns:\n",
    "        JSON response indicating processing status   \n",
    "    Raises:\n",
    "        HTTPException: When no valid URLs are provided\n",
    "    \"\"\"\n",
    "    all_docs = []  \n",
    "    for url in urls:\n",
    "        try:\n",
    "            loader = WebBaseLoader(url)\n",
    "            docs = loader.load()  \n",
    "            for doc in docs:\n",
    "                doc.metadata[\"tenantId\"] = tenantId\n",
    "                doc.metadata[\"source\"] = url\n",
    "            all_docs.extend(docs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading URL {url}: {e}\")\n",
    "            continue\n",
    "    if not all_docs:\n",
    "        raise HTTPException(status_code=400, detail=\"No valid URLs provided or failed to fetch content.\")\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "    chunk_size = 500\n",
    "    chunk_overlap = 50\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    for doc in all_docs:\n",
    "        curr_split_docs = text_splitter.split_documents([doc])\n",
    "        document_id = str(uuid.uuid4())\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "        split_docs.extend(curr_split_docs)\n",
    "        chunk_ids.extend(curr_chunk_ids)\n",
    "        upload_documents[document_id] = {\"source\": doc.metadata[\"source\"], \"id\": document_id, \"tenantId\": tenantId}\n",
    "    vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    return {\"status\": \"success\", \"message\": \"URLs processed and stored successfully.\"}\n",
    "\n",
    "\n",
    "############################# qa@upload #############################\n",
    "class QARequest(BaseModel):\n",
    "    tenantId: str\n",
    "    question: str\n",
    "    answer: str\n",
    "@app.post(\"/api/artificial-intelligence/qa\")\n",
    "async def upload_question_answer(\n",
    "    request: QARequest,\n",
    "    background_task: BackgroundTasks):\n",
    "    \"\"\"\n",
    "    Upload and store a question-answer pair\n",
    "    Args:\n",
    "        request: QA request containing question and answer\n",
    "        background_task: Background task handler    \n",
    "    Returns:\n",
    "        JSON response with QA details and status  \n",
    "    Raises:\n",
    "        HTTPException: For processing errors\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "    chunk_size = 500\n",
    "    chunk_overlap = 50\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    try:\n",
    "        document_id = str(uuid.uuid4())\n",
    "        combined_text = f\"Question: {request.question}\\nAnswer: {request.answer}\"\n",
    "        metadata = {\n",
    "            \"tenantId\": request.tenantId,\n",
    "            \"fileName\": f\"Q&A_input_{document_id}.txt\",\n",
    "            \"id\": document_id,\n",
    "            \"question\": request.question,  \n",
    "            \"answer\": request.answer}\n",
    "        document = Document(page_content=combined_text, metadata=metadata)\n",
    "        curr_split_docs = text_splitter.split_documents([document])\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "        split_docs += curr_split_docs\n",
    "        chunk_ids += curr_chunk_ids\n",
    "        upload_documents[document_id] = metadata\n",
    "        vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump(upload_documents, f, indent=4)\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"Question and Answer processed successfully.\",\n",
    "            \"id\": document_id,\n",
    "            \"question\": request.question,  \n",
    "            \"answer\": request.answer }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error processing the question and answer: {str(e)}\")\n",
    "    \n",
    "\n",
    "############################# qa@edit #############################\n",
    "class QAEditRequest(BaseModel):\n",
    "    #tenantId: str\n",
    "    embeddedId: str\n",
    "    question: str\n",
    "    answer: str\n",
    "@app.put(\"/api/artificial-intelligence/qa\")\n",
    "async def edit_question_answer(\n",
    "    request: QAEditRequest,  \n",
    "    background_task: BackgroundTasks = BackgroundTasks()):\n",
    "    \"\"\"\n",
    "    Edit a question and answer pair by replacing the content in the same embeddedId.\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "    chunk_size = 500\n",
    "    chunk_overlap = 50\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    if request.embeddedId not in upload_documents:\n",
    "        raise HTTPException(status_code=404, detail=\"The embeddedId does not exist.\")\n",
    "    try:\n",
    "        ids_to_delete = [id for id in index.list(prefix=request.embeddedId)]\n",
    "        index.delete(ids=ids_to_delete)\n",
    "        combined_text = f\"Question: {request.question}\\nAnswer: {request.answer}\"\n",
    "        metadata = {\n",
    "            #\"tenantId\": request.tenantId,\n",
    "            \"fileName\": f\"Q&A_input_{request.embeddedId}.txt\",\n",
    "            \"id\": request.embeddedId,\n",
    "            \"question\": request.question,\n",
    "            \"answer\": request.answer}\n",
    "        document = Document(page_content=combined_text, metadata=metadata)\n",
    "        curr_split_docs = text_splitter.split_documents([document])\n",
    "        curr_chunk_ids = [f\"{request.embeddedId}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "        vectorstore.add_documents(documents=curr_split_docs, ids=curr_chunk_ids)\n",
    "        upload_documents[request.embeddedId] = metadata\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump(upload_documents, f, indent=4)\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"Question and Answer edited successfully.\",\n",
    "            \"id\": request.embeddedId,\n",
    "            \"question\": request.question,\n",
    "            \"answer\": request.answer }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error editing the question and answer: {str(e)}\")\n",
    "\n",
    "\n",
    "####################################### qa@delete ##########################\n",
    "class DeleteRequest(BaseModel):\n",
    "    prefix: str\n",
    "@app.delete(\"/api/artificial-intelligence/qa\", summary=\"Delete Question Answer\", description=\"Delete all documents in the Pinecone index that match the given prefix.\")\n",
    "async def delete_documents(request: DeleteRequest):\n",
    "    \"\"\"Delete Question Answer matching prefix\n",
    "    Args:\n",
    "        request: Delete request containing prefix\n",
    "    Returns:\n",
    "        JSON response indicating deletion status\n",
    "    Raises:\n",
    "        HTTPException: When no matching documents found\n",
    "    \"\"\"\n",
    "    prefix = request.prefix\n",
    "    ids_to_delete = [id for id in index.list(prefix=prefix)]\n",
    "    print(ids_to_delete)\n",
    "    if not ids_to_delete:\n",
    "        raise HTTPException(status_code=404, detail=\"No QA found with the given prefix.\")\n",
    "    index.delete(ids=ids_to_delete)\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    if prefix in upload_documents:\n",
    "        del upload_documents[prefix]\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    return {\"message\": f\"Deleted {len(ids_to_delete)} QA with prefix '{prefix}'.\"}\n",
    "\n",
    "\n",
    "############################# tenant_files #############################\n",
    "# @app.get(\"/api/artificial-intelligence/tenant_files\")\n",
    "# async def retrieve_files(tenantId: str = Query(...)):\n",
    "#     uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "#     if not os.path.exists(uploaded_documents_path):\n",
    "#         with open(uploaded_documents_path, \"w\") as f:\n",
    "#             json.dump({}, f)\n",
    "#     with open(uploaded_documents_path, \"r\") as f:\n",
    "#         upload_documents = json.load(f)\n",
    "#     tenantFiles = []\n",
    "#     for item in list(upload_documents.values()):\n",
    "#         if item.get('tenantId') == tenantId:\n",
    "#             tenantFiles.append(item)\n",
    "#     return {\"data\": tenantFiles}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@app.get(\"/api/artificial-intelligence/tenant_files\")\n",
    "async def retrieve_files(tenantId: str = Query(...)):\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    tenantFiles = []\n",
    "    for item in list(upload_documents.values()):\n",
    "        # Only include items that have either 'fileName' (from /upload) \n",
    "        # or 'source' (from /links) but not from /qa\n",
    "        if (item.get('tenantId') == tenantId and \n",
    "            ('fileName' in item or 'source' in item) and \n",
    "            not item.get('fileName', '').startswith('Q&A_input_')):\n",
    "            tenantFiles.append(item)\n",
    "\n",
    "    return {\"data\": tenantFiles}\n",
    "\n",
    "\n",
    "############################# prompts #############################\n",
    "def initializeVectorStore():\n",
    "    \"\"\"Initialize and configure vector store with embeddings\n",
    "    Returns:\n",
    "        Configured PineconeVectorStore instance\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    return vectorstore\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0)\n",
    "@app.get(\"/api/artificial-intelligence/prompts\")\n",
    "async def prompts_keyword(tenantId: str = Query(...), keyword: str = Query(...)):\n",
    "    \"\"\"Process keyword prompts and retrieve answers\n",
    "    Args:\n",
    "        tenantId: Identifier for the tenant\n",
    "        keyword: Search keyword/prompt  \n",
    "    Returns:\n",
    "        PlainText response with answer or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(tenantId, keyword)\n",
    "        vectorStore = initializeVectorStore()\n",
    "        retriever = vectorStore.as_retriever(\n",
    "        search_kwargs={\n",
    "        \"k\": 1,\n",
    "            \"filter\" : {\n",
    "        'tenantId': {'$eq': tenantId} }} )\n",
    "        newQa = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            return_source_documents=False)\n",
    "        answer = newQa({\"query\": keyword})\n",
    "        return PlainTextResponse(answer[\"result\"]) \n",
    "    except Exception as e:\n",
    "        return PlainTextResponse(\n",
    "            f\"Error: {str(e)}. Please contact the support team for assistance.\")\n",
    "\n",
    "####################################### delete ##########################\n",
    "class DeleteRequest(BaseModel):\n",
    "    prefix: str\n",
    "@app.delete(\"/api/artificial-intelligence/delete\", summary=\"Delete Documents\", description=\"Delete all documents in the Pinecone index that match the given prefix.\")\n",
    "async def delete_documents(request: DeleteRequest):\n",
    "    \"\"\"Delete documents matching prefix\n",
    "    Args:\n",
    "        request: Delete request containing prefix\n",
    "    Returns:\n",
    "        JSON response indicating deletion status\n",
    "    Raises:\n",
    "        HTTPException: When no matching documents found\n",
    "    \"\"\"\n",
    "    prefix = request.prefix\n",
    "    ids_to_delete = [id for id in index.list(prefix=prefix)]\n",
    "    print(ids_to_delete)\n",
    "    if not ids_to_delete:\n",
    "        raise HTTPException(status_code=404, detail=\"No documents found with the given prefix.\")\n",
    "    index.delete(ids=ids_to_delete)\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    if prefix in upload_documents:\n",
    "        del upload_documents[prefix]\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    return {\"message\": f\"Deleted {len(ids_to_delete)} documents with prefix '{prefix}'.\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " I am a specialized QA chatbot designed to provide accurate responses based solely on the given context. \n",
    "        Please input your question, and I will respond with the corresponding answer from the provided context. \n",
    "        I will not utilize any general knowledge or external information, ensuring focused and relevant answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        If the user greets you, respond with the \"Hello! How can I assist you today?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting apify-client\n",
      "  Downloading apify_client-1.8.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: langchain-openai in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (0.2.10)\n",
      "Requirement already satisfied: langchain in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (0.3.9)\n",
      "Collecting apify-shared>=1.1.2 (from apify-client)\n",
      "  Downloading apify_shared-1.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from apify-client) (0.27.2)\n",
      "Collecting more_itertools>=10.0.0 (from apify-client)\n",
      "  Downloading more_itertools-10.5.0-py3-none-any.whl.metadata (36 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from langchain-openai) (0.3.21)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.54.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from langchain-openai) (1.54.4)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from langchain-openai) (0.8.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from langchain) (0.1.137)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.16.0)\n",
      "Requirement already satisfied: anyio in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from httpx>=0.25.0->apify-client) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from httpx>=0.25.0->apify-client) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from httpx>=0.25.0->apify-client) (1.0.6)\n",
      "Requirement already satisfied: idna in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from httpx>=0.25.0->apify-client) (3.10)\n",
      "Requirement already satisfied: sniffio in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from httpx>=0.25.0->apify-client) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx>=0.25.0->apify-client) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai) (4.12.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.10)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (0.6.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (4.66.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from anyio->httpx>=0.25.0->apify-client) (1.2.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Downloading apify_client-1.8.1-py3-none-any.whl (73 kB)\n",
      "Downloading apify_shared-1.2.1-py3-none-any.whl (12 kB)\n",
      "Downloading more_itertools-10.5.0-py3-none-any.whl (60 kB)\n",
      "Installing collected packages: more_itertools, apify-shared, apify-client\n",
      "Successfully installed apify-client-1.8.1 apify-shared-1.2.1 more_itertools-10.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install apify-client langchain-openai langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ApifyWrapper\n  Value error, Did not find apify_api_token, please add an environment variable `APIFY_API_TOKEN` which contains it, or pass `apify_api_token` as a named parameter. [type=value_error, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ApifyWrapper\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GoogleSearchAPIWrapper\n\u001b[0;32m----> 7\u001b[0m apify \u001b[38;5;241m=\u001b[39m \u001b[43mApifyWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Call the Actor to obtain text from the crawled webpages\u001b[39;00m\n\u001b[1;32m      9\u001b[0m loader \u001b[38;5;241m=\u001b[39m apify\u001b[38;5;241m.\u001b[39mcall_actor(\n\u001b[1;32m     10\u001b[0m     actor_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapify/website-content-crawler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     run_input\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartUrls\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/docs/integrations/chat/\u001b[39m\u001b[38;5;124m\"\u001b[39m}]},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     ),\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pydantic/main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ApifyWrapper\n  Value error, Did not find apify_api_token, please add an environment variable `APIFY_API_TOKEN` which contains it, or pass `apify_api_token` as a named parameter. [type=value_error, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error"
     ]
    }
   ],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_community.docstore.document import Document\n",
    "from langchain_community.utilities import ApifyWrapper\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "\n",
    "apify = ApifyWrapper()\n",
    "# Call the Actor to obtain text from the crawled webpages\n",
    "loader = apify.call_actor(\n",
    "    actor_id=\"apify/website-content-crawler\",\n",
    "    run_input={\"startUrls\": [{\"url\": \"/docs/integrations/chat/\"}]},\n",
    "    dataset_mapping_function=lambda item: Document(\n",
    "        page_content=item[\"text\"] or \"\", metadata={\"source\": item[\"url\"]}\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create a vector store based on the crawled data\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "\n",
    "# Query the vector store\n",
    "query = \"Are any OpenAI chat models integrated in LangChain?\"\n",
    "result = index.query(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "#from langserve import add_routes\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "from fastapi import UploadFile, File, HTTPException\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from typing import Any\n",
    "import uuid\n",
    "import pylibmagic\n",
    "from fastapi import Query\n",
    "import io\n",
    "import docx\n",
    "import shutil\n",
    "import uvicorn\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.retrievers import PineconeHybridSearchRetriever\n",
    "from fastapi.responses import PlainTextResponse\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import Runnable\n",
    "from pydantic import BaseModel, Field, validator, ValidationError\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from tqdm.auto import tqdm\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv\n",
    "from fastapi.responses import RedirectResponse\n",
    "import glob\n",
    "from fastapi import  UploadFile\n",
    "from fastapi.responses import JSONResponse\n",
    "from uuid import uuid4\n",
    "import time\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi import  Request\n",
    "from fastapi import  BackgroundTasks, UploadFile, Form\n",
    "import getpass\n",
    "import os\n",
    "import concurrent.futures\n",
    "from fastapi import  UploadFile\n",
    "from PyPDF2 import PdfReader  \n",
    "import docx\n",
    "import json\n",
    "\n",
    "class MyModel(BaseModel):\n",
    "    message: BaseMessage\n",
    "class Config:\n",
    "    arbitrary_types_allowed = True\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"version:vac0.1\",\n",
    "    description=\"\")\n",
    "\n",
    "origins = [\"*\"]\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=origins,\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"])\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def add_process_time_header(request: Request, call_next):\n",
    "    start_time = time.perf_counter()\n",
    "    response = await call_next(request)\n",
    "    process_time = time.perf_counter() - start_time\n",
    "    response.headers[\"Processing-Time\"] = str(process_time)\n",
    "    return response\n",
    "\n",
    "######################### keys #############################\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pinecone_index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "if not openai_api_key:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set. Please set it in your environment.\")\n",
    "if not pinecone_api_key:\n",
    "    raise EnvironmentError(\"PINECONE_API_KEY is not set. Please set it in your environment.\")\n",
    "if not pinecone_index_name:\n",
    "    raise EnvironmentError(\"PINECONE_INDEX_NAME is not set. Please set it in your environment.\")\n",
    "if openai_api_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "if pinecone_api_key:\n",
    "    os.environ[\"PINECONE_API_KEY\"] = pinecone_api_key\n",
    "if pinecone_index_name:\n",
    "    os.environ[\"PINECONE_INDEX_NAME\"] = pinecone_index_name\n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "############### vectordatabase initializations ####################\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "pinecone_index_name = os.environ.get(\"PINECONE_INDEX_NAME\")\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "pinecone_client = Pinecone(api_key=pinecone_api_key)\n",
    "def initialize_pinecone_index(client: Pinecone, index_name: str):\n",
    "    existing_indexes = [index_info[\"name\"] for index_info in client.list_indexes()]\n",
    "    if index_name not in existing_indexes:\n",
    "        client.create_index(\n",
    "            name=index_name,\n",
    "            dimension=3072,\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))\n",
    "        while not client.describe_index(index_name).status[\"ready\"]:\n",
    "            time.sleep(1)\n",
    "    return client.Index(index_name)\n",
    "index = initialize_pinecone_index(pinecone_client, pinecone_index_name)\n",
    "\n",
    "############################# upload #############################\n",
    "@app.post(\"/api/artificial-intelligence/upload\")\n",
    "async def upload_files(\n",
    "    tenantId: str = Form(...),\n",
    "    files: List[UploadFile] = File(...),\n",
    "    background_task: BackgroundTasks = BackgroundTasks()):\n",
    "    \"\"\"\n",
    "    Upload and process multiple document files\n",
    "        PDF, DOCX, and TXT files\n",
    "    Args:\n",
    "        tenantId: Identifier for the tenant\n",
    "        files: List of files to upload\n",
    "        background_task: Background task handler\n",
    "    Returns:\n",
    "        JSON response indicating upload status\n",
    "    Raises:\n",
    "        HTTPException: For invalid files or duplicates\n",
    "    \"\"\"\n",
    "    dir_name = str(uuid4())\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    allowed_extensions = {\".pdf\", \".docx\", \".txt\"}\n",
    "    fileName = set()\n",
    "    for file in files:\n",
    "        if file.filename in fileName:\n",
    "            raise HTTPException(\n",
    "                status_code=400, detail=f\"Duplicate file detected: {file.filename}\")\n",
    "        fileName.add(file.filename)\n",
    "        _, extension = os.path.splitext(file.filename)\n",
    "        if extension.lower() not in allowed_extensions:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=f\"Invalid file type: {file.filename}. Only PDF, DOCX, and TXT files are allowed.\",)\n",
    "        destination = os.path.join(dir_name, file.filename)\n",
    "        print('creating doc ' + destination)\n",
    "        with open(destination, \"wb\") as buffer:\n",
    "            shutil.copyfileobj(file.file, buffer)\n",
    "    docs = load_docs(dir_name, tenantId)\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    chunk_size = 800\n",
    "    chunk_overlap = 150\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    for doc in docs:\n",
    "        curr_split_docs = text_splitter.split_documents([doc])\n",
    "        document_id = str(uuid4())\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "        split_docs = split_docs + curr_split_docs\n",
    "        chunk_ids = chunk_ids + curr_chunk_ids\n",
    "        upload_documents[document_id] =  {\"fileName\": doc.metadata['filename'], \"id\": document_id, \"tenantId\": tenantId}\n",
    "    vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    shutil.rmtree(dir_name)\n",
    "    return {\"status\": \"success\", \"message\": \"Files uploaded successfully.\"}\n",
    "def load_docs(directory, tenantId):\n",
    "    loader = DirectoryLoader(directory)\n",
    "    docs = loader.load()\n",
    "    for doc in docs:\n",
    "        doc.metadata['tenantId'] = tenantId\n",
    "        doc.metadata['filename'] = os.path.basename(doc.metadata['source'])\n",
    "        print(\"ooooooooo\" + doc.metadata['filename'])\n",
    "    return docs\n",
    "\n",
    "############################# links@upload #############################\n",
    "class UploadLinkRequest(BaseModel):\n",
    "    tenantId: str\n",
    "    url: str \n",
    "@app.post(\"/api/artificial-intelligence/links\")\n",
    "async def upload_web_url(request: UploadLinkRequest, background_tasks: BackgroundTasks):\n",
    "\n",
    "    \"\"\"\n",
    "    Process and store web URL content.\n",
    "    \n",
    "    Args:\n",
    "        request: UploadLinkRequest containing tenantId and url.\n",
    "    \n",
    "    Returns:\n",
    "        JSON response indicating processing status.\n",
    "    \n",
    "    Raises:\n",
    "        HTTPException: When no valid url are provided.\n",
    "    \"\"\"\n",
    "    all_docs = []  \n",
    "    url = request.url.split(\",\")  \n",
    "    for url in url:\n",
    "        try:\n",
    "            loader = WebBaseLoader(url.strip())  \n",
    "            docs = loader.load()  \n",
    "            for doc in docs:\n",
    "                doc.metadata[\"tenantId\"] = request.tenantId\n",
    "                doc.metadata[\"source\"] = url.strip()\n",
    "            all_docs.extend(docs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading URL {url}: {e}\")\n",
    "            continue\n",
    "    if not all_docs:\n",
    "        raise HTTPException(status_code=400, detail=\"No valid url provided or failed to fetch content.\")\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "    chunk_size = 300\n",
    "    chunk_overlap = 50\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    \n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    \n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    \n",
    "    document_id =  str(uuid.uuid4())\n",
    "    for doc in all_docs:\n",
    "        curr_split_docs = text_splitter.split_documents([doc])\n",
    "        document_id = document_id\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "        split_docs.extend(curr_split_docs)\n",
    "        chunk_ids.extend(curr_chunk_ids)\n",
    "        upload_documents[document_id] = {\"source\": doc.metadata[\"source\"], \"id\": document_id, \"tenantId\": request.tenantId}\n",
    "    \n",
    "    vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "    \n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    \n",
    "    return {\"status\": \"success\", \"message\": \"url processed and stored successfully.\", \"data\": {\"documentId\": document_id}}\n",
    "############################# links@edit #############################\n",
    "class UpdateLinkRequest(BaseModel):\n",
    "    tenantId: str\n",
    "    embeddedId: str\n",
    "    url: str \n",
    "@app.put(\"/api/artificial-intelligence/links\")\n",
    "async def update_web_url(request: UpdateLinkRequest):\n",
    "    \"\"\"\n",
    "    Update web URL content for a specific embeddedId.\n",
    "    \n",
    "    Args:\n",
    "        request: UpdateLinkRequest containing tenantId, embeddedId, and url.\n",
    "    \n",
    "    Returns:\n",
    "        JSON response indicating processing status.\n",
    "    \n",
    "    Raises:\n",
    "        HTTPException: When no valid url are provided or if the embeddedId does not exist.\n",
    "    \"\"\"\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    \n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    \n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    \n",
    "    if request.embeddedId not in upload_documents:\n",
    "        raise HTTPException(status_code=404, detail=\"The embeddedId does not exist.\")\n",
    "    \n",
    "    try:\n",
    "        all_docs = []\n",
    "        url = request.url.split(\",\")  \n",
    "        for url in url:\n",
    "            try:\n",
    "                loader = WebBaseLoader(url.strip())  \n",
    "                docs = loader.load()  \n",
    "                for doc in docs:\n",
    "                    doc.metadata[\"tenantId\"] = request.tenantId\n",
    "                    doc.metadata[\"source\"] = url.strip()\n",
    "                all_docs.extend(docs)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading URL {url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not all_docs:\n",
    "            raise HTTPException(status_code=400, detail=\"No valid URL provided or failed to fetch content.\")\n",
    "        \n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "        vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "        llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "        chunk_size = 3500\n",
    "        chunk_overlap = 1000\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        split_docs = []\n",
    "        chunk_ids = []\n",
    "        \n",
    "        existing_chunk_ids = [key for key in upload_documents if upload_documents[key]['id'] == request.embeddedId]\n",
    "        \n",
    "        for chunk_id in existing_chunk_ids:\n",
    "            del upload_documents[chunk_id]  # Removing old chunks\n",
    "            \n",
    "        ids_to_delete = [id for id in index.list(prefix=request.embeddedId)]\n",
    "        index.delete(ids=ids_to_delete)\n",
    "\n",
    "        for doc in all_docs:\n",
    "            curr_split_docs = text_splitter.split_documents([doc])\n",
    "            document_id = request.embeddedId  # same embeddedId\n",
    "            curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "            split_docs.extend(curr_split_docs)\n",
    "            chunk_ids.extend(curr_chunk_ids)\n",
    "            upload_documents[document_id] = {\"source\": doc.metadata[\"source\"], \"id\": document_id, \"tenantId\": request.tenantId}\n",
    "        \n",
    "        vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "        \n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump(upload_documents, f, indent=4)\n",
    "        \n",
    "      \n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"URL updated successfully.\",\n",
    "            \"embeddedId\": request.embeddedId,\n",
    "            \"updated_url\": request.url\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error updating url: {str(e)}\")\n",
    "############################# links@delete #############################\n",
    "class DeleteRequest(BaseModel):\n",
    "    embeddedId: str\n",
    "@app.delete(\"/api/artificial-intelligence/links\", summary=\"Delete Documents\", description=\"Delete all url documents in the Pinecone index that match the given prefix.\")\n",
    "async def delete_url(request: DeleteRequest):\n",
    "    \"\"\"Delete url documents matching prefix\n",
    "    Args:\n",
    "        request: Delete request containing prefix\n",
    "    Returns:\n",
    "        JSON response indicating deletion status\n",
    "    Raises:\n",
    "        HTTPException: When no matching url documents found\n",
    "    \"\"\"\n",
    "    prefix = request.embeddedId\n",
    "    ids_to_delete = [id for id in index.list(prefix=prefix)]\n",
    "    print(ids_to_delete)\n",
    "    if not ids_to_delete:\n",
    "        raise HTTPException(status_code=404, detail=\"No url documents found with the given prefix.\")\n",
    "    index.delete(ids=ids_to_delete)\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    if prefix in upload_documents:\n",
    "        del upload_documents[prefix]\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    return {\"message\": f\"Deleted {len(ids_to_delete)} documents with embeddedId '{prefix}'.\"}      \n",
    "############################# qa@upload #############################\n",
    "class QARequest(BaseModel):\n",
    "    tenantId: str\n",
    "    question: str\n",
    "    answer: str\n",
    "@app.post(\"/api/artificial-intelligence/qa\")\n",
    "async def upload_question_answer(\n",
    "    request: QARequest,\n",
    "    background_task: BackgroundTasks):\n",
    "    \"\"\"\n",
    "    Upload and store a question-answer pair\n",
    "    Args:\n",
    "        request: QA request containing question and answer\n",
    "        background_task: Background task handler    \n",
    "    Returns:\n",
    "        JSON response with QA details and status  \n",
    "    Raises:\n",
    "        HTTPException: For processing errors\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "    chunk_size = 20\n",
    "    chunk_overlap = 5\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    try:\n",
    "        document_id = str(uuid.uuid4())\n",
    "        combined_text = f\"Question: {request.question}\\nAnswer: {request.answer}\"\n",
    "        metadata = {\n",
    "            \"tenantId\": request.tenantId,\n",
    "            \"fileName\": f\"Q&A_input_{document_id}.txt\",\n",
    "            \"id\": document_id,\n",
    "            \"question\": request.question,  \n",
    "            \"answer\": request.answer}\n",
    "        document = Document(page_content=combined_text, metadata=metadata)\n",
    "        curr_split_docs = text_splitter.split_documents([document])\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "        split_docs += curr_split_docs\n",
    "        chunk_ids += curr_chunk_ids\n",
    "        upload_documents[document_id] = metadata\n",
    "        vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump(upload_documents, f, indent=4)\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"Question and Answer processed successfully.\",\n",
    "            \"id\": document_id,\n",
    "            \"question\": request.question,  \n",
    "            \"answer\": request.answer }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error processing the question and answer: {str(e)}\")\n",
    "    \n",
    "############################# qa@edit #############################\n",
    "class QAEditRequest(BaseModel):\n",
    "    tenantId: str\n",
    "    embeddedId: str\n",
    "    question: str\n",
    "    answer: str\n",
    "@app.put(\"/api/artificial-intelligence/qa\")\n",
    "async def edit_question_answer(\n",
    "    request: QAEditRequest,  \n",
    "    background_task: BackgroundTasks = BackgroundTasks()):\n",
    "    \"\"\"\n",
    "    Edit a question and answer pair by replacing the content in the same embeddedId.\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "    chunk_size = 20\n",
    "    chunk_overlap = 5\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    if request.embeddedId not in upload_documents:\n",
    "        raise HTTPException(status_code=404, detail=\"The embeddedId does not exist.\")\n",
    "    try:\n",
    "        ids_to_delete = [id for id in index.list(prefix=request.embeddedId)]\n",
    "        index.delete(ids=ids_to_delete)\n",
    "        combined_text = f\"Question: {request.question}\\nAnswer: {request.answer}\"\n",
    "        metadata = {\n",
    "            \"tenantId\": request.tenantId,\n",
    "            \"fileName\": f\"Q&A_input_{request.embeddedId}.txt\",\n",
    "            \"id\": request.embeddedId,\n",
    "            \"question\": request.question,\n",
    "            \"answer\": request.answer}\n",
    "        document = Document(page_content=combined_text, metadata=metadata)\n",
    "        curr_split_docs = text_splitter.split_documents([document])\n",
    "        curr_chunk_ids = [f\"{request.embeddedId}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "        vectorstore.add_documents(documents=curr_split_docs, ids=curr_chunk_ids)\n",
    "        upload_documents[request.embeddedId] = metadata\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump(upload_documents, f, indent=4)\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"Question and Answer edited successfully.\",\n",
    "            \"id\": request.embeddedId,\n",
    "            \"question\": request.question,\n",
    "            \"answer\": request.answer }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error editing the question and answer: {str(e)}\")\n",
    "\n",
    "####################################### qa@delete ##########################\n",
    "class DeleteRequest(BaseModel):\n",
    "    prefix: str\n",
    "@app.delete(\"/api/artificial-intelligence/qa\", summary=\"Delete Question Answer\", description=\"Delete all documents in the Pinecone index that match the given prefix.\")\n",
    "async def delete_documents(request: DeleteRequest):\n",
    "    \"\"\"Delete Question Answer matching prefix\n",
    "    Args:\n",
    "        request: Delete request containing prefix\n",
    "    Returns:\n",
    "        JSON response indicating deletion status\n",
    "    Raises:\n",
    "        HTTPException: When no matching documents found\n",
    "    \"\"\"\n",
    "    prefix = request.prefix\n",
    "    ids_to_delete = [id for id in index.list(prefix=prefix)]\n",
    "    print(ids_to_delete)\n",
    "    if not ids_to_delete:\n",
    "        raise HTTPException(status_code=404, detail=\"No QA found with the given prefix.\")\n",
    "    index.delete(ids=ids_to_delete)\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    if prefix in upload_documents:\n",
    "        del upload_documents[prefix]\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    return {\"message\": f\"Deleted {len(ids_to_delete)} QA with prefix '{prefix}'.\"}\n",
    "\n",
    "\n",
    "############################# tenant_files #############################\n",
    "# @app.get(\"/api/artificial-intelligence/tenant_files\")\n",
    "# async def retrieve_files(tenantId: str = Query(...)):\n",
    "#     uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "#     if not os.path.exists(uploaded_documents_path):\n",
    "#         with open(uploaded_documents_path, \"w\") as f:\n",
    "#             json.dump({}, f)\n",
    "#     with open(uploaded_documents_path, \"r\") as f:\n",
    "#         upload_documents = json.load(f)\n",
    "#     tenantFiles = []\n",
    "#     for item in list(upload_documents.values()):\n",
    "#         if item.get('tenantId') == tenantId:\n",
    "#             tenantFiles.append(item)\n",
    "#     return {\"data\": tenantFiles}\n",
    "\n",
    "@app.get(\"/api/artificial-intelligence/tenant_files\")\n",
    "async def retrieve_files(tenantId: str = Query(...)):\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    tenantFiles = []\n",
    "    for item in list(upload_documents.values()):\n",
    "        # Only include items that have either 'fileName' (from /upload) \n",
    "        # or 'source' (from /links) but not from /qa\n",
    "        if (item.get('tenantId') == tenantId and \n",
    "            ('fileName' in item) and \n",
    "            not item.get('fileName', '').startswith('Q&A_input_')):\n",
    "            tenantFiles.append(item)\n",
    "    return {\"data\": tenantFiles}\n",
    "\n",
    "############################# prompts #############################\n",
    "def initializeVectorStore():\n",
    "    \"\"\"Initialize and configure vector store with embeddings\n",
    "    Returns:\n",
    "        Configured PineconeVectorStore instance\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0\n",
    ")\n",
    "@app.get(\"/api/artificial-intelligence/prompts\")\n",
    "async def prompts_keyword(tenantId: str = Query(...), keyword: str = Query(...)):\n",
    "    \"\"\"Process keyword prompts and retrieve answers using improved RAG chain\n",
    "    Args:\n",
    "        tenantId: Identifier for the tenant\n",
    "        keyword: Search keyword/prompt  \n",
    "    Returns:\n",
    "        PlainText response with answer or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(tenantId, keyword)\n",
    "        vectorstore = initializeVectorStore()\n",
    "        template = \"\"\"Answer the question based only on the following context:\n",
    "        {context}\n",
    "        User Question:\n",
    "        {question}\n",
    "         If the user greets you, respond with one of the following without searching information from knowledge base:\n",
    "        - \"Hello! How can I assist you today?\"\n",
    "        - \"Hi there! What can I do for you?\"\n",
    "        - \"Good day! How can I assist you?\"\n",
    "        - \"Hey! What can I help you with?\"\n",
    "        - \"Hi! How can I support you today?\"\n",
    "        - \"Welcome! How may I help you?\"\n",
    "        - \"Salutations! How can I assist you?\"\n",
    "        - \"What's up? How can I help?\"\n",
    "        - \"Good to see you! How can I assist?\"\n",
    "\n",
    "        Please provide a concise and accurate answer based on the context above. If the context does not contain  information to answer the question, respond with: \"I don't have the information you're looking for, please provide additional details.\" And if it's greetings then show greetings response.\n",
    "        Make sure your answer is relevant and accurate to the question and does not repeat the question itself..\n",
    "        In response give only answer but not question.\n",
    "        \"\"\"\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\n",
    "                \"k\": 1,\n",
    "                \"filter\": {\n",
    "                    'tenantId': {'$eq': tenantId}\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        rag_chain = (\n",
    "            {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser())\n",
    "        answer = rag_chain.invoke(keyword)\n",
    "        return PlainTextResponse(answer)\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing your request: {str(e)}. Please contact support for assistance.\"\n",
    "        print(f\"Error in prompts_keyword: {str(e)}\") \n",
    "        return PlainTextResponse(error_msg, status_code=500)\n",
    "\n",
    "############################ Summarizer #################################################\n",
    "class SummarizationRequest(BaseModel):\n",
    "    caseId: str\n",
    "    assignedTo: str\n",
    "    patientName: str\n",
    "    onBehalfOf: str\n",
    "    currentCaseStatus: str\n",
    "    summarizationType: str\n",
    "    notes: List[Any]\n",
    "\n",
    "@app.post(\"/api/artificial-intelligence/summarize\")\n",
    "async def summarize(request: SummarizationRequest):\n",
    "    \"\"\"Combine JSON upload and summary generation into one endpoint.\"\"\"\n",
    "    try:\n",
    "        json_data = {\"caseId\": request.caseId, \"notes\": request.notes, \"patientName\": request.patientName, \"assignedTo\": request.assignedTo, \"currentCaseStatus\": request.currentCaseStatus}\n",
    "        noteLength = sum(len(note['note']) for note in request.notes)\n",
    "        maxTokenToUse = 500\n",
    "        \n",
    "        if(noteLength <= 10):\n",
    "            maxTokenToUse=200\n",
    "        \n",
    "        \n",
    "        json_doc = Document(page_content=json.dumps(json_data), metadata={'fileName': 'query_json'})\n",
    "        split_docs = [json_doc]\n",
    "        print(f\"Processing {len(split_docs)} documents.\")\n",
    "        \n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.0,\n",
    "            max_tokens=maxTokenToUse,\n",
    "            top_p=1.0,\n",
    "        )\n",
    "\n",
    "        full_summary_prompt_template = f\"\"\"\n",
    "        \"{{split_docs}}\"\n",
    "\n",
    "        Please summarize the following notes directly to me, the {request.onBehalfOf} in a structured format, focusing on key actions and communications. Include the following sections:\n",
    "        \n",
    "        1. Initial Request: Briefly explain the initial issue or request.\n",
    "        2. Key Updates: Highlight major actions taken, communications, and progress in chronological order, including dates.\n",
    "        3. Current Status: Summarize the present situation or outcome of the case.\n",
    "\n",
    "        Do not expose any ids\n",
    "\n",
    "        Complete the sentence at the end with a full stop.\n",
    "        Give html format\n",
    "        \"\"\"\n",
    "\n",
    "        # unread_summary_prompt_template = f\"\"\"\n",
    "        # \"{{split_docs}}\"\n",
    "\n",
    "        # Please summarize the following case notes directly to me, the {request.onBehalfOf} in a paragraph format.\n",
    "        \n",
    "        # Do not expose any ids\n",
    "        # Keep it simple and short.\n",
    "        # Complete the sentence at the end with a full stop.\n",
    "        # \"\"\"\n",
    "\n",
    "\n",
    "        unread_summary_prompt_template = f\"\"\"\n",
    "            \"{{split_docs}}\"\n",
    "\n",
    "            Please summarize the following case notes directly to me, the {request.onBehalfOf} in a paragraph format.\n",
    "            \n",
    "            Do not expose any ids\n",
    "            Keep it simple and short.\n",
    "            Complete the sentence at the end with a full stop.\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "        prompt=\"\"\n",
    "        if(request.summarizationType == 'full'):\n",
    "            prompt = full_summary_prompt_template\n",
    "        else:\n",
    "            prompt = unread_summary_prompt_template\n",
    "        \n",
    "        print(prompt)\n",
    "\n",
    "        prompt = PromptTemplate.from_template(prompt)\n",
    "        llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "        stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"split_docs\")\n",
    "        summary = stuff_chain.run(split_docs)\n",
    "\n",
    "        sentences = summary\n",
    "        print(sentences)\n",
    "\n",
    "        # if len(sentences) > 1:\n",
    "        #     summary = '.'.join(sentences[:-1]).strip()  \n",
    "        # else:\n",
    "        #     summary = sentences[0].strip()  \n",
    "\n",
    "        # if not summary.endswith('.'):\n",
    "        #     summary += '.'\n",
    "        \n",
    "        summary = sentences.replace(\"\\n\", \" \")\n",
    "        summary = summary.replace(\"```html\", \" \")\n",
    "        return JSONResponse(content={\n",
    "            \"summary\": summary\n",
    "        })\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error during summarizing the documents: {str(e)}\")\n",
    "\n",
    "####################################### delete #########################\n",
    "class DeleteRequest(BaseModel):\n",
    "    prefix: str\n",
    "@app.delete(\"/api/artificial-intelligence/delete\", summary=\"Delete Documents\", description=\"Delete all documents in the Pinecone index that match the given prefix.\")\n",
    "async def delete_documents(request: DeleteRequest):\n",
    "    \"\"\"Delete documents matching prefix\n",
    "    Args:\n",
    "        request: Delete request containing prefix\n",
    "    Returns:\n",
    "        JSON response indicating deletion status\n",
    "    Raises:\n",
    "        HTTPException: When no matching documents found\n",
    "    \"\"\"\n",
    "    prefix = request.prefix\n",
    "    ids_to_delete = [id for id in index.list(prefix=prefix)]\n",
    "    print(ids_to_delete)\n",
    "    if not ids_to_delete:\n",
    "        raise HTTPException(status_code=404, detail=\"No documents found with the given prefix.\")\n",
    "    index.delete(ids=ids_to_delete)\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    if prefix in upload_documents:\n",
    "        del upload_documents[prefix]\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    return {\"message\": f\"Deleted {len(ids_to_delete)} documents with prefix '{prefix}'.\"}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "#from langserve import add_routes\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "from fastapi import UploadFile, File, HTTPException\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from typing import Any\n",
    "import uuid\n",
    "import pylibmagic\n",
    "from fastapi import Query\n",
    "import io\n",
    "import docx\n",
    "import shutil\n",
    "import uvicorn\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.retrievers import PineconeHybridSearchRetriever\n",
    "from fastapi.responses import PlainTextResponse\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import Runnable\n",
    "from pydantic import BaseModel, Field, validator, ValidationError\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from tqdm.auto import tqdm\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv\n",
    "from fastapi.responses import RedirectResponse\n",
    "import glob\n",
    "from fastapi import  UploadFile\n",
    "from fastapi.responses import JSONResponse\n",
    "from uuid import uuid4\n",
    "import time\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi import  Request\n",
    "from fastapi import  BackgroundTasks, UploadFile, Form\n",
    "import getpass\n",
    "import os\n",
    "import concurrent.futures\n",
    "from fastapi import  UploadFile\n",
    "from PyPDF2 import PdfReader  \n",
    "import docx\n",
    "import json\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "\n",
    "\n",
    "class MyModel(BaseModel):\n",
    "    message: BaseMessage\n",
    "class Config:\n",
    "    arbitrary_types_allowed = True\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"version:vac0.1\",\n",
    "    description=\"\")\n",
    "\n",
    "origins = [\"*\"]\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=origins,\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"])\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def add_process_time_header(request: Request, call_next):\n",
    "    start_time = time.perf_counter()\n",
    "    response = await call_next(request)\n",
    "    process_time = time.perf_counter() - start_time\n",
    "    response.headers[\"Processing-Time\"] = str(process_time)\n",
    "    return response\n",
    "\n",
    "######################### keys #############################\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pinecone_index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "if not openai_api_key:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set. Please set it in your environment.\")\n",
    "if not pinecone_api_key:\n",
    "    raise EnvironmentError(\"PINECONE_API_KEY is not set. Please set it in your environment.\")\n",
    "if not pinecone_index_name:\n",
    "    raise EnvironmentError(\"PINECONE_INDEX_NAME is not set. Please set it in your environment.\")\n",
    "if openai_api_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "if pinecone_api_key:\n",
    "    os.environ[\"PINECONE_API_KEY\"] = pinecone_api_key\n",
    "if pinecone_index_name:\n",
    "    os.environ[\"PINECONE_INDEX_NAME\"] = pinecone_index_name\n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "############### vectordatabase initializations ####################\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "pinecone_index_name = os.environ.get(\"PINECONE_INDEX_NAME\")\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "pinecone_client = Pinecone(api_key=pinecone_api_key)\n",
    "def initialize_pinecone_index(client: Pinecone, index_name: str):\n",
    "    existing_indexes = [index_info[\"name\"] for index_info in client.list_indexes()]\n",
    "    if index_name not in existing_indexes:\n",
    "        client.create_index(\n",
    "            name=index_name,\n",
    "            dimension=3072,\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))\n",
    "        while not client.describe_index(index_name).status[\"ready\"]:\n",
    "            time.sleep(1)\n",
    "    return client.Index(index_name)\n",
    "index = initialize_pinecone_index(pinecone_client, pinecone_index_name)\n",
    "\n",
    "############################# upload #############################\n",
    "@app.post(\"/api/artificial-intelligence/upload\")\n",
    "async def upload_files(\n",
    "    tenantId: str = Form(...),\n",
    "    files: List[UploadFile] = File(...),\n",
    "    background_task: BackgroundTasks = BackgroundTasks()):\n",
    "    \"\"\"\n",
    "    Upload and process multiple document files\n",
    "        PDF, DOCX, and TXT files\n",
    "    Args:\n",
    "        tenantId: Identifier for the tenant\n",
    "        files: List of files to upload\n",
    "        background_task: Background task handler\n",
    "    Returns:\n",
    "        JSON response indicating upload status\n",
    "    Raises:\n",
    "        HTTPException: For invalid files or duplicates\n",
    "    \"\"\"\n",
    "    dir_name = str(uuid4())\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    allowed_extensions = {\".pdf\", \".docx\", \".txt\"}\n",
    "    fileName = set()\n",
    "    for file in files:\n",
    "        if file.filename in fileName:\n",
    "            raise HTTPException(\n",
    "                status_code=400, detail=f\"Duplicate file detected: {file.filename}\")\n",
    "        fileName.add(file.filename)\n",
    "        _, extension = os.path.splitext(file.filename)\n",
    "        if extension.lower() not in allowed_extensions:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=f\"Invalid file type: {file.filename}. Only PDF, DOCX, and TXT files are allowed.\",)\n",
    "        destination = os.path.join(dir_name, file.filename)\n",
    "        print('creating doc ' + destination)\n",
    "        with open(destination, \"wb\") as buffer:\n",
    "            shutil.copyfileobj(file.file, buffer)\n",
    "    docs = load_docs(dir_name, tenantId)\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    chunk_size = 800\n",
    "    chunk_overlap = 150\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    for doc in docs:\n",
    "        curr_split_docs = text_splitter.split_documents([doc])\n",
    "        document_id = str(uuid4())\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "        split_docs = split_docs + curr_split_docs\n",
    "        chunk_ids = chunk_ids + curr_chunk_ids\n",
    "        upload_documents[document_id] =  {\"fileName\": doc.metadata['filename'], \"id\": document_id, \"tenantId\": tenantId}\n",
    "    vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    shutil.rmtree(dir_name)\n",
    "    return {\"status\": \"success\", \"message\": \"Files uploaded successfully.\"}\n",
    "def load_docs(directory, tenantId):\n",
    "    loader = DirectoryLoader(directory)\n",
    "    docs = loader.load()\n",
    "    for doc in docs:\n",
    "        doc.metadata['tenantId'] = tenantId\n",
    "        doc.metadata['filename'] = os.path.basename(doc.metadata['source'])\n",
    "        print(\"ooooooooo\" + doc.metadata['filename'])\n",
    "    return docs\n",
    "\n",
    "############################# links@upload #############################\n",
    "class UploadLinkRequest(BaseModel):\n",
    "    tenantId: str\n",
    "    url: str \n",
    "\n",
    "@app.post(\"/api/artificial-intelligence/links\")\n",
    "async def upload_web_url(request: UploadLinkRequest, background_tasks: BackgroundTasks):\n",
    "    \"\"\"\n",
    "    Process and store web URL content.\n",
    "    \n",
    "    Args:\n",
    "        request: UploadLinkRequest containing tenantId and url.\n",
    "    \n",
    "    Returns:\n",
    "        JSON response indicating processing status.\n",
    "    \n",
    "    Raises:\n",
    "        HTTPException: When no valid url are provided.\n",
    "    \"\"\"\n",
    "    all_docs = []  \n",
    "    urls = request.url.split(\",\")  \n",
    "\n",
    "    for url in urls:\n",
    "        url = url.strip()\n",
    "        try:\n",
    "            async with AsyncWebCrawler(verbose=True) as crawler:\n",
    "                result = await crawler.arun(url=url)  # Running the crawler on the URL\n",
    "                extracted_content = result.markdown  # Extracted content in markdown format\n",
    "                doc = Document(page_content=extracted_content, metadata={\"source\": url, \"tenantId\": request.tenantId})\n",
    "                all_docs.append(doc)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading URL {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not all_docs:\n",
    "        raise HTTPException(status_code=400, detail=\"No valid URL provided or failed to fetch content.\")\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=os.getenv(\"PINECONE_INDEX_NAME\"), embedding=embeddings)\n",
    "    chunk_size = 3500\n",
    "    chunk_overlap = 1000\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    \n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    \n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    \n",
    "    document_id = str(uuid.uuid4())\n",
    "    for doc in all_docs:\n",
    "        curr_split_docs = text_splitter.split_documents([doc])\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "        split_docs.extend(curr_split_docs)\n",
    "        chunk_ids.extend(curr_chunk_ids)\n",
    "        upload_documents[document_id] = {\"source\": doc.metadata[\"source\"], \"id\": document_id, \"tenantId\": request.tenantId}\n",
    "    \n",
    "    vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "    \n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    \n",
    "    return {\"status\": \"success\", \"message\": \"URL processed and stored successfully.\", \"data\": {\"documentId\": document_id}}\n",
    "############################# links@edit #############################\n",
    "class UpdateLinkRequest(BaseModel):\n",
    "    tenantId: str\n",
    "    embeddedId: str\n",
    "    url: str \n",
    "@app.put(\"/api/artificial-intelligence/links\")\n",
    "async def update_web_url(request: UpdateLinkRequest):\n",
    "    \"\"\"\n",
    "    Update web URL content for a specific embeddedId.\n",
    "    \n",
    "    Args:\n",
    "        request: UpdateLinkRequest containing tenantId, embeddedId, and url.\n",
    "    \n",
    "    Returns:\n",
    "        JSON response indicating processing status.\n",
    "    \n",
    "    Raises:\n",
    "        HTTPException: When no valid url are provided or if the embeddedId does not exist.\n",
    "    \"\"\"\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    \n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    \n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    \n",
    "    if request.embeddedId not in upload_documents:\n",
    "        raise HTTPException(status_code=404, detail=\"The embeddedId does not exist.\")\n",
    "    \n",
    "    try:\n",
    "        all_docs = []  \n",
    "        urls = request.url.split(\",\")  \n",
    "\n",
    "        for url in urls:\n",
    "            url = url.strip()\n",
    "            try:\n",
    "                async with AsyncWebCrawler(verbose=True) as crawler:\n",
    "                    result = await crawler.arun(url=url)  # Running the crawler on the URL\n",
    "                    extracted_content = result.markdown  # Extracted content in markdown format\n",
    "                    doc = Document(page_content=extracted_content, metadata={\"source\": url, \"tenantId\": request.tenantId})\n",
    "                    all_docs.append(doc)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading URL {url}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not all_docs:\n",
    "            raise HTTPException(status_code=400, detail=\"No valid URL provided or failed to fetch content.\")\n",
    "        \n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "        vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "        llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "        chunk_size = 3500\n",
    "        chunk_overlap = 1000\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        split_docs = []\n",
    "        chunk_ids = []\n",
    "        \n",
    "        existing_chunk_ids = [key for key in upload_documents if upload_documents[key]['id'] == request.embeddedId]\n",
    "        \n",
    "        for chunk_id in existing_chunk_ids:\n",
    "            del upload_documents[chunk_id]  \n",
    "            \n",
    "        ids_to_delete = [id for id in index.list(prefix=request.embeddedId)]\n",
    "        index.delete(ids=ids_to_delete)\n",
    "\n",
    "        for doc in all_docs:\n",
    "            curr_split_docs = text_splitter.split_documents([doc])\n",
    "            document_id = request.embeddedId  # same embeddedId\n",
    "            curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "            split_docs.extend(curr_split_docs)\n",
    "            chunk_ids.extend(curr_chunk_ids)\n",
    "            upload_documents[document_id] = {\"source\": doc.metadata[\"source\"], \"id\": document_id, \"tenantId\": request.tenantId}\n",
    "        \n",
    "        vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "        \n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump(upload_documents, f, indent=4)\n",
    "        \n",
    "      \n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"URL updated successfully.\",\n",
    "            \"embeddedId\": request.embeddedId,\n",
    "            \"updated_url\": request.url\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error updating url: {str(e)}\")\n",
    "############################# links@delete #############################\n",
    "class DeleteRequest(BaseModel):\n",
    "    embeddedId: str\n",
    "@app.delete(\"/api/artificial-intelligence/links\", summary=\"Delete Documents\", description=\"Delete all url documents in the Pinecone index that match the given prefix.\")\n",
    "async def delete_url(request: DeleteRequest):\n",
    "    \"\"\"Delete url documents matching prefix\n",
    "    Args:\n",
    "        request: Delete request containing prefix\n",
    "    Returns:\n",
    "        JSON response indicating deletion status\n",
    "    Raises:\n",
    "        HTTPException: When no matching url documents found\n",
    "    \"\"\"\n",
    "    prefix = request.embeddedId\n",
    "    ids_to_delete = [id for id in index.list(prefix=prefix)]\n",
    "    print(ids_to_delete)\n",
    "    if not ids_to_delete:\n",
    "        raise HTTPException(status_code=404, detail=\"No url documents found with the given prefix.\")\n",
    "    index.delete(ids=ids_to_delete)\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    if prefix in upload_documents:\n",
    "        del upload_documents[prefix]\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    return {\"message\": f\"Deleted {len(ids_to_delete)} documents with embeddedId '{prefix}'.\"}      \n",
    "############################# qa@upload #############################\n",
    "class QARequest(BaseModel):\n",
    "    tenantId: str\n",
    "    question: str\n",
    "    answer: str\n",
    "@app.post(\"/api/artificial-intelligence/qa\")\n",
    "async def upload_question_answer(\n",
    "    request: QARequest,\n",
    "    background_task: BackgroundTasks):\n",
    "    \"\"\"\n",
    "    Upload and store a question-answer pair\n",
    "    Args:\n",
    "        request: QA request containing question and answer\n",
    "        background_task: Background task handler    \n",
    "    Returns:\n",
    "        JSON response with QA details and status  \n",
    "    Raises:\n",
    "        HTTPException: For processing errors\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "    chunk_size = 20\n",
    "    chunk_overlap = 5\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    try:\n",
    "        document_id = str(uuid.uuid4())\n",
    "        combined_text = f\"Question: {request.question}\\nAnswer: {request.answer}\"\n",
    "        metadata = {\n",
    "            \"tenantId\": request.tenantId,\n",
    "            \"fileName\": f\"Q&A_input_{document_id}.txt\",\n",
    "            \"id\": document_id,\n",
    "            \"question\": request.question,  \n",
    "            \"answer\": request.answer}\n",
    "        document = Document(page_content=combined_text, metadata=metadata)\n",
    "        curr_split_docs = text_splitter.split_documents([document])\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "        split_docs += curr_split_docs\n",
    "        chunk_ids += curr_chunk_ids\n",
    "        upload_documents[document_id] = metadata\n",
    "        vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump(upload_documents, f, indent=4)\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"Question and Answer processed successfully.\",\n",
    "            \"id\": document_id,\n",
    "            \"question\": request.question,  \n",
    "            \"answer\": request.answer }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error processing the question and answer: {str(e)}\")\n",
    "    \n",
    "############################# qa@edit #############################\n",
    "class QAEditRequest(BaseModel):\n",
    "    tenantId: str\n",
    "    embeddedId: str\n",
    "    question: str\n",
    "    answer: str\n",
    "@app.put(\"/api/artificial-intelligence/qa\")\n",
    "async def edit_question_answer(\n",
    "    request: QAEditRequest,  \n",
    "    background_task: BackgroundTasks = BackgroundTasks()):\n",
    "    \"\"\"\n",
    "    Edit a question and answer pair by replacing the content in the same embeddedId.\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "    chunk_size = 20\n",
    "    chunk_overlap = 5\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    if request.embeddedId not in upload_documents:\n",
    "        raise HTTPException(status_code=404, detail=\"The embeddedId does not exist.\")\n",
    "    try:\n",
    "        ids_to_delete = [id for id in index.list(prefix=request.embeddedId)]\n",
    "        index.delete(ids=ids_to_delete)\n",
    "        combined_text = f\"Question: {request.question}\\nAnswer: {request.answer}\"\n",
    "        metadata = {\n",
    "            \"tenantId\": request.tenantId,\n",
    "            \"fileName\": f\"Q&A_input_{request.embeddedId}.txt\",\n",
    "            \"id\": request.embeddedId,\n",
    "            \"question\": request.question,\n",
    "            \"answer\": request.answer}\n",
    "        document = Document(page_content=combined_text, metadata=metadata)\n",
    "        curr_split_docs = text_splitter.split_documents([document])\n",
    "        curr_chunk_ids = [f\"{request.embeddedId}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "        vectorstore.add_documents(documents=curr_split_docs, ids=curr_chunk_ids)\n",
    "        upload_documents[request.embeddedId] = metadata\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump(upload_documents, f, indent=4)\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"Question and Answer edited successfully.\",\n",
    "            \"id\": request.embeddedId,\n",
    "            \"question\": request.question,\n",
    "            \"answer\": request.answer }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error editing the question and answer: {str(e)}\")\n",
    "\n",
    "####################################### qa@delete ##########################\n",
    "class DeleteRequest(BaseModel):\n",
    "    prefix: str\n",
    "@app.delete(\"/api/artificial-intelligence/qa\", summary=\"Delete Question Answer\", description=\"Delete all documents in the Pinecone index that match the given prefix.\")\n",
    "async def delete_documents(request: DeleteRequest):\n",
    "    \"\"\"Delete Question Answer matching prefix\n",
    "    Args:\n",
    "        request: Delete request containing prefix\n",
    "    Returns:\n",
    "        JSON response indicating deletion status\n",
    "    Raises:\n",
    "        HTTPException: When no matching documents found\n",
    "    \"\"\"\n",
    "    prefix = request.prefix\n",
    "    ids_to_delete = [id for id in index.list(prefix=prefix)]\n",
    "    print(ids_to_delete)\n",
    "    if not ids_to_delete:\n",
    "        raise HTTPException(status_code=404, detail=\"No QA found with the given prefix.\")\n",
    "    index.delete(ids=ids_to_delete)\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    if prefix in upload_documents:\n",
    "        del upload_documents[prefix]\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    return {\"message\": f\"Deleted {len(ids_to_delete)} QA with prefix '{prefix}'.\"}\n",
    "\n",
    "\n",
    "############################# tenant_files #############################\n",
    "# @app.get(\"/api/artificial-intelligence/tenant_files\")\n",
    "# async def retrieve_files(tenantId: str = Query(...)):\n",
    "#     uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "#     if not os.path.exists(uploaded_documents_path):\n",
    "#         with open(uploaded_documents_path, \"w\") as f:\n",
    "#             json.dump({}, f)\n",
    "#     with open(uploaded_documents_path, \"r\") as f:\n",
    "#         upload_documents = json.load(f)\n",
    "#     tenantFiles = []\n",
    "#     for item in list(upload_documents.values()):\n",
    "#         if item.get('tenantId') == tenantId:\n",
    "#             tenantFiles.append(item)\n",
    "#     return {\"data\": tenantFiles}\n",
    "\n",
    "@app.get(\"/api/artificial-intelligence/tenant_files\")\n",
    "async def retrieve_files(tenantId: str = Query(...)):\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    tenantFiles = []\n",
    "    for item in list(upload_documents.values()):\n",
    "        # Only include items that have either 'fileName' (from /upload) \n",
    "        # or 'source' (from /links) but not from /qa\n",
    "        if (item.get('tenantId') == tenantId and \n",
    "            ('fileName' in item) and \n",
    "            not item.get('fileName', '').startswith('Q&A_input_')):\n",
    "            tenantFiles.append(item)\n",
    "    return {\"data\": tenantFiles}\n",
    "\n",
    "############################# prompts #############################\n",
    "def initializeVectorStore():\n",
    "    \"\"\"Initialize and configure vector store with embeddings\n",
    "    Returns:\n",
    "        Configured PineconeVectorStore instance\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0\n",
    ")\n",
    "@app.get(\"/api/artificial-intelligence/prompts\")\n",
    "async def prompts_keyword(tenantId: str = Query(...), keyword: str = Query(...)):\n",
    "    \"\"\"Process keyword prompts and retrieve answers using improved RAG chain\n",
    "    Args:\n",
    "        tenantId: Identifier for the tenant\n",
    "        keyword: Search keyword/prompt  \n",
    "    Returns:\n",
    "        PlainText response with answer or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(tenantId, keyword)\n",
    "        vectorstore = initializeVectorStore()\n",
    "        template = \"\"\"Answer the question based only on the following context:\n",
    "        {context}\n",
    "        User Question:\n",
    "        {question}\n",
    "         If the user greets you, respond with one of the following without searching information from knowledge base:\n",
    "        - \"Hello! How can I assist you today?\"\n",
    "        - \"Hi there! What can I do for you?\"\n",
    "        - \"Good day! How can I assist you?\"\n",
    "        - \"Hey! What can I help you with?\"\n",
    "        - \"Hi! How can I support you today?\"\n",
    "        - \"Welcome! How may I help you?\"\n",
    "        - \"Salutations! How can I assist you?\"\n",
    "        - \"What's up? How can I help?\"\n",
    "        - \"Good to see you! How can I assist?\"\n",
    "\n",
    "        Please provide a concise and accurate answer based on the context above. If the context does not contain  information to answer the question, respond with: \"I don't have the information you're looking for, please provide additional details.\" And if it's greetings then show greetings response.\n",
    "        Make sure your answer is relevant and accurate to the question and does not repeat the question itself..\n",
    "        In response give only answer but not question.\n",
    "        \"\"\"\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\n",
    "                \"k\": 1,\n",
    "                \"filter\": {\n",
    "                    'tenantId': {'$eq': tenantId}\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        rag_chain = (\n",
    "            {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser())\n",
    "        answer = rag_chain.invoke(keyword)\n",
    "        return PlainTextResponse(answer)\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing your request: {str(e)}. Please contact support for assistance.\"\n",
    "        print(f\"Error in prompts_keyword: {str(e)}\") \n",
    "        return PlainTextResponse(error_msg, status_code=500)\n",
    "\n",
    "############################ Summarizer #################################################\n",
    "class SummarizationRequest(BaseModel):\n",
    "    caseId: str\n",
    "    assignedTo: str\n",
    "    patientName: str\n",
    "    onBehalfOf: str\n",
    "    currentCaseStatus: str\n",
    "    summarizationType: str\n",
    "    notes: List[Any]\n",
    "\n",
    "@app.post(\"/api/artificial-intelligence/summarize\")\n",
    "async def summarize(request: SummarizationRequest):\n",
    "    \"\"\"Combine JSON upload and summary generation into one endpoint.\"\"\"\n",
    "    try:\n",
    "        json_data = {\"caseId\": request.caseId, \"notes\": request.notes, \"patientName\": request.patientName, \"assignedTo\": request.assignedTo, \"currentCaseStatus\": request.currentCaseStatus}\n",
    "        noteLength = sum(len(note['note']) for note in request.notes)\n",
    "        maxTokenToUse = 500\n",
    "        \n",
    "        if(noteLength <= 10):\n",
    "            maxTokenToUse=200\n",
    "        \n",
    "        \n",
    "        json_doc = Document(page_content=json.dumps(json_data), metadata={'fileName': 'query_json'})\n",
    "        split_docs = [json_doc]\n",
    "        print(f\"Processing {len(split_docs)} documents.\")\n",
    "        \n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.0,\n",
    "            max_tokens=maxTokenToUse,\n",
    "            top_p=1.0,\n",
    "        )\n",
    "\n",
    "        full_summary_prompt_template = f\"\"\"\n",
    "        \"{{split_docs}}\"\n",
    "\n",
    "        Please summarize the following notes directly to me, the {request.onBehalfOf} in a structured format, focusing on key actions and communications. Include the following sections:\n",
    "        \n",
    "        1. Initial Request: Briefly explain the initial issue or request.\n",
    "        2. Key Updates: Highlight major actions taken, communications, and progress in chronological order, including dates.\n",
    "        3. Current Status: Summarize the present situation or outcome of the case.\n",
    "\n",
    "        Do not expose any ids\n",
    "\n",
    "        Complete the sentence at the end with a full stop.\n",
    "        Give html format\n",
    "        \"\"\"\n",
    "\n",
    "        # unread_summary_prompt_template = f\"\"\"\n",
    "        # \"{{split_docs}}\"\n",
    "\n",
    "        # Please summarize the following case notes directly to me, the {request.onBehalfOf} in a paragraph format.\n",
    "        \n",
    "        # Do not expose any ids\n",
    "        # Keep it simple and short.\n",
    "        # Complete the sentence at the end with a full stop.\n",
    "        # \"\"\"\n",
    "\n",
    "\n",
    "        unread_summary_prompt_template = f\"\"\"\n",
    "            \"{{split_docs}}\"\n",
    "\n",
    "            Please summarize the following case notes directly to me, the {request.onBehalfOf} in a paragraph format.\n",
    "            \n",
    "            Do not expose any ids\n",
    "            Keep it simple and short.\n",
    "            Complete the sentence at the end with a full stop.\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "        prompt=\"\"\n",
    "        if(request.summarizationType == 'full'):\n",
    "            prompt = full_summary_prompt_template\n",
    "        else:\n",
    "            prompt = unread_summary_prompt_template\n",
    "        \n",
    "        print(prompt)\n",
    "\n",
    "        prompt = PromptTemplate.from_template(prompt)\n",
    "        llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "        stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"split_docs\")\n",
    "        summary = stuff_chain.run(split_docs)\n",
    "\n",
    "        sentences = summary\n",
    "        print(sentences)\n",
    "\n",
    "        # if len(sentences) > 1:\n",
    "        #     summary = '.'.join(sentences[:-1]).strip()  \n",
    "        # else:\n",
    "        #     summary = sentences[0].strip()  \n",
    "\n",
    "        # if not summary.endswith('.'):\n",
    "        #     summary += '.'\n",
    "        \n",
    "        summary = sentences.replace(\"\\n\", \" \")\n",
    "        summary = summary.replace(\"```html\", \" \")\n",
    "        return JSONResponse(content={\n",
    "            \"summary\": summary\n",
    "        })\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error during summarizing the documents: {str(e)}\")\n",
    "\n",
    "####################################### delete #########################\n",
    "class DeleteRequest(BaseModel):\n",
    "    prefix: str\n",
    "@app.delete(\"/api/artificial-intelligence/delete\", summary=\"Delete Documents\", description=\"Delete all documents in the Pinecone index that match the given prefix.\")\n",
    "async def delete_documents(request: DeleteRequest):\n",
    "    \"\"\"Delete documents matching prefix\n",
    "    Args:\n",
    "        request: Delete request containing prefix\n",
    "    Returns:\n",
    "        JSON response indicating deletion status\n",
    "    Raises:\n",
    "        HTTPException: When no matching documents found\n",
    "    \"\"\"\n",
    "    prefix = request.prefix\n",
    "    ids_to_delete = [id for id in index.list(prefix=prefix)]\n",
    "    print(ids_to_delete)\n",
    "    if not ids_to_delete:\n",
    "        raise HTTPException(status_code=404, detail=\"No documents found with the given prefix.\")\n",
    "    index.delete(ids=ids_to_delete)\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    if prefix in upload_documents:\n",
    "        del upload_documents[prefix]\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    return {\"message\": f\"Deleted {len(ids_to_delete)} documents with prefix '{prefix}'.\"}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "#from langserve import add_routes\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "from fastapi import UploadFile, File, HTTPException\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from typing import Any\n",
    "import uuid\n",
    "import pylibmagic\n",
    "from fastapi import Query\n",
    "import io\n",
    "import docx\n",
    "import shutil\n",
    "import uvicorn\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.retrievers import PineconeHybridSearchRetriever\n",
    "from fastapi.responses import PlainTextResponse\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import Runnable\n",
    "from pydantic import BaseModel, Field, validator, ValidationError\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from tqdm.auto import tqdm\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv\n",
    "from fastapi.responses import RedirectResponse\n",
    "import glob\n",
    "from fastapi import  UploadFile\n",
    "from fastapi.responses import JSONResponse\n",
    "from uuid import uuid4\n",
    "import time\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi import  Request\n",
    "from fastapi import  BackgroundTasks, UploadFile, Form\n",
    "import getpass\n",
    "import os\n",
    "import concurrent.futures\n",
    "from fastapi import  UploadFile\n",
    "from PyPDF2 import PdfReader  \n",
    "import docx\n",
    "import json\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "\n",
    "class MyModel(BaseModel):\n",
    "    message: BaseMessage\n",
    "class Config:\n",
    "    arbitrary_types_allowed = True\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"version:vac0.1\",\n",
    "    description=\"\")\n",
    "\n",
    "origins = [\"*\"]\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=origins,\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"])\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def add_process_time_header(request: Request, call_next):\n",
    "    start_time = time.perf_counter()\n",
    "    response = await call_next(request)\n",
    "    process_time = time.perf_counter() - start_time\n",
    "    response.headers[\"Processing-Time\"] = str(process_time)\n",
    "    return response\n",
    "\n",
    "######################### keys #############################\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pinecone_index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "langchain_project = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "langchain_endpoint = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "if langchain_api_key:\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = langchain_api_key\n",
    "if langchain_project:\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = langchain_project   \n",
    "if langchain_endpoint:\n",
    "    os.environ[\"LANGCHAIN_ENDPOINT\"] = langchain_endpoint  \n",
    "if not openai_api_key:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set. Please set it in your environment.\")\n",
    "if not pinecone_api_key:\n",
    "    raise EnvironmentError(\"PINECONE_API_KEY is not set. Please set it in your environment.\")\n",
    "if not pinecone_index_name:\n",
    "    raise EnvironmentError(\"PINECONE_INDEX_NAME is not set. Please set it in your environment.\")\n",
    "if openai_api_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "if pinecone_api_key:\n",
    "    os.environ[\"PINECONE_API_KEY\"] = pinecone_api_key\n",
    "if pinecone_index_name:\n",
    "    os.environ[\"PINECONE_INDEX_NAME\"] = pinecone_index_name\n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "############### vectordatabase initializations ####################\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "pinecone_index_name = os.environ.get(\"PINECONE_INDEX_NAME\")\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "pinecone_client = Pinecone(api_key=pinecone_api_key)\n",
    "def initialize_pinecone_index(client: Pinecone, index_name: str):\n",
    "    existing_indexes = [index_info[\"name\"] for index_info in client.list_indexes()]\n",
    "    if index_name not in existing_indexes:\n",
    "        client.create_index(\n",
    "            name=index_name,\n",
    "            dimension=3072,\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))\n",
    "        while not client.describe_index(index_name).status[\"ready\"]:\n",
    "            time.sleep(1)\n",
    "    return client.Index(index_name)\n",
    "index = initialize_pinecone_index(pinecone_client, pinecone_index_name)\n",
    "\n",
    "############################# upload #############################\n",
    "@app.post(\"/api/artificial-intelligence/upload\")\n",
    "async def upload_files(\n",
    "    tenantId: str = Form(...),\n",
    "    files: List[UploadFile] = File(...),\n",
    "    background_task: BackgroundTasks = BackgroundTasks()):\n",
    "    \"\"\"\n",
    "    Upload and process multiple document files\n",
    "        PDF, DOCX, and TXT files\n",
    "    Args:\n",
    "        tenantId: Identifier for the tenant\n",
    "        files: List of files to upload\n",
    "        background_task: Background task handler\n",
    "    Returns:\n",
    "        JSON response indicating upload status\n",
    "    Raises:\n",
    "        HTTPException: For invalid files or duplicates\n",
    "    \"\"\"\n",
    "    dir_name = str(uuid4())\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    allowed_extensions = {\".pdf\", \".docx\", \".txt\"}\n",
    "    fileName = set()\n",
    "    for file in files:\n",
    "        if file.filename in fileName:\n",
    "            raise HTTPException(\n",
    "                status_code=400, detail=f\"Duplicate file detected: {file.filename}\")\n",
    "        fileName.add(file.filename)\n",
    "        _, extension = os.path.splitext(file.filename)\n",
    "        if extension.lower() not in allowed_extensions:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=f\"Invalid file type: {file.filename}. Only PDF, DOCX, and TXT files are allowed.\",)\n",
    "        destination = os.path.join(dir_name, file.filename)\n",
    "        print('creating doc ' + destination)\n",
    "        with open(destination, \"wb\") as buffer:\n",
    "            shutil.copyfileobj(file.file, buffer)\n",
    "    docs = load_docs(dir_name, tenantId)\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    chunk_size = 800\n",
    "    chunk_overlap = 150\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    for doc in docs:\n",
    "        curr_split_docs = text_splitter.split_documents([doc])\n",
    "        document_id = str(uuid4())\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "        split_docs = split_docs + curr_split_docs\n",
    "        chunk_ids = chunk_ids + curr_chunk_ids\n",
    "        upload_documents[document_id] =  {\"fileName\": doc.metadata['filename'], \"id\": document_id, \"tenantId\": tenantId}\n",
    "    vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    shutil.rmtree(dir_name)\n",
    "    return {\"status\": \"success\", \"message\": \"Files uploaded successfully.\"}\n",
    "def load_docs(directory, tenantId):\n",
    "    loader = DirectoryLoader(directory)\n",
    "    docs = loader.load()\n",
    "    for doc in docs:\n",
    "        doc.metadata['tenantId'] = tenantId\n",
    "        doc.metadata['filename'] = os.path.basename(doc.metadata['source'])\n",
    "        print(\"ooooooooo\" + doc.metadata['filename'])\n",
    "    return docs\n",
    "\n",
    "############################# links@upload #############################\n",
    "class UploadLinkRequest(BaseModel):\n",
    "    tenantId: str\n",
    "    url: str \n",
    "\n",
    "@app.post(\"/api/artificial-intelligence/links\")\n",
    "async def upload_web_url(request: UploadLinkRequest, background_tasks: BackgroundTasks):\n",
    "    \"\"\"\n",
    "    Process and store web URL content.\n",
    "    \n",
    "    Args:\n",
    "        request: UploadLinkRequest containing tenantId and url.\n",
    "    \n",
    "    Returns:\n",
    "        JSON response indicating processing status.\n",
    "    \n",
    "    Raises:\n",
    "        HTTPException: When no valid url are provided.\n",
    "    \"\"\"\n",
    "    all_docs = []  \n",
    "    urls = request.url.split(\",\")  \n",
    "\n",
    "    for url in urls:\n",
    "        url = url.strip()\n",
    "        try:\n",
    "            async with AsyncWebCrawler(verbose=True) as crawler:\n",
    "                result = await crawler.arun(url=url)  # Running the crawler on the URL\n",
    "                extracted_content = result.markdown  # Extracted content in markdown format\n",
    "                doc = Document(page_content=extracted_content, metadata={\"source\": url, \"tenantId\": request.tenantId})\n",
    "                all_docs.append(doc)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading URL {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not all_docs:\n",
    "        raise HTTPException(status_code=400, detail=\"No valid URL provided or failed to fetch content.\")\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=os.getenv(\"PINECONE_INDEX_NAME\"), embedding=embeddings)\n",
    "    chunk_size = 3500\n",
    "    chunk_overlap = 1000\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    \n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    \n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    \n",
    "    document_id = str(uuid.uuid4())\n",
    "    for doc in all_docs:\n",
    "        curr_split_docs = text_splitter.split_documents([doc])\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "        split_docs.extend(curr_split_docs)\n",
    "        chunk_ids.extend(curr_chunk_ids)\n",
    "        upload_documents[document_id] = {\"source\": doc.metadata[\"source\"], \"id\": document_id, \"tenantId\": request.tenantId}\n",
    "    \n",
    "    vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "    \n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    \n",
    "    return {\"status\": \"success\", \"message\": \"URL processed and stored successfully.\", \"data\": {\"documentId\": document_id}}\n",
    "############################# links@edit #############################\n",
    "class UpdateLinkRequest(BaseModel):\n",
    "    tenantId: str\n",
    "    embeddedId: str\n",
    "    url: str \n",
    "@app.put(\"/api/artificial-intelligence/links\")\n",
    "async def update_web_url(request: UpdateLinkRequest):\n",
    "    \"\"\"\n",
    "    Update web URL content for a specific embeddedId.\n",
    "    Args:\n",
    "        request: UpdateLinkRequest containing tenantId, embeddedId, and url.\n",
    "    Returns:\n",
    "        JSON response indicating processing status.\n",
    "    Raises:\n",
    "        HTTPException: When no valid url are provided or if the embeddedId does not exist.\n",
    "    \"\"\"\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    if request.embeddedId not in upload_documents:\n",
    "        raise HTTPException(status_code=404, detail=\"The embeddedId does not exist.\")\n",
    "    try:\n",
    "        all_docs = []  \n",
    "        urls = request.url.split(\",\")  \n",
    "\n",
    "        for url in urls:\n",
    "            url = url.strip()\n",
    "            try:\n",
    "                async with AsyncWebCrawler(verbose=True) as crawler:\n",
    "                    result = await crawler.arun(url=url)  # Running the crawler on the URL\n",
    "                    extracted_content = result.markdown  # Extracted content in markdown format\n",
    "                    doc = Document(page_content=extracted_content, metadata={\"source\": url, \"tenantId\": request.tenantId})\n",
    "                    all_docs.append(doc)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading URL {url}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not all_docs:\n",
    "            raise HTTPException(status_code=400, detail=\"No valid URL provided or failed to fetch content.\")\n",
    "        \n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "        vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "        llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "        chunk_size = 3500\n",
    "        chunk_overlap = 1000\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        split_docs = []\n",
    "        chunk_ids = []\n",
    "        \n",
    "        existing_chunk_ids = [key for key in upload_documents if upload_documents[key]['id'] == request.embeddedId]\n",
    "        \n",
    "        for chunk_id in existing_chunk_ids:\n",
    "            del upload_documents[chunk_id]  # Removing old chunks\n",
    "            \n",
    "        ids_to_delete = [id for id in index.list(prefix=request.embeddedId)]\n",
    "        index.delete(ids=ids_to_delete)\n",
    "\n",
    "        for doc in all_docs:\n",
    "            curr_split_docs = text_splitter.split_documents([doc])\n",
    "            document_id = request.embeddedId  # same embeddedId\n",
    "            curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "            split_docs.extend(curr_split_docs)\n",
    "            chunk_ids.extend(curr_chunk_ids)\n",
    "            upload_documents[document_id] = {\"source\": doc.metadata[\"source\"], \"id\": document_id, \"tenantId\": request.tenantId}\n",
    "\n",
    "        vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump(upload_documents, f, indent=4)\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"URL updated successfully.\",\n",
    "            \"embeddedId\": request.embeddedId,\n",
    "            \"updated_url\": request.url\n",
    "        }   \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error updating url: {str(e)}\")\n",
    "############################# links@delete #############################\n",
    "class DeleteRequest(BaseModel):\n",
    "    embeddedId: str\n",
    "@app.delete(\"/api/artificial-intelligence/links\", summary=\"Delete Documents\", description=\"Delete all url documents in the Pinecone index that match the given prefix.\")\n",
    "async def delete_url(request: DeleteRequest):\n",
    "    \"\"\"Delete url documents matching prefix\n",
    "    Args:\n",
    "        request: Delete request containing prefix\n",
    "    Returns:\n",
    "        JSON response indicating deletion status\n",
    "    Raises:\n",
    "        HTTPException: When no matching url documents found\n",
    "    \"\"\"\n",
    "    prefix = request.embeddedId\n",
    "    ids_to_delete = [id for id in index.list(prefix=prefix)]\n",
    "    print(ids_to_delete)\n",
    "    if not ids_to_delete:\n",
    "        raise HTTPException(status_code=404, detail=\"No url documents found with the given prefix.\")\n",
    "    index.delete(ids=ids_to_delete)\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    if prefix in upload_documents:\n",
    "        del upload_documents[prefix]\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    return {\"message\": f\"Deleted {len(ids_to_delete)} documents with embeddedId '{prefix}'.\"}      \n",
    "############################# qa@upload #############################\n",
    "class QARequest(BaseModel):\n",
    "    tenantId: str\n",
    "    question: str\n",
    "    answer: str\n",
    "@app.post(\"/api/artificial-intelligence/qa\")\n",
    "async def upload_question_answer(\n",
    "    request: QARequest,\n",
    "    background_task: BackgroundTasks):\n",
    "    \"\"\"\n",
    "    Upload and store a question-answer pair\n",
    "    Args:\n",
    "        request: QA request containing question and answer\n",
    "        background_task: Background task handler    \n",
    "    Returns:\n",
    "        JSON response with QA details and status  \n",
    "    Raises:\n",
    "        HTTPException: For processing errors\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "    chunk_size = 20\n",
    "    chunk_overlap = 5\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    try:\n",
    "        document_id = str(uuid.uuid4())\n",
    "        combined_text = f\"Question: {request.question}\\nAnswer: {request.answer}\"\n",
    "        metadata = {\n",
    "            \"tenantId\": request.tenantId,\n",
    "            \"fileName\": f\"Q&A_input_{document_id}.txt\",\n",
    "            \"id\": document_id,\n",
    "            \"question\": request.question,  \n",
    "            \"answer\": request.answer}\n",
    "        document = Document(page_content=combined_text, metadata=metadata)\n",
    "        curr_split_docs = text_splitter.split_documents([document])\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "        split_docs += curr_split_docs\n",
    "        chunk_ids += curr_chunk_ids\n",
    "        upload_documents[document_id] = metadata\n",
    "        vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump(upload_documents, f, indent=4)\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"Question and Answer processed successfully.\",\n",
    "            \"id\": document_id,\n",
    "            \"question\": request.question,  \n",
    "            \"answer\": request.answer }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error processing the question and answer: {str(e)}\")\n",
    "    \n",
    "############################# qa@edit #############################\n",
    "class QAEditRequest(BaseModel):\n",
    "    tenantId: str\n",
    "    embeddedId: str\n",
    "    question: str\n",
    "    answer: str\n",
    "@app.put(\"/api/artificial-intelligence/qa\")\n",
    "async def edit_question_answer(\n",
    "    request: QAEditRequest,  \n",
    "    background_task: BackgroundTasks = BackgroundTasks()):\n",
    "    \"\"\"\n",
    "    Edit a question and answer pair by replacing the content in the same embeddedId.\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "    chunk_size = 20\n",
    "    chunk_overlap = 5\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    if request.embeddedId not in upload_documents:\n",
    "        raise HTTPException(status_code=404, detail=\"The embeddedId does not exist.\")\n",
    "    try:\n",
    "        ids_to_delete = [id for id in index.list(prefix=request.embeddedId)]\n",
    "        index.delete(ids=ids_to_delete)\n",
    "        combined_text = f\"Question: {request.question}\\nAnswer: {request.answer}\"\n",
    "        metadata = {\n",
    "            \"tenantId\": request.tenantId,\n",
    "            \"fileName\": f\"Q&A_input_{request.embeddedId}.txt\",\n",
    "            \"id\": request.embeddedId,\n",
    "            \"question\": request.question,\n",
    "            \"answer\": request.answer}\n",
    "        document = Document(page_content=combined_text, metadata=metadata)\n",
    "        curr_split_docs = text_splitter.split_documents([document])\n",
    "        curr_chunk_ids = [f\"{request.embeddedId}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "        vectorstore.add_documents(documents=curr_split_docs, ids=curr_chunk_ids)\n",
    "        upload_documents[request.embeddedId] = metadata\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump(upload_documents, f, indent=4)\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"Question and Answer edited successfully.\",\n",
    "            \"id\": request.embeddedId,\n",
    "            \"question\": request.question,\n",
    "            \"answer\": request.answer }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error editing the question and answer: {str(e)}\")\n",
    "\n",
    "####################################### qa@delete ##########################\n",
    "class DeleteRequest(BaseModel):\n",
    "    prefix: str\n",
    "@app.delete(\"/api/artificial-intelligence/qa\", summary=\"Delete Question Answer\", description=\"Delete all documents in the Pinecone index that match the given prefix.\")\n",
    "async def delete_documents(request: DeleteRequest):\n",
    "    \"\"\"Delete Question Answer matching prefix\n",
    "    Args:\n",
    "        request: Delete request containing prefix\n",
    "    Returns:\n",
    "        JSON response indicating deletion status\n",
    "    Raises:\n",
    "        HTTPException: When no matching documents found\n",
    "    \"\"\"\n",
    "    prefix = request.prefix\n",
    "    ids_to_delete = [id for id in index.list(prefix=prefix)]\n",
    "    print(ids_to_delete)\n",
    "    if not ids_to_delete:\n",
    "        raise HTTPException(status_code=404, detail=\"No QA found with the given prefix.\")\n",
    "    index.delete(ids=ids_to_delete)\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    if prefix in upload_documents:\n",
    "        del upload_documents[prefix]\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    return {\"message\": f\"Deleted {len(ids_to_delete)} QA with prefix '{prefix}'.\"}\n",
    "\n",
    "\n",
    "############################# tenant_files #############################\n",
    "# @app.get(\"/api/artificial-intelligence/tenant_files\")\n",
    "# async def retrieve_files(tenantId: str = Query(...)):\n",
    "#     uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "#     if not os.path.exists(uploaded_documents_path):\n",
    "#         with open(uploaded_documents_path, \"w\") as f:\n",
    "#             json.dump({}, f)\n",
    "#     with open(uploaded_documents_path, \"r\") as f:\n",
    "#         upload_documents = json.load(f)\n",
    "#     tenantFiles = []\n",
    "#     for item in list(upload_documents.values()):\n",
    "#         if item.get('tenantId') == tenantId:\n",
    "#             tenantFiles.append(item)\n",
    "#     return {\"data\": tenantFiles}\n",
    "\n",
    "@app.get(\"/api/artificial-intelligence/tenant_files\")\n",
    "async def retrieve_files(tenantId: str = Query(...)):\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    tenantFiles = []\n",
    "    for item in list(upload_documents.values()):\n",
    "        # Only include items that have either 'fileName' (from /upload) \n",
    "        # or 'source' (from /links) but not from /qa\n",
    "        if (item.get('tenantId') == tenantId and \n",
    "            ('fileName' in item) and \n",
    "            not item.get('fileName', '').startswith('Q&A_input_')):\n",
    "            tenantFiles.append(item)\n",
    "    return {\"data\": tenantFiles}\n",
    "\n",
    "############################# prompts #############################\n",
    "def initializeVectorStore():\n",
    "    \"\"\"Initialize and configure vector store with embeddings\n",
    "    Returns:\n",
    "        Configured PineconeVectorStore instance\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0\n",
    ")\n",
    "@app.get(\"/api/artificial-intelligence/prompts\")\n",
    "async def prompts_keyword(tenantId: str = Query(...), keyword: str = Query(...)):\n",
    "    \"\"\"Process keyword prompts and retrieve answers using improved RAG chain\n",
    "    Args:\n",
    "        tenantId: Identifier for the tenant\n",
    "        keyword: Search keyword/prompt  \n",
    "    Returns:\n",
    "        PlainText response with answer or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(tenantId, keyword)\n",
    "        vectorstore = initializeVectorStore()\n",
    "        template = \"\"\"Answer the question based only on the following context:\n",
    "        {context}\n",
    "        User Question:\n",
    "        {question}\n",
    "         If the user greets you, respond with one of the following without searching information from knowledge base:\n",
    "        - \"Hello! How can I assist you today?\"\n",
    "        - \"Hi there! What can I do for you?\"\n",
    "        - \"Good day! How can I assist you?\"\n",
    "        - \"Hey! What can I help you with?\"\n",
    "        - \"Hi! How can I support you today?\"\n",
    "        - \"Welcome! How may I help you?\"\n",
    "        - \"Salutations! How can I assist you?\"\n",
    "        - \"What's up? How can I help?\"\n",
    "        - \"Good to see you! How can I assist?\"\n",
    "\n",
    "        Please provide a concise and accurate answer based on the context above. If the context does not contain  information to answer the question, respond with: \"I don't have the information you're looking for, please provide additional details.\" And if it's greetings then show greetings response.\n",
    "        Make sure your answer is relevant and accurate to the question and does not repeat the question itself..\n",
    "        In response give only answer but not question.\n",
    "        \"\"\"\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\n",
    "                \"k\": 1,\n",
    "                \"filter\": {\n",
    "                    'tenantId': {'$eq': tenantId}\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        rag_chain = (\n",
    "            {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser())\n",
    "        answer = rag_chain.invoke(keyword)\n",
    "        return PlainTextResponse(answer)\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing your request: {str(e)}. Please contact support for assistance.\"\n",
    "        print(f\"Error in prompts_keyword: {str(e)}\") \n",
    "        return PlainTextResponse(error_msg, status_code=500)\n",
    "\n",
    "############################ Summarizer #################################################\n",
    "class SummarizationRequest(BaseModel):\n",
    "    caseId: str\n",
    "    assignedTo: str\n",
    "    patientName: str\n",
    "    onBehalfOf: str\n",
    "    currentCaseStatus: str\n",
    "    summarizationType: str\n",
    "    notes: List[Any]\n",
    "\n",
    "@app.post(\"/api/artificial-intelligence/summarize\")\n",
    "async def summarize(request: SummarizationRequest):\n",
    "    \"\"\"Combine JSON upload and summary generation into one endpoint.\"\"\"\n",
    "    try:\n",
    "        json_data = {\"caseId\": request.caseId, \"notes\": request.notes, \"patientName\": request.patientName, \"assignedTo\": request.assignedTo, \"currentCaseStatus\": request.currentCaseStatus}\n",
    "        noteLength = sum(len(note['note']) for note in request.notes)\n",
    "        maxTokenToUse = 500\n",
    "        \n",
    "        if(noteLength <= 10):\n",
    "            maxTokenToUse=200\n",
    "        \n",
    "        \n",
    "        json_doc = Document(page_content=json.dumps(json_data), metadata={'fileName': 'query_json'})\n",
    "        split_docs = [json_doc]\n",
    "        print(f\"Processing {len(split_docs)} documents.\")\n",
    "        \n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.0,\n",
    "            max_tokens=maxTokenToUse,\n",
    "            top_p=1.0,\n",
    "        )\n",
    "\n",
    "        full_summary_prompt_template = f\"\"\"\n",
    "        \"{{split_docs}}\"\n",
    "\n",
    "        Please summarize the following notes directly to me, the {request.onBehalfOf} in a structured format, focusing on key actions and communications. Include the following sections:\n",
    "        \n",
    "        1. Initial Request: Briefly explain the initial issue or request.\n",
    "        2. Key Updates: Highlight major actions taken, communications, and progress in chronological order, including dates.\n",
    "        3. Current Status: Summarize the present situation or outcome of the case.\n",
    "\n",
    "        Do not expose any ids\n",
    "\n",
    "        Complete the sentence at the end with a full stop.\n",
    "        Give html format\n",
    "        \"\"\"\n",
    "\n",
    "        unread_summary_prompt_template = f\"\"\"\n",
    "        \"{{split_docs}}\"\n",
    "\n",
    "        Please summarize the following case notes directly to me, the {request.onBehalfOf} in a paragraph format.\n",
    "        \n",
    "        Do not expose any ids\n",
    "        Keep it simple and short.\n",
    "        Complete the sentence at the end with a full stop.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        prompt=\"\"\n",
    "        if(request.summarizationType == 'full'):\n",
    "            prompt = full_summary_prompt_template\n",
    "        else:\n",
    "            prompt = unread_summary_prompt_template\n",
    "        \n",
    "        print(prompt)\n",
    "\n",
    "        prompt = PromptTemplate.from_template(prompt)\n",
    "        llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "        stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"split_docs\")\n",
    "        summary = stuff_chain.run(split_docs)\n",
    "\n",
    "        sentences = summary\n",
    "        print(sentences)\n",
    "\n",
    "        # if len(sentences) > 1:\n",
    "        #     summary = '.'.join(sentences[:-1]).strip()  \n",
    "        # else:\n",
    "        #     summary = sentences[0].strip()  \n",
    "\n",
    "        # if not summary.endswith('.'):\n",
    "        #     summary += '.'\n",
    "        \n",
    "        summary = sentences.replace(\"\\n\", \" \")\n",
    "        summary = summary.replace(\"```html\", \" \")\n",
    "        return JSONResponse(content={\n",
    "            \"summary\": summary\n",
    "        })\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error during summarizing the documents: {str(e)}\")\n",
    "\n",
    "####################################### delete #########################\n",
    "class DeleteRequest(BaseModel):\n",
    "    prefix: str\n",
    "@app.delete(\"/api/artificial-intelligence/delete\", summary=\"Delete Documents\", description=\"Delete all documents in the Pinecone index that match the given prefix.\")\n",
    "async def delete_documents(request: DeleteRequest):\n",
    "    \"\"\"Delete documents matching prefix\n",
    "    Args:\n",
    "        request: Delete request containing prefix\n",
    "    Returns:\n",
    "        JSON response indicating deletion status\n",
    "    Raises:\n",
    "        HTTPException: When no matching documents found\n",
    "    \"\"\"\n",
    "    prefix = request.prefix\n",
    "    ids_to_delete = [id for id in index.list(prefix=prefix)]\n",
    "    print(ids_to_delete)\n",
    "    if not ids_to_delete:\n",
    "        raise HTTPException(status_code=404, detail=\"No documents found with the given prefix.\")\n",
    "    index.delete(ids=ids_to_delete)\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    if prefix in upload_documents:\n",
    "        del upload_documents[prefix]\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    return {\"message\": f\"Deleted {len(ids_to_delete)} documents with prefix '{prefix}'.\"}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# links@upload #############################\n",
    "class UploadLinkRequest(BaseModel):\n",
    "    tenantId: str\n",
    "    url: str \n",
    "\n",
    "@app.post(\"/api/artificial-intelligence/links\")\n",
    "async def upload_web_url(request: UploadLinkRequest, background_tasks: BackgroundTasks):\n",
    "    \"\"\"\n",
    "    Process and store web URL content.\n",
    "    \n",
    "    Args:\n",
    "        request: UploadLinkRequest containing tenantId and url.\n",
    "    \n",
    "    Returns:\n",
    "        JSON response indicating processing status.\n",
    "    \n",
    "    Raises:\n",
    "        HTTPException: When no valid url are provided.\n",
    "    \"\"\"\n",
    "    all_docs = []  \n",
    "    urls = request.url.split(\",\")  \n",
    "\n",
    "    for url in urls:\n",
    "        url = url.strip()\n",
    "        try:\n",
    "            async with AsyncWebCrawler(verbose=True) as crawler:\n",
    "                result = await crawler.arun(url=url)  # Running the crawler on the URL\n",
    "                extracted_content = result.markdown  # Extracted content in markdown format\n",
    "                doc = Document(page_content=extracted_content, metadata={\"source\": url, \"tenantId\": request.tenantId})\n",
    "                all_docs.append(doc)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading URL {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not all_docs:\n",
    "        raise HTTPException(status_code=400, detail=\"No valid URL provided or failed to fetch content.\")\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=os.getenv(\"PINECONE_INDEX_NAME\"), embedding=embeddings)\n",
    "    chunk_size = 4500\n",
    "    chunk_overlap = 1000\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    \n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    \n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    \n",
    "    document_id = str(uuid.uuid4())\n",
    "    for doc in all_docs:\n",
    "        curr_split_docs = text_splitter.split_documents([doc])\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "        split_docs.extend(curr_split_docs)\n",
    "        chunk_ids.extend(curr_chunk_ids)\n",
    "        upload_documents[document_id] = {\"source\": doc.metadata[\"source\"], \"id\": document_id, \"tenantId\": request.tenantId}\n",
    "    \n",
    "    vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "    \n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    \n",
    "    return {\"status\": \"success\", \"message\": \"URL processed and stored successfully.\", \"data\": {\"documentId\": document_id}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        full_summary_prompt_template = f\"\"\"\n",
    "        \"{{split_docs}}\"\n",
    "\n",
    "        Please summarize the following notes directly to me, the {request.onBehalfOf} in a structured format, focusing on key actions and communications. Include the following sections:\n",
    "        \n",
    "        1. Initial Request: Briefly explain the initial issue or request.\n",
    "        2. Key Updates: Highlight major actions taken, communications, and progress in chronological order, including dates.\n",
    "        3. Current Status: Summarize the present situation or outcome of the case.\n",
    "\n",
    "        Do not expose any ids\n",
    "\n",
    "        Complete the sentence at the end with a full stop.\n",
    "        Give html format\n",
    "        Don't give farewell messages like 'End of summary' or 'Thank you for your attention to this matter' at the end of summary.\n",
    "        \"\"\"\n",
    "\n",
    "        unread_summary_prompt_template = f\"\"\"\n",
    "        \"{{split_docs}}\"\n",
    "\n",
    "        Please summarize the following case notes directly to me, the {request.onBehalfOf} in a paragraph format.\n",
    "        \n",
    "        Do not expose any ids\n",
    "        Keep it simple and short.\n",
    "        Complete the sentence at the end with a full stop.\n",
    "        Don't give farewell messages like 'End of summary' or 'Thank you for your attention to this matter' at the end of summary.\n",
    "\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " template = \"\"\"Answer the question based only on the following context:\n",
    "        {context}\n",
    "        User Question:\n",
    "        {question}\n",
    "         If the user greets you, respond with one of the following without searching information from knowledge base:\n",
    "        - \"Hello! How can I assist you today?\"\n",
    "        - \"Hi there! What can I do for you?\"\n",
    "        - \"Good day! How can I assist you?\"\n",
    "        - \"Hey! What can I help you with?\"\n",
    "        - \"Hi! How can I support you today?\"\n",
    "        - \"Welcome! How may I help you?\"\n",
    "        - \"Salutations! How can I assist you?\"\n",
    "        - \"What's up? How can I help?\"\n",
    "        - \"Good to see you! How can I assist?\"\n",
    "\n",
    "        Please provide a concise and accurate answer based on the context above. If the context does not contain  information to answer the question, respond with: \"I don't have the information you're looking for, please provide additional details.\" And if it's greetings then give greetings response.\n",
    "        Make sure your answer is relevant and accurate to the question and does not repeat the question itself..\n",
    "        In response give only answer but not question.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.responses import StreamingResponse\n",
    "from pydantic import BaseModel\n",
    "import asyncio\n",
    "from typing import Optional, Dict, Any\n",
    "from contextlib import asynccontextmanager\n",
    "import json\n",
    "import time\n",
    "# Import necessary functions from webrover.py\n",
    "#from .webrover import setup_browser_2, main_agent_graph\n",
    "import webrover\n",
    "from webrover import * \n",
    "\n",
    "\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"http://localhost:3000\"],  # Your Next.js app URL\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Global variable to store browser session\n",
    "browser_session: Dict[str, Any] = {\n",
    "    \"playwright\": None,\n",
    "    \"browser\": None,\n",
    "    \"page\": None\n",
    "}\n",
    "\n",
    "# Global queue for browser events\n",
    "browser_events = asyncio.Queue()\n",
    "\n",
    "class BrowserSetupRequest(BaseModel):\n",
    "    url: str = \"https://www.google.com\"\n",
    "\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "\n",
    "@app.post(\"/setup-browser\")\n",
    "async def setup_browser(request: BrowserSetupRequest):\n",
    "    try:\n",
    "        # Clear any existing session\n",
    "        if browser_session[\"playwright\"]:\n",
    "            await cleanup_browser()\n",
    "            \n",
    "        # Setup new browser session\n",
    "        playwright, browser, page = await setup_browser_2(request.url)\n",
    "        \n",
    "        # Store session info\n",
    "        browser_session.update({\n",
    "            \"playwright\": playwright,\n",
    "            \"browser\": browser,\n",
    "            \"page\": page\n",
    "        })\n",
    "        \n",
    "        return {\"status\": \"success\", \"message\": \"Browser setup complete\"}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Failed to setup browser: {str(e)}\")\n",
    "\n",
    "@app.post(\"/cleanup\")\n",
    "async def cleanup_browser():\n",
    "    try:\n",
    "        if browser_session[\"page\"]:\n",
    "            await browser_session[\"page\"].close()\n",
    "        if browser_session[\"browser\"]:\n",
    "            await browser_session[\"browser\"].close()\n",
    "        if browser_session[\"playwright\"]:\n",
    "            await browser_session[\"playwright\"].stop()\n",
    "            \n",
    "        # Reset session\n",
    "        browser_session.update({\n",
    "            \"playwright\": None,\n",
    "            \"browser\": None,\n",
    "            \"page\": None\n",
    "        })\n",
    "        \n",
    "        return {\"status\": \"success\", \"message\": \"Browser cleanup complete\"}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Failed to cleanup browser: {str(e)}\")\n",
    "\n",
    "async def emit_browser_event(event_type: str, data: Dict[str, Any]):\n",
    "    await browser_events.put({\n",
    "        \"type\": event_type,\n",
    "        \"data\": data\n",
    "    })\n",
    "\n",
    "@app.get(\"/browser-events\")\n",
    "async def browser_events_endpoint():\n",
    "    async def event_generator():\n",
    "        while True:\n",
    "            try:\n",
    "                event = await browser_events.get()\n",
    "                yield f\"data: {json.dumps(event)}\\n\\n\"\n",
    "            except asyncio.CancelledError:\n",
    "                break\n",
    "    \n",
    "    return StreamingResponse(\n",
    "        event_generator(),\n",
    "        media_type=\"text/event-stream\",\n",
    "        headers={\n",
    "            \"Cache-Control\": \"no-cache\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "async def stream_agent_response(query: str, page):\n",
    "    try:\n",
    "        initial_state = {\n",
    "            \"input\": query,\n",
    "            \"page\": page,\n",
    "            \"image\": \"\",\n",
    "            \"master_plan\": None,\n",
    "            \"bboxes\": [],\n",
    "            \"actions_taken\": [],\n",
    "            \"action\": None,\n",
    "            \"last_action\": \"\",\n",
    "            \"notes\": [],\n",
    "            \"answer\": \"\"\n",
    "        }\n",
    "        \n",
    "        # Keep track of last event for potential retries\n",
    "        last_event = None\n",
    "        retry_count = 0\n",
    "        max_retries = 3\n",
    "        \n",
    "        async for event in main_agent_graph.astream(\n",
    "            initial_state,\n",
    "             {\"recursion_limit\": 400}\n",
    "            ):\n",
    "            try:\n",
    "                # Send periodic keepalive to prevent timeout\n",
    "                yield f\"data: {{\\n  \\\"type\\\": \\\"keepalive\\\",\\n  \\\"timestamp\\\": {time.time()}\\n}}\\n\\n\"\n",
    "                \n",
    "                if isinstance(event, dict):\n",
    "                    last_event = event\n",
    "                    \n",
    "                    if \"parse_action\" in event:\n",
    "                        action = event[\"parse_action\"][\"action\"]\n",
    "                        thought = event[\"parse_action\"][\"notes\"][-1]\n",
    "                        \n",
    "                        # Ensure proper encoding and escaping of JSON\n",
    "                        thought_json = json.dumps(thought, ensure_ascii=False)\n",
    "                        yield f\"data: {{\\n  \\\"type\\\": \\\"thought\\\",\\n  \\\"content\\\": {thought_json}\\n}}\\n\\n\"\n",
    "                        \n",
    "                        if isinstance(action, dict):\n",
    "                            action_json = json.dumps(action, ensure_ascii=False)\n",
    "                            yield f\"data: {{\\n  \\\"type\\\": \\\"action\\\",\\n  \\\"content\\\": {action_json}\\n}}\\n\\n\"\n",
    "                            \n",
    "                            # Handle browser events\n",
    "                            action_type = action.get(\"action\", \"\")\n",
    "                            if action_type == \"goto\":\n",
    "                                await emit_browser_event(\"navigation\", {\n",
    "                                    \"url\": action[\"args\"],\n",
    "                                    \"status\": \"loading\"\n",
    "                                })\n",
    "                    \n",
    "                    if \"answer_node\" in event:\n",
    "                        answer = event[\"answer_node\"][\"answer\"]\n",
    "                        answer_json = json.dumps(answer, ensure_ascii=False)\n",
    "                        yield f\"data: {{\\n  \\\"type\\\": \\\"final_answer\\\",\\n  \\\"content\\\": {answer_json}\\n}}\\n\\n\"\n",
    "                        \n",
    "                    # Reset retry count on successful event\n",
    "                    retry_count = 0\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing event: {str(e)}\")\n",
    "                retry_count += 1\n",
    "                if retry_count <= max_retries and last_event:\n",
    "                    # Retry last event\n",
    "                    yield f\"data: {{\\n  \\\"type\\\": \\\"retry\\\",\\n  \\\"content\\\": \\\"Retrying last action...\\\"\\n}}\\n\\n\"\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "                    \n",
    "    except Exception as e:\n",
    "        error_json = json.dumps(str(e), ensure_ascii=False)\n",
    "        yield f\"data: {{\\n  \\\"type\\\": \\\"error\\\",\\n  \\\"content\\\": {error_json}\\n}}\\n\\n\"\n",
    "    finally:\n",
    "        # Ensure proper stream closure\n",
    "        yield f\"data: {{\\n  \\\"type\\\": \\\"end\\\",\\n  \\\"content\\\": \\\"Stream completed\\\"\\n}}\\n\\n\"\n",
    "\n",
    "@app.post(\"/query\")\n",
    "async def query_agent(request: QueryRequest):\n",
    "    if not browser_session[\"page\"]:\n",
    "        raise HTTPException(status_code=400, detail=\"Browser not initialized. Call /setup-browser first\")\n",
    "    \n",
    "    return StreamingResponse(\n",
    "        stream_agent_response(request.query, browser_session[\"page\"]),\n",
    "        media_type=\"text/event-stream\"\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "except Exception as e:\n",
    "    print(f\"Warning: NLTK download failed, but continuing execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# prompts #############################\n",
    "def initializeVectorStore():\n",
    "    \"\"\"Initialize and configure vector store with embeddings\n",
    "    Returns:\n",
    "        Configured PineconeVectorStore instance\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0\n",
    ")\n",
    "@app.get(\"/api/artificial-intelligence/prompts\")\n",
    "async def prompts_keyword(tenantId: str = Query(...), keyword: str = Query(...)):\n",
    "    \"\"\"Process keyword prompts and retrieve answers using improved RAG chain\n",
    "    Args:\n",
    "        tenantId: Identifier for the tenant\n",
    "        keyword: Search keyword/prompt  \n",
    "    Returns:\n",
    "        PlainText response with answer or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(tenantId, keyword)\n",
    "        vectorstore = initializeVectorStore()\n",
    "        template = \"\"\"Answer the question based only on the following context:\n",
    "        {context}\n",
    "        User Question:\n",
    "        {question}\n",
    "         If the user greets you, respond with one of the following without searching information from knowledge base:\n",
    "        - \"Hello! How can I assist you today?\"\n",
    "        - \"Hi there! What can I do for you?\"\n",
    "        - \"Good day! How can I assist you?\"\n",
    "        - \"Hey! What can I help you with?\"\n",
    "        - \"Hi! How can I support you today?\"\n",
    "        - \"Welcome! How may I help you?\"\n",
    "        - \"Salutations! How can I assist you?\"\n",
    "        - \"What's up? How can I help?\"\n",
    "        - \"Good to see you! How can I assist?\"\n",
    "\n",
    "        Instructions for answering:\n",
    "        1. Extract relevant information directly from the web content\n",
    "        2. If the question is about specific data or facts, cite them accurately\n",
    "        3. If the content contains multiple relevant sections, synthesize them into a coherent answer\n",
    "        4. For technical or complex topics, provide clear, simplified explanations\n",
    "        5. If the information is not available in the context, respond with: \"This information is not available in the source content. Please try a different question or provide more details.\"\n",
    "        6. For greetings, use one of the greeting responses above\n",
    "        7. If the user asks questions that are synonyms or rephrased versions of questions in URL links, provide relevant answers based on the intent of the question, not just the exact wording.\n",
    "        8. When answering, focus on matching the intent of the user's question, even if the exact words are not present in the context. Use contextual understanding to infer the meaning and provide accurate responses.\n",
    "        9. If the question is ambiguous or unclear, ask for clarification to ensure the response aligns with the user's intent.\n",
    "        10. For numerical data or specific metrics (e.g., measurements, statistics, or specifications), provide the exact match from the context.\n",
    "        Remember:- For example: The secondary sensor of the Google Pixel 9 with 48 MP ultra-wide-angle, f2.2 and field of view (FOV) is 125.8˚and\n",
    "                                 The secondary sensor of the Galaxy S25 with 12 MP ultra-wide-angle, f/2.2 aperture and field of view (FOV) is 120˚ \n",
    "        \"\"\"\n",
    "\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\n",
    "                \"k\": 5,  \n",
    "                \"filter\": {\n",
    "                    'tenantId': {'$eq': tenantId}\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Getting initial documents\n",
    "        docs = retriever.get_relevant_documents(keyword)\n",
    "\n",
    "        # Preparing documents for BM25 reranking\n",
    "        corpus = [doc.page_content for doc in docs]\n",
    "        tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]\n",
    "        bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "        #  BM25 scores for the keyword\n",
    "        tokenized_query = word_tokenize(keyword.lower())\n",
    "        bm25_scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "        # Combining vector similarity with BM25 scores\n",
    "        reranked_pairs = list(zip(docs, bm25_scores))\n",
    "        reranked_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Taking top document after reranking\n",
    "        reranked_docs = [pair[0] for pair in reranked_pairs[:1]]\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        rag_chain = (\n",
    "            {\"context\": lambda x: reranked_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser())\n",
    "        \n",
    "        answer = rag_chain.invoke(keyword)\n",
    "        return PlainTextResponse(answer)\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing your request: {str(e)}. Please contact support for assistance.\"\n",
    "        print(f\"Error in prompts_keyword: {str(e)}\") \n",
    "        return PlainTextResponse(error_msg, status_code=500)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "#from langserve import add_routes\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "from fastapi import UploadFile, File, HTTPException\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from enum import Enum\n",
    "from typing import Any\n",
    "import uuid\n",
    "import pylibmagic\n",
    "from fastapi import Query\n",
    "import random\n",
    "import io\n",
    "import docx\n",
    "import shutil\n",
    "import uvicorn\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.retrievers import PineconeHybridSearchRetriever\n",
    "from fastapi.responses import PlainTextResponse\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import Runnable\n",
    "from pydantic import BaseModel, Field, validator, ValidationError\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from tqdm.auto import tqdm\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv\n",
    "from fastapi.responses import RedirectResponse\n",
    "import glob\n",
    "from fastapi import  UploadFile\n",
    "from fastapi.responses import JSONResponse\n",
    "from uuid import uuid4\n",
    "import time\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi import  Request\n",
    "from fastapi import  BackgroundTasks, UploadFile, Form\n",
    "import getpass\n",
    "import os\n",
    "import concurrent.futures\n",
    "from fastapi import  UploadFile\n",
    "from PyPDF2 import PdfReader  \n",
    "import docx\n",
    "import json\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "class MyModel(BaseModel):\n",
    "    message: BaseMessage\n",
    "class Config:\n",
    "    arbitrary_types_allowed = True\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"version:vac0.1\",\n",
    "    description=\"\")\n",
    "\n",
    "origins = [\"*\"]\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=origins,\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"])\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def add_process_time_header(request: Request, call_next):\n",
    "    start_time = time.perf_counter()\n",
    "    response = await call_next(request)\n",
    "    process_time = time.perf_counter() - start_time\n",
    "    response.headers[\"Processing-Time\"] = str(process_time)\n",
    "    return response\n",
    "\n",
    "######################### keys #############################\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pinecone_index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set. Please set it in your environment.\")\n",
    "if not pinecone_api_key:\n",
    "    raise EnvironmentError(\"PINECONE_API_KEY is not set. Please set it in your environment.\")\n",
    "if not pinecone_index_name:\n",
    "    raise EnvironmentError(\"PINECONE_INDEX_NAME is not set. Please set it in your environment.\")\n",
    "if openai_api_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "if pinecone_api_key:\n",
    "    os.environ[\"PINECONE_API_KEY\"] = pinecone_api_key\n",
    "if pinecone_index_name:\n",
    "    os.environ[\"PINECONE_INDEX_NAME\"] = pinecone_index_name\n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "############### vectordatabase initializations ####################\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "pinecone_index_name = os.environ.get(\"PINECONE_INDEX_NAME\")\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "pinecone_client = Pinecone(api_key=pinecone_api_key)\n",
    "def initialize_pinecone_index(client: Pinecone, index_name: str):\n",
    "    existing_indexes = [index_info[\"name\"] for index_info in client.list_indexes()]\n",
    "    if index_name not in existing_indexes:\n",
    "        client.create_index(\n",
    "            name=index_name,\n",
    "            dimension=3072,\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))\n",
    "        while not client.describe_index(index_name).status[\"ready\"]:\n",
    "            time.sleep(1)\n",
    "    return client.Index(index_name)\n",
    "index = initialize_pinecone_index(pinecone_client, pinecone_index_name)\n",
    "\n",
    "############################# upload #############################\n",
    "@app.post(\"/api/artificial-intelligence/upload\")\n",
    "async def upload_files(\n",
    "    tenantId: str = Form(...),\n",
    "    files: List[UploadFile] = File(...),\n",
    "    background_task: BackgroundTasks = BackgroundTasks()):\n",
    "    \"\"\"\n",
    "    Upload and process multiple document files\n",
    "        PDF, DOCX, and TXT files\n",
    "    Args:\n",
    "        tenantId: Identifier for the tenant\n",
    "        files: List of files to upload\n",
    "        background_task: Background task handler\n",
    "    Returns:\n",
    "        JSON response indicating upload status\n",
    "    Raises:\n",
    "        HTTPException: For invalid files or duplicates\n",
    "    \"\"\"\n",
    "    dir_name = str(uuid4())\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    allowed_extensions = {\".pdf\", \".docx\", \".txt\"}\n",
    "    fileName = set()\n",
    "    for file in files:\n",
    "        if file.filename in fileName:\n",
    "            raise HTTPException(\n",
    "                status_code=400, detail=f\"Duplicate file detected: {file.filename}\")\n",
    "        fileName.add(file.filename)\n",
    "        _, extension = os.path.splitext(file.filename)\n",
    "        if extension.lower() not in allowed_extensions:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=f\"Invalid file type: {file.filename}. Only PDF, DOCX, and TXT files are allowed.\",)\n",
    "        destination = os.path.join(dir_name, file.filename)\n",
    "        print('creating doc ' + destination)\n",
    "        with open(destination, \"wb\") as buffer:\n",
    "            shutil.copyfileobj(file.file, buffer)\n",
    "    docs = load_docs(dir_name, tenantId)\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    chunk_size = 800\n",
    "    chunk_overlap = 150\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    for doc in docs:\n",
    "        curr_split_docs = text_splitter.split_documents([doc])\n",
    "        document_id = str(uuid4())\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "        split_docs = split_docs + curr_split_docs\n",
    "        chunk_ids = chunk_ids + curr_chunk_ids\n",
    "        upload_documents[document_id] =  {\"fileName\": doc.metadata['filename'], \"id\": document_id, \"tenantId\": tenantId}\n",
    "    vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    shutil.rmtree(dir_name)\n",
    "    return {\"status\": \"success\", \"message\": \"Files uploaded successfully.\"}\n",
    "def load_docs(directory, tenantId):\n",
    "    loader = DirectoryLoader(directory)\n",
    "    docs = loader.load()\n",
    "    for doc in docs:\n",
    "        doc.metadata['tenantId'] = tenantId\n",
    "        doc.metadata['filename'] = os.path.basename(doc.metadata['source'])\n",
    "        print(\"ooooooooo\" + doc.metadata['filename'])\n",
    "    return docs\n",
    "\n",
    "############################# links@upload #############################\n",
    "class UploadLinkRequest(BaseModel):\n",
    "    tenantId: str\n",
    "    url: str \n",
    "    name: str\n",
    "\n",
    "class DocumentType(str, Enum):\n",
    "    FILE = \"File\"\n",
    "    LINK = \"Link\"\n",
    "\n",
    "@app.post(\"/api/artificial-intelligence/links\")\n",
    "async def upload_web_url(request: UploadLinkRequest, background_tasks: BackgroundTasks):\n",
    "    \"\"\"\n",
    "    Process and store web URL content.\n",
    "    \n",
    "    Args:\n",
    "        request: UploadLinkRequest containing tenantId and url.\n",
    "    \n",
    "    Returns:\n",
    "        JSON response indicating processing status.\n",
    "    \n",
    "    Raises:\n",
    "        HTTPException: When no valid url are provided.\n",
    "    \"\"\"\n",
    "    all_docs = []  \n",
    "    urls = request.url.split(\",\")  \n",
    "\n",
    "    for url in urls:\n",
    "        url = url.strip()\n",
    "        try:\n",
    "            async with AsyncWebCrawler(verbose=True) as crawler:\n",
    "                result = await crawler.arun(url=url)  # Running the crawler on the URL\n",
    "                extracted_content = result.markdown  # Extracted content in markdown format\n",
    "                doc = Document(page_content=extracted_content, metadata={\"source\": url, \"fileName\":request.name, \"tenantId\": request.tenantId,\"type\": DocumentType.LINK })\n",
    "                all_docs.append(doc)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading URL {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not all_docs:\n",
    "        raise HTTPException(status_code=400, detail=\"No valid URL provided or failed to fetch content.\")\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=os.getenv(\"PINECONE_INDEX_NAME\"), embedding=embeddings)\n",
    "    chunk_size = 12500\n",
    "    chunk_overlap = 1000\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    \n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    \n",
    "    document_id = str(uuid.uuid4())\n",
    "    for doc in all_docs:\n",
    "        curr_split_docs = text_splitter.split_documents([doc])\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "        split_docs.extend(curr_split_docs)\n",
    "        chunk_ids.extend(curr_chunk_ids)\n",
    "        upload_documents[document_id] = {\"source\": doc.metadata[\"source\"],\"fileName\":request.name, \"id\": document_id, \"tenantId\": request.tenantId, \"type\": DocumentType.LINK}\n",
    "    \n",
    "    vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "    \n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    \n",
    "    return {\"status\": \"success\", \"message\": \"URL processed and stored successfully.\", \"data\": {\"documentId\": document_id}}\n",
    "############################# links@edit #############################\n",
    "class UpdateLinkRequest(BaseModel):\n",
    "    tenantId: str\n",
    "    embeddedId: str\n",
    "    url: str \n",
    "    name: str\n",
    "\n",
    "# class DocumentType(str, Enum):\n",
    "#     FILE = \"File\"\n",
    "#     LINK = \"Link\"    \n",
    "@app.put(\"/api/artificial-intelligence/links\")\n",
    "async def update_web_url(request: UpdateLinkRequest):\n",
    "    \"\"\"\n",
    "    Update web URL content for a specific embeddedId.\n",
    "    Args:\n",
    "        request: UpdateLinkRequest containing tenantId, embeddedId, and url.\n",
    "    Returns:\n",
    "        JSON response indicating processing status.\n",
    "    Raises:\n",
    "        HTTPException: When no valid url are provided or if the embeddedId does not exist.\n",
    "    \"\"\"\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    if request.embeddedId not in upload_documents:\n",
    "        raise HTTPException(status_code=404, detail=\"The embeddedId does not exist.\")\n",
    "    try:\n",
    "        all_docs = []  \n",
    "        urls = request.url.split(\",\")  \n",
    "\n",
    "        for url in urls:\n",
    "            url = url.strip()\n",
    "            try:\n",
    "                async with AsyncWebCrawler(verbose=True) as crawler:\n",
    "                    result = await crawler.arun(url=url)  # Running the crawler on the URL\n",
    "                    extracted_content = result.markdown  # Extracted content in markdown format\n",
    "                    doc = Document(page_content=extracted_content, metadata={\"source\": url,\"fileName\":request.name, \"tenantId\": request.tenantId, \"type\": DocumentType.LINK})\n",
    "                    all_docs.append(doc)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading URL {url}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not all_docs:\n",
    "            raise HTTPException(status_code=400, detail=\"No valid URL provided or failed to fetch content.\")\n",
    "        \n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "        vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "        chunk_size = 6500\n",
    "        chunk_overlap = 1000\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        split_docs = []\n",
    "        chunk_ids = []\n",
    "        \n",
    "        existing_chunk_ids = [key for key in upload_documents if upload_documents[key]['id'] == request.embeddedId]\n",
    "        \n",
    "        for chunk_id in existing_chunk_ids:\n",
    "            del upload_documents[chunk_id]  # Removing old chunks\n",
    "            \n",
    "        ids_to_delete = [id for id in index.list(prefix=request.embeddedId)]\n",
    "        index.delete(ids=ids_to_delete)\n",
    "\n",
    "        for doc in all_docs:\n",
    "            curr_split_docs = text_splitter.split_documents([doc])\n",
    "            document_id = request.embeddedId  # same embeddedId\n",
    "            curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "            split_docs.extend(curr_split_docs)\n",
    "            chunk_ids.extend(curr_chunk_ids)\n",
    "            upload_documents[document_id] = {\"source\": doc.metadata[\"source\"],\"id\": document_id, \"fileName\":request.name, \"tenantId\": request.tenantId, \"type\": DocumentType.LINK}\n",
    "\n",
    "        vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump(upload_documents, f, indent=4)\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"URL updated successfully.\",\n",
    "            \"embeddedId\": request.embeddedId,\n",
    "            \"type\": DocumentType.LINK,\n",
    "            \"updated_url\": request.url\n",
    "        }   \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error updating url: {str(e)}\")\n",
    "############################# links@delete #############################\n",
    "class DeleteRequest(BaseModel):\n",
    "    embeddedId: str\n",
    "@app.delete(\"/api/artificial-intelligence/links\", summary=\"Delete Documents\", description=\"Delete all url documents in the Pinecone index that match the given prefix.\")\n",
    "async def delete_url(request: DeleteRequest):\n",
    "    \"\"\"Delete url documents matching prefix\n",
    "    Args:\n",
    "        request: Delete request containing prefix\n",
    "    Returns:\n",
    "        JSON response indicating deletion status\n",
    "    Raises:\n",
    "        HTTPException: When no matching url documents found\n",
    "    \"\"\"\n",
    "    prefix = request.embeddedId\n",
    "    ids_to_delete = [id for id in index.list(prefix=prefix)]\n",
    "    print(ids_to_delete)\n",
    "    if not ids_to_delete:\n",
    "        raise HTTPException(status_code=404, detail=\"No url documents found with the given prefix.\")\n",
    "    index.delete(ids=ids_to_delete)\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    if prefix in upload_documents:\n",
    "        del upload_documents[prefix]\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    return {\"message\": f\"Deleted {len(ids_to_delete)} documents with embeddedId '{prefix}'.\"}      \n",
    "############################# qa@upload #############################\n",
    "class QARequest(BaseModel):\n",
    "    tenantId: str\n",
    "    question: str\n",
    "    answer: str\n",
    "@app.post(\"/api/artificial-intelligence/qa\")\n",
    "async def upload_question_answer(\n",
    "    request: QARequest,\n",
    "    background_task: BackgroundTasks):\n",
    "    \"\"\"\n",
    "    Upload and store a question-answer pair\n",
    "    Args:\n",
    "        request: QA request containing question and answer\n",
    "        background_task: Background task handler    \n",
    "    Returns:\n",
    "        JSON response with QA details and status  \n",
    "    Raises:\n",
    "        HTTPException: For processing errors\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "    chunk_size = 500\n",
    "    chunk_overlap = 100\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    try:\n",
    "        document_id = str(uuid.uuid4())\n",
    "        combined_text = f\"Question: {request.question}\\nAnswer: {request.answer}\"\n",
    "        metadata = {\n",
    "            \"tenantId\": request.tenantId,\n",
    "            \"fileName\": f\"Q&A_input_{document_id}.txt\",\n",
    "            \"id\": document_id,\n",
    "            \"question\": request.question,  \n",
    "            \"answer\": request.answer}\n",
    "        document = Document(page_content=combined_text, metadata=metadata)\n",
    "        curr_split_docs = text_splitter.split_documents([document])\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "        split_docs += curr_split_docs\n",
    "        chunk_ids += curr_chunk_ids\n",
    "        upload_documents[document_id] = metadata\n",
    "        vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump(upload_documents, f, indent=4)\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"Question and Answer processed successfully.\",\n",
    "            \"id\": document_id,\n",
    "            \"question\": request.question,  \n",
    "            \"answer\": request.answer }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error processing the question and answer: {str(e)}\")      \n",
    "############################# qa@edit #############################\n",
    "class QAEditRequest(BaseModel):\n",
    "    tenantId: str\n",
    "    embeddedId: str\n",
    "    question: str\n",
    "    answer: str\n",
    "@app.put(\"/api/artificial-intelligence/qa\")\n",
    "async def edit_question_answer(\n",
    "    request: QAEditRequest,  \n",
    "    background_task: BackgroundTasks = BackgroundTasks()):\n",
    "    \"\"\"\n",
    "    Edit a question and answer pair by replacing the content in the same embeddedId.\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "    chunk_size = 20\n",
    "    chunk_overlap = 5\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    if request.embeddedId not in upload_documents:\n",
    "        raise HTTPException(status_code=404, detail=\"The embeddedId does not exist.\")\n",
    "    try:\n",
    "        ids_to_delete = [id for id in index.list(prefix=request.embeddedId)]\n",
    "        index.delete(ids=ids_to_delete)\n",
    "        combined_text = f\"Question: {request.question}\\nAnswer: {request.answer}\"\n",
    "        metadata = {\n",
    "            \"tenantId\": request.tenantId,\n",
    "            \"fileName\": f\"Q&A_input_{request.embeddedId}.txt\",\n",
    "            \"id\": request.embeddedId,\n",
    "            \"question\": request.question,\n",
    "            \"answer\": request.answer}\n",
    "        document = Document(page_content=combined_text, metadata=metadata)\n",
    "        curr_split_docs = text_splitter.split_documents([document])\n",
    "        curr_chunk_ids = [f\"{request.embeddedId}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "        vectorstore.add_documents(documents=curr_split_docs, ids=curr_chunk_ids)\n",
    "        upload_documents[request.embeddedId] = metadata\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump(upload_documents, f, indent=4)\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"Question and Answer edited successfully.\",\n",
    "            \"id\": request.embeddedId,\n",
    "            \"question\": request.question,\n",
    "            \"answer\": request.answer }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error editing the question and answer: {str(e)}\")\n",
    "\n",
    "####################################### qa@delete ##########################\n",
    "class DeleteRequest(BaseModel):\n",
    "    prefix: str\n",
    "@app.delete(\"/api/artificial-intelligence/qa\", summary=\"Delete Question Answer\", description=\"Delete all documents in the Pinecone index that match the given prefix.\")\n",
    "async def delete_documents(request: DeleteRequest):\n",
    "    \"\"\"Delete Question Answer matching prefix\n",
    "    Args:\n",
    "        request: Delete request containing prefix\n",
    "    Returns:\n",
    "        JSON response indicating deletion status\n",
    "    Raises:\n",
    "        HTTPException: When no matching documents found\n",
    "    \"\"\"\n",
    "    prefix = request.prefix\n",
    "    ids_to_delete = [id for id in index.list(prefix=prefix)]\n",
    "    print(ids_to_delete)\n",
    "    if not ids_to_delete:\n",
    "        raise HTTPException(status_code=404, detail=\"No QA found with the given prefix.\")\n",
    "    index.delete(ids=ids_to_delete)\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    if prefix in upload_documents:\n",
    "        del upload_documents[prefix]\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    return {\"message\": f\"Deleted {len(ids_to_delete)} QA with prefix '{prefix}'.\"}\n",
    "\n",
    "\n",
    "############################# tenant_files #############################\n",
    "# @app.get(\"/api/artificial-intelligence/tenant_files\")\n",
    "# async def retrieve_files(tenantId: str = Query(...)):\n",
    "#     uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "#     if not os.path.exists(uploaded_documents_path):\n",
    "#         with open(uploaded_documents_path, \"w\") as f:\n",
    "#             json.dump({}, f)\n",
    "#     with open(uploaded_documents_path, \"r\") as f:\n",
    "#         upload_documents = json.load(f)\n",
    "#     tenantFiles = []\n",
    "#     for item in list(upload_documents.values()):\n",
    "#         if item.get('tenantId') == tenantId:\n",
    "#             tenantFiles.append(item)\n",
    "#     return {\"data\": tenantFiles}\n",
    "\n",
    "############################# tenant_files #############################\n",
    "@app.get(\"/api/artificial-intelligence/tenant_files\")\n",
    "async def retrieve_files(tenantId: str = Query(...)):\n",
    "    print(\"Fetching uploaded file lists\")\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    tenantFiles = []\n",
    "    for item in upload_documents.values():\n",
    "        if item.get('tenantId') == tenantId:\n",
    "            if 'fileName' in item and not item.get('fileName', '').startswith('Q&A_input_'):\n",
    "                tenantFiles.append(item)\n",
    "            elif 'source' in item and item['source'].startswith('https'):\n",
    "                tenantFiles.append(item)\n",
    "\n",
    "    return {\"data\": tenantFiles}\n",
    "\n",
    "############################# prompts #############################\n",
    "def initializeVectorStore():\n",
    "    \"\"\"Initialize and configure vector store with embeddings\n",
    "    Returns:\n",
    "        Configured PineconeVectorStore instance\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "def detect_request_type(keyword, vectorstore, tenantId):\n",
    "    \"\"\"Detect the type of request based on the keyword and metadata of the documents.\"\"\"\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\n",
    "            \"k\": 5,\n",
    "            \"filter\": {\n",
    "                'tenantId': {'$eq': tenantId}\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    docs = retriever.get_relevant_documents(keyword)\n",
    "    \n",
    "    #  metadata to determine the type\n",
    "    tenant_files = []\n",
    "    for doc in docs:\n",
    "        metadata = doc.metadata  \n",
    "        if metadata.get('tenantId') == tenantId:\n",
    "            if 'fileName' in metadata and not metadata.get('fileName', '').startswith('Q&A_input_'):\n",
    "                tenant_files.append(metadata)\n",
    "            elif 'source' in metadata and metadata['source'].startswith('https'):\n",
    "                tenant_files.append(metadata)\n",
    "    \n",
    "    #  type based on metadata\n",
    "    if any(metadata.get('fileName', '').startswith('Q&A_input_') for metadata in tenant_files):\n",
    "        return \"qa\"\n",
    "    elif any(metadata.get('source', '').startswith('https') for metadata in tenant_files):\n",
    "        return \"Link\"\n",
    "    else:\n",
    "        return \"Document\"\n",
    "\n",
    "@app.get(\"/api/artificial-intelligence/prompts\")\n",
    "async def unified_prompts_endpoint(\n",
    "    tenantId: str = Query(...), \n",
    "    keyword: str = Query(...)\n",
    "):\n",
    "    \"\"\"Unified endpoint to handle different types of prompts\n",
    "    Args:\n",
    "        tenantId: Identifier for the tenant\n",
    "        keyword: Search keyword/prompt\n",
    "    Returns:\n",
    "        PlainText response with answer or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(tenantId, keyword)\n",
    "        vectorstore = initializeVectorStore()\n",
    "        \n",
    "        # the type of request\n",
    "        request_type = detect_request_type(keyword, vectorstore, tenantId)\n",
    "        \n",
    "        if request_type == \"Document\":\n",
    "            template = \"\"\"Answer the question based only on the following context:\n",
    "            {context}\n",
    "            User Question:\n",
    "            {question}\n",
    "            If the user asks you, 'Who are you?' or 'What are you?' respond with: \"I am an AI assistant.\"\n",
    "            If the user greets you, respond with one of the following without searching information from knowledge base:\n",
    "            - \"Hello! How can I assist you today?\"\n",
    "            - \"Hi there! What can I do for you?\"\n",
    "            - \"Good day! How can I assist you?\"\n",
    "            - \"Hey! What can I help you with?\"\n",
    "            - \"Hi! How can I support you today?\"\n",
    "            - \"Welcome! How may I help you?\"\n",
    "            - \"Salutations! How can I assist you?\"\n",
    "            - \"What's up? How can I help?\"\n",
    "            - \"Good to see you! How can I assist?\"\n",
    "            Please provide a concise and accurate answer based on the context above. If the context does not contain information to answer the question, respond with: \"I don't have the information you're looking for, please provide additional details.\"\n",
    "            Make sure your answer is relevant and accurate to the question and does not repeat the question itself.\n",
    "            In response give only answer but not question.\n",
    "            \"\"\"\n",
    "            retriever = vectorstore.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={\n",
    "                    \"k\": 5,\n",
    "                    \"filter\": {\n",
    "                        'tenantId': {'$eq': tenantId}\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            prompt = ChatPromptTemplate.from_template(template)\n",
    "            rag_chain = (\n",
    "                {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "                | prompt\n",
    "                | llm\n",
    "                | StrOutputParser())\n",
    "            answer = rag_chain.invoke(keyword)\n",
    "            return PlainTextResponse(answer)\n",
    "\n",
    "        elif request_type == \"Link\":\n",
    "            template = \"\"\"Based on the provided web content, answer the following question:\n",
    "            {context}\n",
    "            User Question:\n",
    "            {question}\n",
    "            If the user asks you, 'Who are you?' or 'What are you?' respond with: \"I am an AI assistant.\"\n",
    "            If the user greets you, respond with one of the following without searching information from knowledge base:\n",
    "            - \"Hello! How can I assist you today?\"\n",
    "            - \"Hi there! What can I do for you?\"\n",
    "            - \"Good day! How can I assist you?\"\n",
    "            - \"Hey! What can I help you with?\"\n",
    "            - \"Hi! How can I support you today?\"\n",
    "            - \"Welcome! How may I help you?\"\n",
    "            - \"Salutations! How can I assist you?\"\n",
    "            - \"What's up? How can I help?\"\n",
    "            - \"Good to see you! How can I assist?\"\n",
    "             Instructions for answering:\n",
    "            1. Extract relevant information directly from the web content\n",
    "            2. If the question is about specific data or facts, cite them accurately\n",
    "            3. If the content contains multiple relevant sections, synthesize them into a coherent answer\n",
    "            4. For technical or complex topics, provide clear, simplified explanations\n",
    "            5. If the information is not available in the context, respond with: \"This information is not available in the source content. Please try a different question or provide more details.\"\n",
    "            6. For greetings, use one of the greeting responses above\n",
    "            7. If the user asks questions that are synonyms or rephrased versions of questions in URL links, provide relevant answers based on the intent of the question, not just the exact wording.\n",
    "            8. When answering, focus on matching the intent of the user's question, even if the exact words are not present in the context. Use contextual understanding to infer the meaning and provide accurate responses.\n",
    "            9. If the question is ambiguous or unclear, ask for clarification to ensure the response aligns with the user's intent.\n",
    "            10. For numerical data or specific metrics (e.g., measurements, statistics, or specifications), provide the exact match from the context.\n",
    "            Remember:- For example: The secondary sensor of the Google Pixel 9 with 48 MP ultra-wide-angle, f2.2 and field of view (FOV) is 125.8˚and\n",
    "                                 The secondary sensor of the Galaxy S25 with 12 MP ultra-wide-angle, f/2.2 aperture and field of view (FOV) is 120˚ \n",
    "            \"\"\"\n",
    "            retriever = vectorstore.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={\n",
    "                    \"k\": 8,  \n",
    "                    \"filter\": {\n",
    "                        'tenantId': {'$eq': tenantId}\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            docs = retriever.get_relevant_documents(keyword)\n",
    "            corpus = [doc.page_content for doc in docs]\n",
    "            tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]\n",
    "            bm25 = BM25Okapi(tokenized_corpus)\n",
    "            tokenized_query = word_tokenize(keyword.lower())\n",
    "            bm25_scores = bm25.get_scores(tokenized_query)\n",
    "            reranked_pairs = list(zip(docs, bm25_scores))\n",
    "            reranked_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "            reranked_docs = [pair[0] for pair in reranked_pairs[:1]]\n",
    "            prompt = ChatPromptTemplate.from_template(template)\n",
    "            rag_chain = (\n",
    "                {\"context\": lambda x: reranked_docs, \"question\": RunnablePassthrough()}\n",
    "                | prompt\n",
    "                | llm\n",
    "                | StrOutputParser())\n",
    "            answer = rag_chain.invoke(keyword)\n",
    "            return PlainTextResponse(answer)\n",
    "\n",
    "        elif request_type == \"qa\":\n",
    "            template = \"\"\"Answer the question based only on the following context:\n",
    "            {context}\n",
    "            User Question:\n",
    "            {question}\n",
    "            If the user asks you, 'Who are you?' or 'What are you?' respond with: \"I am an AI assistant.\"\n",
    "            If the user greets you, respond with one of the following without searching information from knowledge base:\n",
    "            - \"Hello! How can I assist you today?\"\n",
    "            - \"Hi there! What can I do for you?\"\n",
    "            - \"Good day! How can I assist you?\"\n",
    "            - \"Hey! What can I help you with?\"\n",
    "            - \"Hi! How can I support you today?\"\n",
    "            - \"Welcome! How may I help you?\"\n",
    "            - \"Salutations! How can I assist you?\"\n",
    "            - \"What's up? How can I help?\"\n",
    "            - \"Good to see you! How can I assist?\"\n",
    "            Please follow these rules:\n",
    "            1. If the user's question EXACTLY matches a question in the context (case-insensitive), provide the corresponding answer.\n",
    "            2. If there is no exact match, respond with: \"Please provide additional details.\"\n",
    "            Make sure your answer is relevant and accurate to the question and does not repeat the question itself.\n",
    "            In response give only answer but not question.\n",
    "            \"\"\"\n",
    "            retriever = vectorstore.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={\n",
    "                    \"k\": 1,  \n",
    "                    \"filter\": {\n",
    "                        'tenantId': {'$eq': tenantId}\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            docs = retriever.get_relevant_documents(keyword)\n",
    "            exact_match = False\n",
    "            exact_answer = None\n",
    "            for doc in docs:\n",
    "                if \"Question:\" in doc.page_content and \"Answer:\" in doc.page_content:\n",
    "                    qa_parts = doc.page_content.split(\"\\n\")\n",
    "                    if len(qa_parts) >= 2:\n",
    "                        stored_question = qa_parts[0].replace(\"Question:\", \"\").strip()\n",
    "                        stored_answer = qa_parts[1].replace(\"Answer:\", \"\").strip()\n",
    "                        if stored_question.lower() == keyword.lower():\n",
    "                            exact_match = True\n",
    "                            exact_answer = stored_answer\n",
    "                            break\n",
    "            if exact_match:\n",
    "                return PlainTextResponse(exact_answer)\n",
    "            # else:\n",
    "            #     prompt = ChatPromptTemplate.from_template(template)\n",
    "            #     rag_chain = (\n",
    "            #         {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            #         | prompt\n",
    "            #         | llm\n",
    "            #         | StrOutputParser())\n",
    "            #     answer = rag_chain.invoke(keyword)\n",
    "            #     return PlainTextResponse(answer)\n",
    "\n",
    "        else:\n",
    "            raise HTTPException(status_code=400, detail=\"Invalid type parameter. Use 'upload', 'links', or 'qa'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing your request: {str(e)}. Please contact support for assistance.\"\n",
    "        print(f\"Error in unified_prompts_endpoint: {str(e)}\") \n",
    "        return PlainTextResponse(error_msg, status_code=500)\n",
    "\n",
    "############################ Summarizer #################################################\n",
    "class SummarizationRequest(BaseModel):\n",
    "    caseId: str\n",
    "    assignedTo: str\n",
    "    patientName: str\n",
    "    onBehalfOf: str\n",
    "    currentCaseStatus: str\n",
    "    summarizationType: str\n",
    "    notes: List[Any]\n",
    "\n",
    "@app.post(\"/api/artificial-intelligence/summarize\")\n",
    "async def summarize(request: SummarizationRequest):\n",
    "    \"\"\"Combine JSON upload and summary generation into one endpoint.\"\"\"\n",
    "    try:\n",
    "        json_data = {\"caseId\": request.caseId, \"notes\": request.notes, \"patientName\": request.patientName, \"assignedTo\": request.assignedTo, \"currentCaseStatus\": request.currentCaseStatus}\n",
    "        noteLength = sum(len(note['note']) for note in request.notes)\n",
    "        maxTokenToUse = 500\n",
    "        \n",
    "        if(noteLength <= 10):\n",
    "            maxTokenToUse=200\n",
    "        \n",
    "        \n",
    "        json_doc = Document(page_content=json.dumps(json_data), metadata={'fileName': 'query_json'})\n",
    "        split_docs = [json_doc]\n",
    "        print(f\"Processing {len(split_docs)} documents.\")\n",
    "        \n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.0,\n",
    "            max_tokens=maxTokenToUse,\n",
    "            top_p=1.0,\n",
    "        )\n",
    "\n",
    "        full_summary_prompt_template = f\"\"\"\n",
    "        \"{{split_docs}}\"\n",
    "\n",
    "        Please summarize the following notes directly to me, the {request.onBehalfOf} in a structured format, focusing on key actions and communications. Include the following sections:\n",
    "        \n",
    "        1. Initial Request: Briefly explain the initial issue or request.\n",
    "        2. Key Updates: Highlight major actions taken, communications, and progress in chronological order, including dates.\n",
    "        3. Current Status: Summarize the present situation or outcome of the case.\n",
    "\n",
    "        Do not expose any ids\n",
    "\n",
    "        Complete the sentence at the end with a full stop.\n",
    "        Give html format\n",
    "        \"\"\"\n",
    "\n",
    "        unread_summary_prompt_template = f\"\"\"\n",
    "        \"{{split_docs}}\"\n",
    "\n",
    "        Please summarize the following case notes directly to me, the {request.onBehalfOf} in a paragraph format.\n",
    "        \n",
    "        Do not expose any ids\n",
    "        Keep it simple and short.\n",
    "        Complete the sentence at the end with a full stop.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        prompt=\"\"\n",
    "        if(request.summarizationType == 'full'):\n",
    "            prompt = full_summary_prompt_template\n",
    "        else:\n",
    "            prompt = unread_summary_prompt_template\n",
    "        \n",
    "        print(prompt)\n",
    "\n",
    "        prompt = PromptTemplate.from_template(prompt)\n",
    "        llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "        stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"split_docs\")\n",
    "        summary = stuff_chain.run(split_docs)\n",
    "\n",
    "        sentences = summary\n",
    "        print(sentences)\n",
    "\n",
    "        # if len(sentences) > 1:\n",
    "        #     summary = '.'.join(sentences[:-1]).strip()  \n",
    "        # else:\n",
    "        #     summary = sentences[0].strip()  \n",
    "\n",
    "        # if not summary.endswith('.'):\n",
    "        #     summary += '.'\n",
    "        \n",
    "        summary = sentences.replace(\"\\n\", \" \")\n",
    "        summary = summary.replace(\"```html\", \" \")\n",
    "        return JSONResponse(content={\n",
    "            \"summary\": summary\n",
    "        })\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error during summarizing the documents: {str(e)}\")\n",
    "\n",
    "####################################### delete #########################\n",
    "class DeleteRequest(BaseModel):\n",
    "    prefix: str\n",
    "@app.delete(\"/api/artificial-intelligence/delete\", summary=\"Delete Documents\", description=\"Delete all documents in the Pinecone index that match the given prefix.\")\n",
    "async def delete_documents(request: DeleteRequest):\n",
    "    \"\"\"Delete documents matching prefix\n",
    "    Args:\n",
    "        request: Delete request containing prefix\n",
    "    Returns:\n",
    "        JSON response indicating deletion status\n",
    "    Raises:\n",
    "        HTTPException: When no matching documents found\n",
    "    \"\"\"\n",
    "    prefix = request.prefix\n",
    "    ids_to_delete = [id for id in index.list(prefix=prefix)]\n",
    "    print(ids_to_delete)\n",
    "    if not ids_to_delete:\n",
    "        raise HTTPException(status_code=404, detail=\"No documents found with the given prefix.\")\n",
    "    index.delete(ids=ids_to_delete)\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    if prefix in upload_documents:\n",
    "        del upload_documents[prefix]\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    return {\"message\": f\"Deleted {len(ids_to_delete)} documents with prefix '{prefix}'.\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "11. If the user asks any date related information, search the context for date and provide the  match.\n",
    "                The date extraction should work with formats like:    \n",
    "                - 2022/02/09\n",
    "                - 04/03/2024\n",
    "                - 01/07/2023\n",
    "                - 2022, January 09\n",
    "                - March 09, 2022\n",
    "            Remember:- For example: The Samsung Galaxy S22 Ultra 5G was launch on 2022, February 09.\n",
    "\n",
    "\n",
    "            5. If the information is not available in the context, respond with: \"This information is not available in the source content. Please try a different question or provide more details.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
