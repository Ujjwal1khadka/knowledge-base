{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \\\n",
    "#   langchain_community \\\n",
    "#   langchain_pinecone \\\n",
    "#   langchain_openai \\\n",
    "#   unstructured \\\n",
    "#   langchain-text-splitters \\\n",
    "#   pinecone-text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.retrievers import (\n",
    "    PineconeHybridSearchRetriever)\n",
    "#from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "#from langchain.vectorstores import PineconeVectorStore  \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_core.runnables import Runnable\n",
    "from pinecone import ServerlessSpec\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from uuid import uuid4\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import pinecone\n",
    "import uuid\n",
    "import os\n",
    "import glob\n",
    "import getpass\n",
    "import pylibmagic\n",
    "import hashlib\n",
    "from tqdm.autonotebook import tqdm\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from pinecone import Pinecone\n",
    "from pinecone.grpc import PineconeGRPC as Pinecon\n",
    "from typing import Sequence\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "os.environ['USER_AGENT'] = 'myagent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# Fetch API keys from environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "momento_api_key = os.getenv('MOMENTO_API_KEY')\n",
    "\n",
    "# Set the environment variables\n",
    "if openai_api_key:\n",
    "    os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "if pinecone_api_key:\n",
    "    os.environ['PINECONE_API_KEY'] = pinecone_api_key \n",
    "\n",
    "#Verify that the keys are loaded\n",
    "#print(f\"OpenAI API Key: {os.environ.get('OPENAI_API_KEY')}\")\n",
    "#print(f\"Pinecone API Key: {os.environ.get('PINECONE_API_KEY')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x345eeeee0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x345edcc70>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",                             #response time is 9s  #infloat/e5-base-V2 has 3.53sec response time.\n",
    ")\n",
    "embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinecone Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "import time\n",
    "\n",
    "index_name = \"test-3\"  \n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=3072,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "#pc.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory = 'data'\n",
    "\n",
    "# def load_docs(directory):\n",
    "#     loader = DirectoryLoader(directory)\n",
    "#     docs = loader.load()\n",
    "#     return docs\n",
    "\n",
    "# docs = load_docs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory = 'data'\n",
    "\n",
    "def load_docs(directory, tenant_id):\n",
    "    loader = DirectoryLoader(directory)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc.metadata['tenantId'] = tenant_id  \n",
    "\n",
    "        doc.metadata['fileName'] = os.path.basename(doc.metadata['source'])\n",
    "        print(doc.metadata['fileName'])\n",
    "\n",
    "    return docs\n",
    "\n",
    "tenantId = \"tenant8\"\n",
    "docs = load_docs(directory, tenantId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain_community beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://collegerecon.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader_multiple_pages = WebBaseLoader(\"https://collegerecon.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Metadata: {'source': 'https://collegerecon.com/', 'title': 'Military Tuition Assistance, Veterans Education Benefits, VA Benefits', 'description': 'Uncover Tuition Assistance, College Education Benefits, and scholarships for Military Service Members, Veterans, Spouses, and Dependents.', 'language': 'en', 'tenantId': 'tenant1'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "tenantId = \"tenant1\"\n",
    "\n",
    "loader = WebBaseLoader(\"https://collegerecon.com/\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "for doc in docs:\n",
    "    doc.metadata[\"tenantId\"] = tenantId\n",
    "\n",
    "if docs:\n",
    "    print(f\"Updated Metadata: {docs[0].metadata}\")\n",
    "else:\n",
    "    print(\"No documents loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 0 links.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_all_links(base_url, depth=1):\n",
    "    \"\"\"Recursively fetch all links from the website up to a given depth.\"\"\"\n",
    "    visited = set()\n",
    "    links_to_visit = {base_url}\n",
    "    all_links = set()\n",
    "\n",
    "    for _ in range(depth):  \n",
    "        new_links = set()\n",
    "        for url in links_to_visit:\n",
    "            if url not in visited:\n",
    "                try:\n",
    "                    response = requests.get(url)\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    for a_tag in soup.find_all(\"a\", href=True):\n",
    "                        href = a_tag[\"href\"]\n",
    "                        if href.startswith(\"/\"):\n",
    "                            href = base_url + href\n",
    "                        if href.startswith(\"http\"):\n",
    "                            new_links.add(href)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fetching {url}: {e}\")\n",
    "                visited.add(url)\n",
    "        links_to_visit = new_links\n",
    "        all_links.update(new_links)\n",
    "\n",
    "    return list(all_links)\n",
    "\n",
    "base_url = \"https://collegerecon.com/\"\n",
    "all_links = get_all_links(base_url, depth=2)  \n",
    "print(f\"Discovered {len(all_links)} links.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Use the collected links\n",
    "loader = WebBaseLoader(all_links)\n",
    "\n",
    "# Load the documents\n",
    "docs = loader.load()\n",
    "\n",
    "# Example: Print the metadata and content of the first document\n",
    "if docs:\n",
    "    print(f\"Document Metadata: {docs[0].metadata}\")\n",
    "    print(f\"Document Content: {docs[0].page_content[:500]}...\")  # Print the first 500 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate uniqueId for Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 500 \n",
    "chunk_overlap = 50\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "\n",
    "# split_docs = []\n",
    "# chunk_ids = []\n",
    "\n",
    "# uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "# if not os.path.exists(uploaded_documents_path):\n",
    "#         with open(uploaded_documents_path, \"w\") as f:\n",
    "#             json.dump({}, f)\n",
    "\n",
    "# with open(uploaded_documents_path, \"r\") as f:\n",
    "#         upload_documents = json.load(f)\n",
    "\n",
    "# for doc in docs:\n",
    "#         curr_split_docs = text_splitter.split_documents([doc])\n",
    "\n",
    "#         document_id = str(uuid4())\n",
    "\n",
    "#         # Create unique IDs for each chunk with the document ID as a prefix\n",
    "#         curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "\n",
    "#         split_docs = split_docs + curr_split_docs\n",
    "#         chunk_ids = chunk_ids + curr_chunk_ids\n",
    "\n",
    "#         upload_documents[document_id] =  {\"fileName\": doc.metadata['fileName'], \"id\": document_id, \"tenantId\": tenantId}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "split_docs = []\n",
    "chunk_ids = []\n",
    "\n",
    "uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "\n",
    "with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "for doc in docs:\n",
    "        curr_split_docs = text_splitter.split_documents([doc])\n",
    "\n",
    "        document_id = str(uuid4())\n",
    "\n",
    "        # Create unique IDs for each chunk with the document ID as a prefix\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "\n",
    "        split_docs = split_docs + curr_split_docs\n",
    "        chunk_ids = chunk_ids + curr_chunk_ids\n",
    "\n",
    "        upload_documents[document_id] =  {\"id\": document_id, \"tenantId\": tenantId}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name = \"test-3\"\n",
    "\n",
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = vectorstore.as_retriever(\n",
    "#     search_type=\"similarity\",\n",
    "#     search_kwargs={\n",
    "#         \"k\": 1,\n",
    "#             \"filter\" : {\n",
    "#         'source': {'$eq': 'data/xyz/Cases Module âœ“.docx'}  \n",
    "#     },\n",
    "#             }\n",
    "# )\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model=\"gpt-4o\",\n",
    "#     temperature=0.0,\n",
    "# )\n",
    "\n",
    "# qa = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     chain_type=\"stuff\",\n",
    "#     retriever=vectorstore.as_retriever(),\n",
    "\n",
    "# )\n",
    "# qa.invoke(\"how to issue a refund? \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query1 = \"how to make a new case?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With and Without Knowledge Base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Chat with knowledge:\")\n",
    "# print(qa.invoke(query1).get(\"result\"))\n",
    "# #print(\"\\nChat without knowledge:\")\n",
    "# print(llm.invoke(query1).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter based on built-in metadata [Source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriveal QA chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How to issue refund?',\n",
       " 'result': 'To issue a refund, you typically need to follow the specific refund process of the company or platform you are dealing with. This may involve logging into your account, locating the transaction, and selecting the option to issue a refund. If you are a business owner, you may need to process the refund through your payment processor or point-of-sale system. If you provide more details about the specific situation or platform you are using, I may be able to provide more specific guidance.'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    #search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 1,\n",
    "            \"filter\" : {\n",
    "        'tenantId': {'$eq': 'tenant7'}  \n",
    "    },\n",
    "            }\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    ")\n",
    "\n",
    "# answer = qa(\"How to add a new case?\")\n",
    "# print(answer)\n",
    "answer = qa.invoke(\"How to issue refund?\")\n",
    "answer\n",
    "\n",
    "# async for response in qa.astream({\"query\": \"How to issue a refund?\"}):\n",
    "#     print(response)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deletion_protection': 'enabled',\n",
       " 'dimension': 1536,\n",
       " 'host': 'test-3-unx28qm.svc.aped-4627-b74a.pinecone.io',\n",
       " 'metric': 'cosine',\n",
       " 'name': 'test-3',\n",
       " 'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
       " 'status': {'ready': True, 'state': 'Ready'}}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.describe_index(\"test-3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete by embeddedId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids in index.list(prefix='71580ad0-1042-4260-8a9b-d41a2325070e_'):\n",
    "    print(ids)\n",
    "    index.delete(ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Chain Initiations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'How to issue refund?', 'result': \"I don't know.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    #search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 1,\n",
    "            \"filter\" : {\n",
    "        'tenantId': {'$eq': 'tenant7'}  \n",
    "    },\n",
    "            }\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0,\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    ")\n",
    "\n",
    "answer = qa(\"How to issue refund?\")\n",
    "print(answer)\n",
    "# answer = qa.invoke(\"How to add new appointment? \")\n",
    "# answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG : Parallel Langchain Chains with Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer: \"\"\"\n",
    "retriever = vectorstore.as_retriever(\n",
    "    #search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 1,\n",
    "            \"filter\" : {\n",
    "        'tenantId': {'$eq': 'tenant7'}  \n",
    "    },\n",
    "            }\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# def docs2str(docs):\n",
    "#     return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact customer support for assistance."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"How to issue refund?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q/A with RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: how to issue refund?\n",
      "Answer: Contact customer support for assistance with issuing a refund.\n"
     ]
    }
   ],
   "source": [
    "question = \"how to issue refund?\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add chat history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    #search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 1,\n",
    "            \"filter\" : {\n",
    "        'tenantId': {'$eq': 'tenant7'}  \n",
    "    },\n",
    "            }\n",
    ")\n",
    "\n",
    "### Contextualize question ###\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "\n",
    "### Answer question ###\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "### Statefully manage chat history ###\n",
    "\n",
    "\n",
    "# We define a dict representing the state of the application.\n",
    "# This state has the same input and output keys as `rag_chain`.\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    chat_history: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    context: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# We then define a simple node that runs the `rag_chain`.\n",
    "# The `return` values of the node update the graph state, so here we just\n",
    "# update the chat history with the input message and response.\n",
    "def call_model(state: State):\n",
    "    response = rag_chain.invoke(state)\n",
    "    return {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(state[\"input\"]),\n",
    "            AIMessage(response[\"answer\"]),\n",
    "        ],\n",
    "        \"context\": response[\"context\"],\n",
    "        \"answer\": response[\"answer\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# Our graph consists only of one node:\n",
    "workflow = StateGraph(state_schema=State)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Finally, we compile the graph with a checkpointer object.\n",
    "# This persists the state, in this case in memory.\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To issue a refund, you typically need to access your payment processing system or platform where the original transaction took place. Locate the specific transaction that requires a refund and follow the steps provided by the platform to initiate the refund process. Make sure to confirm the refund amount and provide any necessary details before finalizing the refund transaction.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"a2\"}}\n",
    "\n",
    "result = app.invoke(\n",
    "    {\"input\": \"How to issue refund?\"},\n",
    "    config=config,\n",
    ")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One way to issue a refund is through your online payment processor or platform. Log in to your account, locate the transaction that needs a refund, and follow the instructions to process the refund. Ensure that you enter the correct refund amount and provide any required information before confirming the refund.\n"
     ]
    }
   ],
   "source": [
    "result = app.invoke(\n",
    "    {\"input\": \"Tell me one of the way of doing it\"},\n",
    "    config=config,\n",
    ")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/typing.py:292: RuntimeWarning: coroutine 'AsyncChromiumLoader.ascrape_playwright' was never awaited\n",
      "  ev_args = tuple(_eval_type(a, globalns, localns, recursive_guard) for a in t.__args__)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You should contact the customer support or technical support team of the payment processing platform you use to issue a refund. They can guide you through the process, provide assistance, and address any issues you may encounter while issuing the refund. Be prepared to provide details about the transaction in question to expedite the refund process.\n"
     ]
    }
   ],
   "source": [
    "result = app.invoke(\n",
    "    {\"input\": \"Who should i contact to do it?\"},\n",
    "    config=config,\n",
    ")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "pinecone_index_name = os.environ.get(\"PINECONE_INDEX_NAME\")\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "pinecone_client = Pinecone(api_key=pinecone_api_key)\n",
    "def initialize_pinecone_index(client: Pinecone, index_name: str):\n",
    "    existing_indexes = [index_info[\"name\"] for index_info in client.list_indexes()]\n",
    "    if index_name not in existing_indexes:\n",
    "        client.create_index(\n",
    "            name=index_name,\n",
    "            dimension=3072,\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))\n",
    "        while not client.describe_index(index_name).status[\"ready\"]:\n",
    "            time.sleep(1)\n",
    "    return client.Index(index_name)\n",
    "index = initialize_pinecone_index(pinecone_client, pinecone_index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL to scrape\n",
    "url = \"https://www.noveltytechnology.com/about-us\"\n",
    "\n",
    "# Fetch the webpage content\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    html_content = response.text\n",
    "else:\n",
    "    raise Exception(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "\n",
    "# Parse the HTML content using Beautiful Soup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extract the main content of the page\n",
    "page_content = \"\\n\".join([p.get_text() for p in soup.find_all('p')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Document object with the extracted content\n",
    "document = Document(page_content=page_content.strip(), metadata={\"source\": url})\n",
    "\n",
    "# Initialize the OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Generate embeddings for the document\n",
    "doc_embeddings = embeddings.embed_documents([document.page_content])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for upsert\n",
    "# Assuming 'doc_embeddings' is a list of embeddings and 'document' has a unique ID\n",
    "vectors = [(str(i), embedding) for i, embedding in enumerate(doc_embeddings)]\n",
    "\n",
    "# Upsert the vectors into Pinecone\n",
    "#index.upsert(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ForbiddenException",
     "evalue": "(403)\nReason: Forbidden\nHTTP response headers: HTTPHeaderDict({'Date': 'Thu, 02 Jan 2025 06:11:13 GMT', 'Content-Type': 'text/plain', 'Content-Length': '9', 'Connection': 'keep-alive', 'x-pinecone-auth-rejected-reason': 'Wrong API key', 'www-authenticate': 'Wrong API key', 'server': 'envoy'})\nHTTP response body: Forbidden\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mForbiddenException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m query_embedding \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39membed_query(query)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Perform a similarity search in Pinecone\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m search_results \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Process and print the results\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m search_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatches\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pinecone/utils/error_handling.py:11\u001b[0m, in \u001b[0;36mvalidate_and_convert_errors.<locals>.inner_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_func\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ProtocolError):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pinecone/data/index.py:429\u001b[0m, in \u001b[0;36mIndex.query\u001b[0;34m(self, top_k, vector, id, namespace, filter, include_values, include_metadata, sparse_vector, *args, **kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m sparse_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_sparse_values_arg(sparse_vector)\n\u001b[1;32m    416\u001b[0m args_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_non_empty_args(\n\u001b[1;32m    417\u001b[0m     [\n\u001b[1;32m    418\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector\u001b[39m\u001b[38;5;124m\"\u001b[39m, vector),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    427\u001b[0m     ]\n\u001b[1;32m    428\u001b[0m )\n\u001b[0;32m--> 429\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vector_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mQueryRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_check_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_check_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_OPENAPI_ENDPOINT_PARAMS\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_OPENAPI_ENDPOINT_PARAMS\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parse_query_response(response)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pinecone/core/openapi/shared/api_client.py:761\u001b[0m, in \u001b[0;36mEndpoint.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    751\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This method is invoked when endpoints are called\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    759\u001b[0m \n\u001b[1;32m    760\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pinecone/core/openapi/data/api/data_plane_api.py:639\u001b[0m, in \u001b[0;36mDataPlaneApi.__init__.<locals>.__query\u001b[0;34m(self, query_request, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_host_index\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_host_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    638\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_request\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m query_request\n\u001b[0;32m--> 639\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_with_http_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pinecone/core/openapi/shared/api_client.py:819\u001b[0m, in \u001b[0;36mEndpoint.call_with_http_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m     header_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_client\u001b[38;5;241m.\u001b[39mselect_header_content_type(content_type_headers_list)\n\u001b[1;32m    817\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m header_list\n\u001b[0;32m--> 819\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mendpoint_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp_method\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbody\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43masync_req\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_check_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_check_return_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_return_http_data_only\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_preload_content\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_request_timeout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcollection_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pinecone/core/openapi/shared/api_client.py:380\u001b[0m, in \u001b[0;36mApiClient.call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Makes the HTTP request (synchronous) and returns deserialized data.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03mTo make an async_req request, set the async_req parameter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03m    then the method will return the response directly.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m async_req:\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__call_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresource_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_check_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mapply_async(\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__call_api,\n\u001b[1;32m    401\u001b[0m     (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    418\u001b[0m     ),\n\u001b[1;32m    419\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pinecone/core/openapi/shared/api_client.py:187\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PineconeApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    186\u001b[0m     e\u001b[38;5;241m.\u001b[39mbody \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mbody\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_response \u001b[38;5;241m=\u001b[39m response_data\n\u001b[1;32m    191\u001b[0m return_data \u001b[38;5;241m=\u001b[39m response_data\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pinecone/core/openapi/shared/api_client.py:175\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    171\u001b[0m     url \u001b[38;5;241m=\u001b[39m _host \u001b[38;5;241m+\u001b[39m resource_path\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# perform request and return response\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     response_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PineconeApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    186\u001b[0m     e\u001b[38;5;241m.\u001b[39mbody \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mbody\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pinecone/core/openapi/shared/api_client.py:460\u001b[0m, in \u001b[0;36mApiClient.request\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrest_client\u001b[38;5;241m.\u001b[39mOPTIONS(\n\u001b[1;32m    451\u001b[0m         url,\n\u001b[1;32m    452\u001b[0m         query_params\u001b[38;5;241m=\u001b[39mquery_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    457\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    458\u001b[0m     )\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrest_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOST\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrest_client\u001b[38;5;241m.\u001b[39mPUT(\n\u001b[1;32m    471\u001b[0m         url,\n\u001b[1;32m    472\u001b[0m         query_params\u001b[38;5;241m=\u001b[39mquery_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    477\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    478\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pinecone/core/openapi/shared/rest.py:345\u001b[0m, in \u001b[0;36mRESTClientObject.POST\u001b[0;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPOST\u001b[39m(\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    337\u001b[0m     url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    343\u001b[0m     _request_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    344\u001b[0m ):\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pinecone/core/openapi/shared/rest.py:271\u001b[0m, in \u001b[0;36mRESTClientObject.request\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnauthorizedException(http_resp\u001b[38;5;241m=\u001b[39mr)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ForbiddenException(http_resp\u001b[38;5;241m=\u001b[39mr)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFoundException(http_resp\u001b[38;5;241m=\u001b[39mr)\n",
      "\u001b[0;31mForbiddenException\u001b[0m: (403)\nReason: Forbidden\nHTTP response headers: HTTPHeaderDict({'Date': 'Thu, 02 Jan 2025 06:11:13 GMT', 'Content-Type': 'text/plain', 'Content-Length': '9', 'Connection': 'keep-alive', 'x-pinecone-auth-rejected-reason': 'Wrong API key', 'www-authenticate': 'Wrong API key', 'server': 'envoy'})\nHTTP response body: Forbidden\n"
     ]
    }
   ],
   "source": [
    "# Define your query\n",
    "query = \"Tell me about Novelty Technology\"\n",
    "\n",
    "# Generate the query embedding\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "# Perform a similarity search in Pinecone\n",
    "search_results = index.query(queries=[query_embedding], top_k=5)\n",
    "\n",
    "# Process and print the results\n",
    "for match in search_results['matches']:\n",
    "    print(f\"Score: {match['score']}\")\n",
    "    # Retrieve the original document content or metadata as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "apify_api_key = os.getenv(\"APIFY_API_TOKEN\")\n",
    "if apify_api_key:\n",
    "    os.environ[\"APIFY_API_TOKEN\"] = apify_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The argument order for `query()` has changed; please use keyword arguments instead of positional arguments. Example: index.query(vector=[0.1, 0.2, 0.3], top_k=10, namespace='my_namespace')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 20\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Create a vector store based on the crawled data\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#index = VectorstoreIndexCreator().from_loaders([loader])\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Query the vector store\u001b[39;00m\n\u001b[1;32m     19\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me about the novelty technology\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pinecone/utils/error_handling.py:11\u001b[0m, in \u001b[0;36mvalidate_and_convert_errors.<locals>.inner_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_func\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ProtocolError):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pinecone/data/index.py:406\u001b[0m, in \u001b[0;36mIndex.query\u001b[0;34m(self, top_k, vector, id, namespace, filter, include_values, include_metadata, sparse_vector, *args, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03mThe Query operation searches a namespace, using a query vector.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03mIt retrieves the ids of the most similar items in a namespace, along with their similarity scores.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03m         and namespace name.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe argument order for `query()` has changed; please use keyword arguments instead of positional arguments. Example: index.query(vector=[0.1, 0.2, 0.3], top_k=10, namespace=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_namespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     )\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot specify both `id` and `vector`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The argument order for `query()` has changed; please use keyword arguments instead of positional arguments. Example: index.query(vector=[0.1, 0.2, 0.3], top_k=10, namespace='my_namespace')"
     ]
    }
   ],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_community.docstore.document import Document\n",
    "from langchain_community.utilities import ApifyWrapper\n",
    "\n",
    "apify = ApifyWrapper()\n",
    "# Call the Actor to obtain text from the crawled webpages\n",
    "loader = apify.call_actor(\n",
    "    actor_id=\"apify/website-content-crawler\",\n",
    "    run_input={\"startUrls\": [{\"url\": \"https://www.noveltytechnology.com/about-us\"}]},\n",
    "    dataset_mapping_function=lambda item: Document(\n",
    "        page_content=item[\"text\"] or \"\", metadata={\"source\": item[\"url\"]}\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create a vector store based on the crawled data\n",
    "#index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "\n",
    "\n",
    "# Query the vector store\n",
    "query = \"Tell me about the novelty technology\"\n",
    "result = index.query(query, top_k=5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Output with message history and desired output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'thread_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mability\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVitafy System\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHow to issue refund?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigurable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msession_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabc12\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, AIMessage):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/__init__.py:1749\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1748\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1749\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1751\u001b[0m     config,\n\u001b[1;32m   1752\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1753\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1754\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1755\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1756\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1758\u001b[0m ):\n\u001b[1;32m   1759\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1760\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/__init__.py:1425\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_modes:\n\u001b[1;32m   1422\u001b[0m     config[CONF][CONFIG_KEY_STREAM_WRITER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m c: stream\u001b[38;5;241m.\u001b[39mput(\n\u001b[1;32m   1423\u001b[0m         ((), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m, c)\n\u001b[1;32m   1424\u001b[0m     )\n\u001b[0;32m-> 1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SyncPregelLoop(\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1427\u001b[0m     stream\u001b[38;5;241m=\u001b[39mStreamProtocol(stream\u001b[38;5;241m.\u001b[39mput, stream_modes),\n\u001b[1;32m   1428\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m   1429\u001b[0m     store\u001b[38;5;241m=\u001b[39mstore,\n\u001b[1;32m   1430\u001b[0m     checkpointer\u001b[38;5;241m=\u001b[39mcheckpointer,\n\u001b[1;32m   1431\u001b[0m     nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes,\n\u001b[1;32m   1432\u001b[0m     specs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels,\n\u001b[1;32m   1433\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1434\u001b[0m     stream_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_channels_asis,\n\u001b[1;32m   1435\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1436\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m loop:\n\u001b[1;32m   1437\u001b[0m     \u001b[38;5;66;03m# create runner\u001b[39;00m\n\u001b[1;32m   1438\u001b[0m     runner \u001b[38;5;241m=\u001b[39m PregelRunner(\n\u001b[1;32m   1439\u001b[0m         submit\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39msubmit,\n\u001b[1;32m   1440\u001b[0m         put_writes\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39mput_writes,\n\u001b[1;32m   1441\u001b[0m         node_finished\u001b[38;5;241m=\u001b[39mconfig[CONF]\u001b[38;5;241m.\u001b[39mget(CONFIG_KEY_NODE_FINISHED),\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[1;32m   1443\u001b[0m     \u001b[38;5;66;03m# enable subgraph streaming\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/loop.py:706\u001b[0m, in \u001b[0;36mSyncPregelLoop.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CheckpointNotLatest\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer:\n\u001b[0;32m--> 706\u001b[0m     saved \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpointer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    708\u001b[0m     saved \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/checkpoint/memory/__init__.py:112\u001b[0m, in \u001b[0;36mMemorySaver.get_tuple\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_tuple\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[CheckpointTuple]:\n\u001b[1;32m     99\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a checkpoint tuple from the in-memory storage.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    This method retrieves a checkpoint tuple from the in-memory storage based on the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m        Optional[CheckpointTuple]: The retrieved checkpoint tuple, or None if no matching checkpoint was found.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     thread_id \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigurable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthread_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    113\u001b[0m     checkpoint_ns \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint_ns\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m checkpoint_id \u001b[38;5;241m:=\u001b[39m get_checkpoint_id(config):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'thread_id'"
     ]
    }
   ],
   "source": [
    "response = app.invoke(\n",
    "    {\"ability\": \"Vitafy System\", \"input\": \"How to issue refund?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc12\"}},\n",
    ")\n",
    "\n",
    "if isinstance(response, AIMessage):\n",
    "    print(response.content)\n",
    "else:\n",
    "    print(\"The system is under maintainance so, I recommend reaching out to your system administrator or support team for further assistance.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One way to issue a refund using the Vitafy System is to access the transaction details of the specific order for which you need to issue a refund. Within the transaction details, there should be an option or button to initiate a refund. Click on this option, enter the refund amount, provide any necessary details or reasons for the refund, and then confirm the refund to process it through the Vitafy System. Be sure to double-check the refund amount and details before finalizing the refund to ensure accuracy.\n"
     ]
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"ability\": \"Vitafy System\", \"input\": \"Tell me one of the way of doing it\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc12\"}},\n",
    ")\n",
    "\n",
    "if isinstance(response, AIMessage):\n",
    "    print(response.content)\n",
    "else:\n",
    "    print(\"The system is under maintainance so, I recommend reaching out to your system administrator or support team for further assistance.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable_with_history = RunnableWithMessageHistory(\n",
    "    model,\n",
    "    get_session_history,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='To issue a refund, you typically need to follow these steps:\\n\\n1. Log in to your payment processing platform or account.\\n2. Locate the transaction for which you want to issue a refund.\\n3. Click on the option to issue a refund or process a return.\\n4. Enter the refund amount and any additional details required.\\n5. Confirm the refund and complete the process.\\n\\nPlease note that the specific steps may vary depending on the payment processor or platform you are using. If you need further assistance, I recommend reaching out to the customer support of your payment processor or platform for detailed instructions.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 119, 'prompt_tokens': 59, 'total_tokens': 178, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a139dd90-d843-41ff-996f-d707c37263d5-0', usage_metadata={'input_tokens': 59, 'output_tokens': 119, 'total_tokens': 178, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable_with_history.invoke(\n",
    "    [HumanMessage(content=\"How to issue refund?\")],\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Ujjwal!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 39, 'total_tokens': 46, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-65af7fd4-dcd5-42a1-88e3-c80df40c8bf6-0', usage_metadata={'input_tokens': 39, 'output_tokens': 7, 'total_tokens': 46, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable_with_history.invoke(\n",
    "    [HumanMessage(content=\"whats my name?\")],\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tenant5:1': InMemoryHistory(messages=[AIMessage(content='hello, this is a tenant-specific message!', additional_kwargs={}, response_metadata={})])}\n",
      "content=\"One way to process a refund is to follow these steps:\\n\\n1. Confirm the reason for the refund request and verify the details of the transaction.\\n2. Access the payment system or platform used for the original transaction.\\n3. Locate the transaction in question and select the option to issue a refund.\\n4. Enter the refund amount and any necessary details or comments.\\n5. Process the refund through the system, which will typically notify the customer of the refund.\\n6. Confirm with the customer that the refund has been processed and provide any additional information or assistance as needed.\\n\\nIt's important to ensure accurate and timely processing of refunds to maintain customer satisfaction.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 129, 'prompt_tokens': 42, 'total_tokens': 171, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-bf8064f6-8f16-4d65-a61c-ad39523d1047-0' usage_metadata={'input_tokens': 42, 'output_tokens': 129, 'total_tokens': 171, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "{'tenant5:1': InMemoryHistory(messages=[AIMessage(content='hello, this is a tenant-specific message!', additional_kwargs={}, response_metadata={}), HumanMessage(content='One way of doing refund', additional_kwargs={}, response_metadata={}), AIMessage(content=\"One way to process a refund is to follow these steps:\\n\\n1. Confirm the reason for the refund request and verify the details of the transaction.\\n2. Access the payment system or platform used for the original transaction.\\n3. Locate the transaction in question and select the option to issue a refund.\\n4. Enter the refund amount and any necessary details or comments.\\n5. Process the refund through the system, which will typically notify the customer of the refund.\\n6. Confirm with the customer that the refund has been processed and provide any additional information or assistance as needed.\\n\\nIt's important to ensure accurate and timely processing of refunds to maintain customer satisfaction.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 129, 'prompt_tokens': 42, 'total_tokens': 171, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-bf8064f6-8f16-4d65-a61c-ad39523d1047-0', usage_metadata={'input_tokens': 42, 'output_tokens': 129, 'total_tokens': 171, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})])}\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from typing import List\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import BaseMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    ConfigurableFieldSpec,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n",
    "    \"\"\"In memory implementation of chat message history.\"\"\"\n",
    "\n",
    "    messages: List[BaseMessage] = Field(default_factory=list)\n",
    "\n",
    "    def add_messages(self, messages: List[BaseMessage]) -> None:\n",
    "        \"\"\"Add a list of messages to the store\"\"\"\n",
    "        self.messages.extend(messages)\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.messages = []\n",
    "\n",
    "\n",
    "# Global store to manage tenant-specific chat histories\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_by_tenant_and_session_id(tenant_id: str, session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"\n",
    "    Retrieves or creates a tenant-specific chat message history for a given session ID.\n",
    "    \"\"\"\n",
    "    # Use a composite key combining tenant_id and session_id\n",
    "    key = f\"{tenant_id}:{session_id}\"\n",
    "    if key not in store:\n",
    "        store[key] = InMemoryHistory()\n",
    "    return store[key]\n",
    "\n",
    "\n",
    "# Example usage: Add messages for a specific tenant and session\n",
    "tenantIdd = \"tenant5\"\n",
    "session_id = \"1\"\n",
    "\n",
    "# Retrieve history for the specific tenant and session\n",
    "history = get_by_tenant_and_session_id(tenantId, session_id)\n",
    "history.add_message(AIMessage(content=\"hello, this is a tenant-specific message!\"))\n",
    "print(store)  # Inspect tenant-specific chat history\n",
    "\n",
    "\n",
    "# Example of using RunnableWithMessageHistory with tenant-specific history\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You're an assistant who's good at {ability}\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    # Use the tenant-specific retrieval function\n",
    "    lambda session_id: get_by_tenant_and_session_id(tenant_id, session_id),\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "# Simulate a conversation for a specific tenant and session\n",
    "response = chain_with_history.invoke(\n",
    "    {\"ability\": \"   Vitafy AI\", \"question\": \"One way of doing refund\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(response)  # Output for the question\n",
    "print(store)  # Inspect the store to see the tenant-specific history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_stream = with_message_history.astream(\n",
    "#     {\"ability\": \"Vitafy System\", \"input\": \"How to contact them?\"},\n",
    "#     config={\"configurable\": {\"session_id\": \"abc12\"}},\n",
    "# )\n",
    "\n",
    "# # Stream and print only the content\n",
    "# async for chunk in response_stream:\n",
    "#     if \"content\" in chunk:\n",
    "#         print(chunk[\"content\"], end=\"\", flush=True)\n",
    "\n",
    "# if isinstance(response, AIMessage):\n",
    "#     print(response.content)\n",
    "# else:\n",
    "#     print(\"Unexpected response format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'StrOutputParser' from 'langchain_core.prompts' (/Users/ujjwal/Library/Python/3.9/lib/python/site-packages/langchain_core/prompts/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[263], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_pinecone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PineconeVectorStore\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StrOutputParser\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# In-memory tenant-specific chat history\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mInMemoryHistory\u001b[39;00m(BaseChatMessageHistory, BaseModel):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'StrOutputParser' from 'langchain_core.prompts' (/Users/ujjwal/Library/Python/3.9/lib/python/site-packages/langchain_core/prompts/__init__.py)"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import StrOutputParser\n",
    "\n",
    "# In-memory tenant-specific chat history\n",
    "class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n",
    "    \"\"\"In memory implementation of chat message history.\"\"\"\n",
    "\n",
    "    messages: List[BaseMessage] = Field(default_factory=list)\n",
    "\n",
    "    def add_messages(self, messages: List[BaseMessage]) -> None:\n",
    "        \"\"\"Add a list of messages to the store.\"\"\"\n",
    "        self.messages.extend(messages)\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.messages = []\n",
    "\n",
    "# Tenant-specific history store\n",
    "store = {}\n",
    "\n",
    "def get_by_tenant_and_session_id(tenant_id: str, session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"\n",
    "    Retrieves or creates a tenant-specific chat message history for a given session ID.\n",
    "    \"\"\"\n",
    "    key = f\"{tenant_id}:{session_id}\"\n",
    "    if key not in store:\n",
    "        store[key] = InMemoryHistory()\n",
    "    return store[key]\n",
    "\n",
    "# Initialize Pinecone VectorStore\n",
    "def initialize_vectorstore():\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=\"test-index\", embedding=embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "# Define RAG Chain\n",
    "def create_rag_chain(vectorstore, tenant_id: str, llm):\n",
    "    template = \"\"\"Answer the question based only on the following context:\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    Answer: \"\"\"\n",
    "\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_kwargs={\n",
    "            \"k\": 1,\n",
    "            \"filter\": {\n",
    "                \"tenantId\": {\"$eq\": tenant_id}\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return rag_chain\n",
    "\n",
    "# Example of using RAG Chain with tenant-specific message history\n",
    "def main(tenant_id: str, session_id: str, question: str):\n",
    "    # Retrieve tenant-specific history\n",
    "    history = get_by_tenant_and_session_id(tenant_id, session_id)\n",
    "\n",
    "    # Initialize VectorStore\n",
    "    vectorstore = initialize_vectorstore()\n",
    "\n",
    "    # Initialize LLM\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.2,\n",
    "    )\n",
    "\n",
    "    # Create RAG Chain\n",
    "    rag_chain = create_rag_chain(retriever, tenantId, llm)\n",
    "\n",
    "    # Define the prompt with message history\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are an assistant for tenant-specific queries.\"),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Combine RAG Chain and message history\n",
    "    chain_with_history = RunnableWithMessageHistory(\n",
    "        rag_chain,\n",
    "        lambda session_id: get_by_tenant_and_session_id(tenant_id, session_id),\n",
    "        input_messages_key=\"question\",\n",
    "        history_messages_key=\"history\",\n",
    "    )\n",
    "\n",
    "    # Process the question with the RAG chain\n",
    "    response = chain_with_history.invoke(\n",
    "        {\"question\": question},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "\n",
    "    # Add message history\n",
    "    history.add_messages([AIMessage(content=response)])\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from momento import CacheClient, Configurations, CredentialProvider\n",
    "from momento.responses import CacheGet, CacheSet, CreateCache, ListCaches, CacheIncrement\n",
    "\n",
    "import os\n",
    "\n",
    "def create_client():\n",
    "  momento_api_key = CredentialProvider.from_environment_variable('MOMENTO_API_KEY')\n",
    "  ttl  = timedelta(seconds=int(os.getenv('MOMENTO_TTL_SECONDS', '600')))\n",
    "  config = {\n",
    "    'configuration': Configurations.Laptop.v1(),\n",
    "    'credential_provider': momento_api_key,\n",
    "    'default_ttl':  ttl\n",
    "  }\n",
    "  return CacheClient.create(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cache(client, cache_name: str) -> None:\n",
    "    resp = client.create_cache(cache_name)\n",
    "    \n",
    "    if isinstance(resp, CreateCache.Success):\n",
    "        print(\"Cache created.\")\n",
    "    elif isinstance(resp, CreateCache.Error):\n",
    "        print(f\"Error creating cache: {resp.message}\")\n",
    "    else:\n",
    "        print(f\"Unreachable with {resp.message}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionDeniedException",
     "evalue": "PermissionDeniedException(message='Unauthorized: Token is not authorized to make this call', error_code=<MomentoErrorCode.PERMISSION_ERROR: (6,)>, transport_details=MomentoErrorTransportDetails(grpc=MomentoGrpcErrorDetails(code=<StatusCode.PERMISSION_DENIED: (7, 'permission denied')>, details='Unauthorized: Token is not authorized to make this call', metadata=Metadata((('err', 'momento_general_err'), ('content-length', '0'), ('vary', 'origin, access-control-request-method, access-control-request-headers'), ('access-control-allow-credentials', 'true'), ('access-control-expose-headers', 'grpc-status,grpc-message,grpc-encoding,grpc-accept-encoding,err'), ('date', 'Thu, 28 Nov 2024 00:18:07 GMT'))))), message_wrapper='Insufficient permissions to perform an operation on the cache')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionDeniedException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[250], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m cache_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m ttl \u001b[38;5;241m=\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mMomentoChatMessageHistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_client_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mttl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m history\u001b[38;5;241m.\u001b[39madd_user_message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhi!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m history\u001b[38;5;241m.\u001b[39madd_ai_message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhats up?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_community/chat_message_histories/momento.py:124\u001b[0m, in \u001b[0;36mMomentoChatMessageHistory.from_client_params\u001b[0;34m(cls, session_id, cache_name, ttl, configuration, api_key, auth_token, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m credentials \u001b[38;5;241m=\u001b[39m CredentialProvider\u001b[38;5;241m.\u001b[39mfrom_string(api_key)\n\u001b[1;32m    123\u001b[0m cache_client \u001b[38;5;241m=\u001b[39m CacheClient(configuration, credentials, default_ttl\u001b[38;5;241m=\u001b[39mttl)\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mttl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mttl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_community/chat_message_histories/momento.py:85\u001b[0m, in \u001b[0;36mMomentoChatMessageHistory.__init__\u001b[0;34m(self, session_id, cache_client, cache_name, key_prefix, ttl, ensure_cache_exists)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_client must be a momento.CacheClient object.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_cache_exists:\n\u001b[0;32m---> 85\u001b[0m     \u001b[43m_ensure_cache_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m=\u001b[39m key_prefix \u001b[38;5;241m+\u001b[39m session_id\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_client \u001b[38;5;241m=\u001b[39m cache_client\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_community/chat_message_histories/momento.py:34\u001b[0m, in \u001b[0;36m_ensure_cache_exists\u001b[0;34m(cache_client, cache_name)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(create_cache_response, CreateCache\u001b[38;5;241m.\u001b[39mError):\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m create_cache_response\u001b[38;5;241m.\u001b[39minner_exception\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected response cache creation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcreate_cache_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mPermissionDeniedException\u001b[0m: PermissionDeniedException(message='Unauthorized: Token is not authorized to make this call', error_code=<MomentoErrorCode.PERMISSION_ERROR: (6,)>, transport_details=MomentoErrorTransportDetails(grpc=MomentoGrpcErrorDetails(code=<StatusCode.PERMISSION_DENIED: (7, 'permission denied')>, details='Unauthorized: Token is not authorized to make this call', metadata=Metadata((('err', 'momento_general_err'), ('content-length', '0'), ('vary', 'origin, access-control-request-method, access-control-request-headers'), ('access-control-allow-credentials', 'true'), ('access-control-expose-headers', 'grpc-status,grpc-message,grpc-encoding,grpc-accept-encoding,err'), ('date', 'Thu, 28 Nov 2024 00:18:07 GMT'))))), message_wrapper='Insufficient permissions to perform an operation on the cache')"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "from langchain_community.chat_message_histories import MomentoChatMessageHistory\n",
    "\n",
    "session_id = \"foo\"\n",
    "cache_name = \"langchain\"\n",
    "ttl = timedelta(days=1)\n",
    "history = MomentoChatMessageHistory.from_client_params(\n",
    "    session_id,\n",
    "    cache_name,\n",
    "    ttl,\n",
    ")\n",
    "\n",
    "history.add_user_message(\"hi!\")\n",
    "\n",
    "history.add_ai_message(\"whats up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-redis langchain-openai redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Redis at: redis://localhost:6379\n"
     ]
    }
   ],
   "source": [
    "if os.environ.get(\"ENV\") == \"production\":\n",
    "    REDIS_URL = \"redis://production-redis-server:6379\"\n",
    "elif os.environ.get(\"ENV\") == \"staging\":\n",
    "    REDIS_URL = \"redis://staging-redis-server:6379\"\n",
    "\n",
    "print(f\"Connecting to Redis at: {REDIS_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_redis import RedisChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "unknown command 'FT.INFO', with args beginning with: 'idx:chat_history' ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[282], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize RedisChatMessageHistory\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mRedisChatMessageHistory\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser123\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mredis_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mREDIS_URL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Add messages to the history\u001b[39;00m\n\u001b[1;32m      5\u001b[0m history\u001b[38;5;241m.\u001b[39madd_user_message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, AI assistant!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_redis/chat_message_history.py:100\u001b[0m, in \u001b[0;36mRedisChatMessageHistory.__init__\u001b[0;34m(self, session_id, redis_url, key_prefix, ttl, index_name, redis_client, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mttl \u001b[38;5;241m=\u001b[39m ttl\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_name \u001b[38;5;241m=\u001b[39m index_name\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_redis/chat_message_history.py:108\u001b[0m, in \u001b[0;36mRedisChatMessageHistory._ensure_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ensure_index\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mredis_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mft\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ResponseError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown index name\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/redis/commands/search/commands.py:452\u001b[0m, in \u001b[0;36mSearchCommands.info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfo\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    445\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03m    Get info an stats about the the current index, including the number of\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03m    documents, memory consumption, etc\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m    For more information see `FT.INFO <https://redis.io/commands/ft.info>`_.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 452\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINFO_CMD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_results(INFO_CMD, res)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/redis/client.py:559\u001b[0m, in \u001b[0;36mRedis.execute_command\u001b[0;34m(self, *args, **options)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions):\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/redis/client.py:567\u001b[0m, in \u001b[0;36mRedis._execute_command\u001b[0;34m(self, *args, **options)\u001b[0m\n\u001b[1;32m    565\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection \u001b[38;5;129;01mor\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mget_connection(command_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_command_parse_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_disconnect_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/redis/retry.py:62\u001b[0m, in \u001b[0;36mRetry.call_with_retry\u001b[0;34m(self, do, fail)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_supported_errors \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m     64\u001b[0m         failures \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/redis/client.py:568\u001b[0m, in \u001b[0;36mRedis._execute_command.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    565\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection \u001b[38;5;129;01mor\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mget_connection(command_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mretry\u001b[38;5;241m.\u001b[39mcall_with_retry(\n\u001b[0;32m--> 568\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_command_parse_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    571\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m error: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disconnect_raise(conn, error),\n\u001b[1;32m    572\u001b[0m     )\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/redis/client.py:542\u001b[0m, in \u001b[0;36mRedis._send_command_parse_response\u001b[0;34m(self, conn, command_name, *args, **options)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03mSend a command and parse the response\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m conn\u001b[38;5;241m.\u001b[39msend_command(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m--> 542\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/redis/client.py:584\u001b[0m, in \u001b[0;36mRedis.parse_response\u001b[0;34m(self, connection, command_name, **options)\u001b[0m\n\u001b[1;32m    582\u001b[0m         options\u001b[38;5;241m.\u001b[39mpop(NEVER_DECODE)\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ResponseError:\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m EMPTY_RESPONSE \u001b[38;5;129;01min\u001b[39;00m options:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/redis/connection.py:616\u001b[0m, in \u001b[0;36mAbstractConnection.read_response\u001b[0;34m(self, disable_decoding, disconnect_on_error, push_request)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, ResponseError):\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 616\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m response\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    618\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m response  \u001b[38;5;66;03m# avoid creating ref cycles\u001b[39;00m\n",
      "\u001b[0;31mResponseError\u001b[0m: unknown command 'FT.INFO', with args beginning with: 'idx:chat_history' "
     ]
    }
   ],
   "source": [
    "# Initialize RedisChatMessageHistory\n",
    "history = RedisChatMessageHistory(session_id=\"user123\", redis_url=REDIS_URL)\n",
    "\n",
    "# Add messages to the history\n",
    "history.add_user_message(\"Hello, AI assistant!\")\n",
    "history.add_ai_message(\"Hello! How can I assist you today?\")\n",
    "\n",
    "# Retrieve messages\n",
    "print(\"Chat History:\")\n",
    "for message in history.messages:\n",
    "    print(f\"{type(message).__name__}: {message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import pylibmagic\n",
    "import io\n",
    "import pinecone\n",
    "import shutil\n",
    "import uvicorn\n",
    "import numpy as np\n",
    "import os\n",
    "import pinecone\n",
    "import glob\n",
    "import time\n",
    "import getpass\n",
    "import concurrent.futures\n",
    "import docx\n",
    "import json\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from fastapi import  Form\n",
    "from uuid import uuid4\n",
    "from typing import List, Dict\n",
    "from typing import Any\n",
    "from fastapi.responses import JSONResponse\n",
    "from PyPDF2 import PdfReader  \n",
    "from fastapi import Form, BackgroundTasks\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from fastapi.responses import StreamingResponse\n",
    "from fastapi.responses import PlainTextResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi import Request\n",
    "from fastapi import BackgroundTasks\n",
    "from fastapi import Query\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv\n",
    "from fastapi.responses import RedirectResponse\n",
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langserve import add_routes\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.runnables import Runnable\n",
    "from pydantic import BaseModel, Field, validator, ValidationError\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "from fastapi import UploadFile, File, HTTPException\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.retrievers import PineconeHybridSearchRetriever\n",
    "\n",
    "class MyModel(BaseModel):\n",
    "    message: BaseMessage\n",
    "\n",
    "\n",
    "class Config:\n",
    "    arbitrary_types_allowed = True\n",
    "\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"o1\",\n",
    "    description=\"\",\n",
    ")\n",
    "# Set all CORS enabled origins\n",
    "origins = [\"*\"]\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=origins,\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def add_process_time_header(request: Request, call_next):\n",
    "    start_time = time.perf_counter()\n",
    "    response = await call_next(request)\n",
    "    process_time = time.perf_counter() - start_time\n",
    "    response.headers[\"Processing-Time\"] = str(process_time)\n",
    "    return response\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pinecone_index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "# langchain_tracing = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "# langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "# langchain_project = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "# langchain_endpoint = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "\n",
    "if openai_api_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "if pinecone_api_key:\n",
    "    os.environ[\"PINECONE_API_KEY\"] = pinecone_api_key\n",
    "if pinecone_index_name:\n",
    "    os.environ[\"PINECONE_INDEX_NAME\"] = pinecone_index_name\n",
    "# if langchain_tracing:\n",
    "#     os.environ[\"LANGCHAIN_TRACING_V2\"] = langchain_tracing\n",
    "# if langchain_api_key:\n",
    "#     os.environ[\"LANGCHAIN_API_KEY\"] = langchain_api_key  \n",
    "# if langchain_project:\n",
    "#     os.environ[\"LANGCHAIN_PROJECT\"] = langchain_project    \n",
    "# if langchain_endpoint:\n",
    "#     os.environ[\"LANGCHAIN_ENDPOINT\"] = langchain_endpoint             \n",
    "\n",
    "# Verify that the keys are loaded\n",
    "# print(f\"OpenAI API Key: {os.environ.get('OPENAI_API_KEY')}\")\n",
    "# print(f\"Pinecone API Key: {os.environ.get('PINECONE_API_KEY')}\")\n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "index_name = os.environ.get(\"PINECONE_INDEX_NAME\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=3072,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "############################# upload #############################\n",
    "\n",
    "############################# upload #############################\n",
    "\n",
    "############################# upload #############################\n",
    "@app.post(\"/api/artificial-intelligence/upload\")\n",
    "async def upload_files(\n",
    "    tenantId: str = Form(...),\n",
    "    files: List[UploadFile] = File(...),\n",
    "    background_task: BackgroundTasks = BackgroundTasks(),\n",
    "):\n",
    "    \"\"\"Upload multiple PDF, DOCX, and TXT files\"\"\"\n",
    "    dir_name = str(uuid4())\n",
    "\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "    # Allowed file extensions\n",
    "    allowed_extensions = {\".pdf\", \".docx\", \".txt\"}\n",
    "\n",
    "    # Tracking file names to check for duplicates\n",
    "    fileName = set()\n",
    "\n",
    "    # Saving each uploaded file\n",
    "    for file in files:\n",
    "        if file.filename in fileName:\n",
    "            raise HTTPException(\n",
    "                status_code=400, detail=f\"Duplicate file detected: {file.filename}\"\n",
    "            )\n",
    "\n",
    "        fileName.add(file.filename)\n",
    "\n",
    "        # Check for file extension\n",
    "        _, extension = os.path.splitext(file.filename)\n",
    "        if extension.lower() not in allowed_extensions:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=f\"Invalid file type: {file.filename}. Only PDF, DOCX, and TXT files are allowed.\",\n",
    "            )\n",
    "\n",
    "        # Define the destination path for the uploaded file\n",
    "        destination = os.path.join(dir_name, file.filename)\n",
    "\n",
    "        print('creating doc ' + destination)\n",
    "        # Save the uploaded file to the tenant's directory\n",
    "        with open(destination, \"wb\") as buffer:\n",
    "            shutil.copyfileobj(file.file, buffer)\n",
    "\n",
    "\n",
    "\n",
    "    # Load the documents from the tenant's directory\n",
    "    docs = load_docs(dir_name, tenantId)\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\", #response time is 9s  #infloat/e5-base-V2 has 3.53sec response time.\n",
    "    )\n",
    "    vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "\n",
    "    chunk_size = 1000 \n",
    "    chunk_overlap = 20\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    for doc in docs:\n",
    "        curr_split_docs = text_splitter.split_documents([doc])\n",
    "\n",
    "         # Generate a unique document ID\n",
    "        document_id = str(uuid4())\n",
    "\n",
    "        # Create unique IDs for each chunk with the document ID as a prefix\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "\n",
    "        split_docs = split_docs + curr_split_docs\n",
    "        chunk_ids = chunk_ids + curr_chunk_ids\n",
    "\n",
    "        upload_documents[document_id] =  {\"fileName\": doc.metadata['filename'], \"id\": document_id, \"tenantId\": tenantId}\n",
    "\n",
    "    # Add documents to vector store with unique chunk IDs\n",
    "    vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "\n",
    "    shutil.rmtree(dir_name)\n",
    "\n",
    "    return {\"status\": \"success\", \"message\": \"Files uploaded successfully.\"}\n",
    "\n",
    "    \n",
    "\n",
    "def load_docs(directory, tenantId):\n",
    "    loader = DirectoryLoader(directory)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc.metadata['tenantId'] = tenantId\n",
    "\n",
    "        doc.metadata['filename'] = os.path.basename(doc.metadata['source'])\n",
    "        print(doc.metadata['filename'])\n",
    "\n",
    "    return docs\n",
    "\n",
    "############################# Web-URL #############################\n",
    "\n",
    "############################# Web-URL #############################\n",
    "\n",
    "############################# Web-URL #############################\n",
    "@app.post(\"/api/artificial-intelligence/websites\")\n",
    "async def upload_web_urls(\n",
    "    tenantId: str = Form(...),\n",
    "    urls: List[str] = Form(...),\n",
    "):\n",
    "    \"\"\"\n",
    "    Upload and process web URLs, then store content in Pinecone.\n",
    "    \"\"\"\n",
    "    all_docs = []  \n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            loader = WebBaseLoader(url)\n",
    "            docs = loader.load()  \n",
    "            \n",
    "            for doc in docs:\n",
    "                doc.metadata[\"tenantId\"] = tenantId\n",
    "                doc.metadata[\"source\"] = url\n",
    "            all_docs.extend(docs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading URL {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not all_docs:\n",
    "        raise HTTPException(status_code=400, detail=\"No valid URLs provided or failed to fetch content.\")\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "\n",
    "    chunk_size = 1000\n",
    "    chunk_overlap = 20\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    for doc in all_docs:\n",
    "        curr_split_docs = text_splitter.split_documents([doc])\n",
    "\n",
    "        # Generate a unique document ID\n",
    "        document_id = str(uuid.uuid4())\n",
    "\n",
    "        # Create unique IDs for each chunk with the document ID as a prefix\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "\n",
    "        split_docs.extend(curr_split_docs)\n",
    "        chunk_ids.extend(curr_chunk_ids)\n",
    "\n",
    "        upload_documents[document_id] = {\"source\": doc.metadata[\"source\"], \"id\": document_id, \"tenantId\": tenantId}\n",
    "\n",
    "    # Add documents to vector store with unique chunk IDs\n",
    "    vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "\n",
    "    return {\"status\": \"success\", \"message\": \"URLs processed and stored successfully.\"}\n",
    "\n",
    "############################# Add Q&A #############################\n",
    "\n",
    "############################# Add Q&A #############################\n",
    "\n",
    "############################# Add Q&A #############################\n",
    "@app.post(\"/api/artificial-intelligence/Add Q&A\")\n",
    "async def upload_question_answer(\n",
    "    tenantId: str = Form(...),\n",
    "    question: List[str] = Form(...),\n",
    "    answer: List[str] = Form(...),\n",
    "    background_task: BackgroundTasks = BackgroundTasks(),\n",
    "):\n",
    "    \"\"\"\n",
    "    Upload and process a question and answer pair and store it in Pinecone.\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "    chunk_size = 1000\n",
    "    chunk_overlap = 20\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    try:\n",
    "        document_id = str(uuid.uuid4())\n",
    "\n",
    "        combined_text = f\"Question: {question}\\nAnswer: {answer}\"\n",
    "\n",
    "        document_id = str(uuid.uuid4())\n",
    "\n",
    "        metadata = {\n",
    "            \"tenantId\": tenantId,\n",
    "            \"fileName\": f\"Q&A_input_{document_id}.txt\",\n",
    "            \"id\": document_id,\n",
    "            \"question\": question[0],  \n",
    "            \"answer\": answer[0], \n",
    "\n",
    "        }\n",
    "\n",
    "        document = Document(page_content=combined_text, metadata=metadata)\n",
    "\n",
    "        curr_split_docs = text_splitter.split_documents([document])\n",
    "\n",
    "\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "\n",
    "        split_docs += curr_split_docs\n",
    "        chunk_ids += curr_chunk_ids\n",
    "\n",
    "        upload_documents[document_id] = metadata\n",
    "        \n",
    "\n",
    "        # Add to Pinecone vector store\n",
    "        vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump(upload_documents, f, indent=4)\n",
    "\n",
    "        #return {\"status\": \"success\", \"message\": \"Question and Answer processed and stored successfully.\"}\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"Question and Answer processed  successfully.\",\n",
    "            \"id\": document_id,\n",
    "            \"question\": question[0],  \n",
    "            \"answer\": answer[0]       \n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error processing the question and answer: {str(e)}\")\n",
    " \n",
    "############################# tenant-files #############################\n",
    "\n",
    "############################# tenant-files #############################\n",
    "\n",
    "############################# tenant-files #############################\n",
    "@app.get(\"/api/artificial-intelligence/tenant_files\")\n",
    "async def retrieve_files(tenantId: str = Query(...)):\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    tenantFiles = []\n",
    "    for item in list(upload_documents.values()):\n",
    "        if item.get('tenantId') == tenantId:\n",
    "            tenantFiles.append(item)\n",
    "\n",
    "    return {\"data\": tenantFiles}\n",
    "\n",
    "############################# prompt #############################\n",
    "\n",
    "############################# prompt #############################\n",
    "\n",
    "############################# prompt #############################\n",
    "def initializeVectorStore():\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",                            \n",
    "    )\n",
    "    vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    #model=\"gpt-4o\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "@app.get(\"/api/artificial-intelligence/prompts\")\n",
    "async def prompts_keyword(tenantId: str = Query(...), keyword: str = Query(...)):\n",
    "    try:\n",
    "        print(tenantId, keyword)\n",
    "        vectorStore = initializeVectorStore()\n",
    "        retriever = vectorStore.as_retriever(\n",
    "    \n",
    "    search_kwargs={\n",
    "        \"k\": 1,\n",
    "            \"filter\" : {\n",
    "        'tenantId': {'$eq': tenantId}  \n",
    "    \n",
    "    },\n",
    "            }\n",
    ")      \n",
    "        newQa = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            return_source_documents=False,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        answer = newQa({\"query\": keyword})\n",
    "    #For JSON Output\n",
    "    #     return {\"answer\": answer}\n",
    "    # except Exception as e:\n",
    "    #     return {\"error\": str(e)}\n",
    "    \n",
    "    #For Plain Text Output\n",
    "        return PlainTextResponse(answer[\"result\"]) \n",
    "\n",
    "    except Exception as e:\n",
    "        return PlainTextResponse(\n",
    "            f\"Error: {str(e)}. Please contact the support team for assistance.\"\n",
    "        )\n",
    "\n",
    "############################# Delete #############################\n",
    "\n",
    "############################# Delete #############################\n",
    "\n",
    "############################# Delete #############################\n",
    "class DeleteRequest(BaseModel):\n",
    "    prefix: str\n",
    "@app.delete(\"/api/artificial-intelligence/delete\", summary=\"Delete documents by embeddedId\", description=\"Delete all documents in the Pinecone index that match the given prefix.\")\n",
    "async def delete_documents(request: DeleteRequest):\n",
    "    \"\"\"\n",
    "    Deletes all document IDs in the Pinecone index that start with the given prefix.\n",
    "\n",
    "    Parameters:\n",
    "    - **prefix**: The prefix string to filter and delete documents by.\n",
    "\n",
    "    Returns:\n",
    "    - A JSON response with a success message or an error if no IDs were found with the given prefix.\n",
    "    \"\"\"\n",
    "    prefix = request.prefix\n",
    "    # List all IDs with the given prefix\n",
    "    ids_to_delete = [id for id in index.list(prefix=prefix)]\n",
    "    \n",
    "    print(ids_to_delete)\n",
    "\n",
    "    if not ids_to_delete:\n",
    "        raise HTTPException(status_code=404, detail=\"No documents found with the given prefix.\")\n",
    "\n",
    "    # Delete the IDs from the index\n",
    "    index.delete(ids=ids_to_delete)\n",
    "\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    if prefix in upload_documents:\n",
    "        del upload_documents[prefix]\n",
    "    \n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "\n",
    "    return {\"message\": f\"Deleted {len(ids_to_delete)} documents with prefix '{prefix}'.\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### Dev #####\n",
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "#from langserve import add_routes\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "from fastapi import UploadFile, File, HTTPException\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from typing import Any\n",
    "import uuid\n",
    "import pylibmagic\n",
    "from fastapi import Form, BackgroundTasks\n",
    "from fastapi import Query\n",
    "import io\n",
    "import pinecone\n",
    "from PyPDF2 import PdfReader\n",
    "import docx\n",
    "import shutil\n",
    "import uvicorn\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.retrievers import PineconeHybridSearchRetriever\n",
    "from pinecone import ServerlessSpec\n",
    "# from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "# from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import Runnable\n",
    "from pydantic import BaseModel, Field, validator, ValidationError\n",
    "from tqdm.auto import tqdm\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv\n",
    "from fastapi.responses import RedirectResponse, JSONResponse\n",
    "import pinecone\n",
    "import glob\n",
    "from fastapi import FastAPI, File, UploadFile, HTTPException, Form\n",
    "from fastapi.responses import JSONResponse\n",
    "from uuid import uuid4\n",
    "import time\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi import FastAPI, BackgroundTasks, UploadFile, File, Form, HTTPException\n",
    "import getpass\n",
    "import os\n",
    "import concurrent.futures\n",
    "from fastapi import FastAPI, UploadFile, File, Form\n",
    "from fastapi.responses import JSONResponse\n",
    "from PyPDF2 import PdfReader  # For reading PDF files\n",
    "import docx\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "\n",
    "class MyModel(BaseModel):\n",
    "    message: BaseMessage\n",
    "\n",
    "\n",
    "class Config:\n",
    "    arbitrary_types_allowed = True\n",
    "\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"o1\",\n",
    "    description=\"\",\n",
    ")\n",
    "# Set all CORS enabled origins\n",
    "origins = [\"*\"]\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=origins,\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def add_process_time_header(request: Request, call_next):\n",
    "    start_time = time.perf_counter()\n",
    "    response = await call_next(request)\n",
    "    process_time = time.perf_counter() - start_time\n",
    "    response.headers[\"Processing-Time\"] = str(process_time)\n",
    "    return response\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pinecone_index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if openai_api_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "if pinecone_api_key:\n",
    "    os.environ[\"PINECONE_API_KEY\"] = pinecone_api_key\n",
    "if pinecone_index_name:\n",
    "    os.environ[\"PINECONE_INDEX_NAME\"] = pinecone_index_name\n",
    "\n",
    "# Verify that the keys are loaded\n",
    "# print(f\"OpenAI API Key: {os.environ.get('OPENAI_API_KEY')}\")\n",
    "# print(f\"Pinecone API Key: {os.environ.get('PINECONE_API_KEY')}\")\n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "index_name = os.environ.get(\"PINECONE_INDEX_NAME\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "import time\n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=3072,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@app.post(\"/api/artificial-intelligence/upload\")\n",
    "async def upload_files(\n",
    "    tenantId: str = Form(...),\n",
    "    files: List[UploadFile] = File(...),\n",
    "    background_task: BackgroundTasks = BackgroundTasks(),\n",
    "):\n",
    "    \"\"\"Upload multiple PDF, DOCX, and TXT files\"\"\"\n",
    "    dir_name = str(uuid4())\n",
    "\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "    # Allowed file extensions\n",
    "    allowed_extensions = {\".pdf\", \".docx\", \".txt\"}\n",
    "\n",
    "    # Tracking file names to check for duplicates\n",
    "    fileName = set()\n",
    "\n",
    "    # Saving each uploaded file\n",
    "    for file in files:\n",
    "        if file.filename in fileName:\n",
    "            raise HTTPException(\n",
    "                status_code=400, detail=f\"Duplicate file detected: {file.filename}\"\n",
    "            )\n",
    "\n",
    "        fileName.add(file.filename)\n",
    "\n",
    "        # Check for file extension\n",
    "        _, extension = os.path.splitext(file.filename)\n",
    "        if extension.lower() not in allowed_extensions:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=f\"Invalid file type: {file.filename}. Only PDF, DOCX, and TXT files are allowed.\",\n",
    "            )\n",
    "\n",
    "        # Define the destination path for the uploaded file\n",
    "        destination = os.path.join(dir_name, file.filename)\n",
    "\n",
    "        print('creating doc ' + destination)\n",
    "        # Save the uploaded file to the tenant's directory\n",
    "        with open(destination, \"wb\") as buffer:\n",
    "            shutil.copyfileobj(file.file, buffer)\n",
    "\n",
    "\n",
    "\n",
    "    # Load the documents from the tenant's directory\n",
    "    docs = load_docs(dir_name, tenantId)\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\", #response time is 9s  #infloat/e5-base-V2 has 3.53sec response time.\n",
    "    )\n",
    "    vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "\n",
    "    chunk_size = 1000 \n",
    "    chunk_overlap = 20\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    for doc in docs:\n",
    "        curr_split_docs = text_splitter.split_documents([doc])\n",
    "\n",
    "         # Generate a unique document ID\n",
    "        document_id = str(uuid4())\n",
    "\n",
    "        # Create unique IDs for each chunk with the document ID as a prefix\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "\n",
    "        split_docs = split_docs + curr_split_docs\n",
    "        chunk_ids = chunk_ids + curr_chunk_ids\n",
    "\n",
    "        upload_documents[document_id] =  {\"fileName\": doc.metadata['filename'], \"id\": document_id, \"tenantId\": tenantId}\n",
    "\n",
    "    # Add documents to vector store with unique chunk IDs\n",
    "    vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "\n",
    "    shutil.rmtree(dir_name)\n",
    "\n",
    "    return {\"status\": \"success\", \"message\": \"Files uploaded successfully.\"}\n",
    "\n",
    "    \n",
    "\n",
    "def load_docs(directory, tenantId):\n",
    "    loader = DirectoryLoader(directory)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc.metadata['tenantId'] = tenantId\n",
    "\n",
    "        # Extract the filename from the 'source' path\n",
    "        doc.metadata['filename'] = os.path.basename(doc.metadata['source'])\n",
    "        print(\"ooooooooo\" + doc.metadata['filename'])\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################# Web-URL #############################\n",
    "\n",
    "############################# Web-URL #############################\n",
    "\n",
    "############################# Web-URL #############################\n",
    "@app.post(\"/api/artificial-intelligence/links\")\n",
    "async def upload_web_urls(\n",
    "    tenantId: str = Form(...),\n",
    "    urls: List[str] = Form(...),\n",
    "):\n",
    "    \"\"\"\n",
    "    Upload and process web URLs, then store content in Pinecone.\n",
    "    \"\"\"\n",
    "    all_docs = []  \n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            loader = WebBaseLoader(url)\n",
    "            docs = loader.load()  \n",
    "            \n",
    "            for doc in docs:\n",
    "                doc.metadata[\"tenantId\"] = tenantId\n",
    "                doc.metadata[\"source\"] = url\n",
    "            all_docs.extend(docs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading URL {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not all_docs:\n",
    "        raise HTTPException(status_code=400, detail=\"No valid URLs provided or failed to fetch content.\")\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "\n",
    "    chunk_size = 1000\n",
    "    chunk_overlap = 20\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    for doc in all_docs:\n",
    "        curr_split_docs = text_splitter.split_documents([doc])\n",
    "\n",
    "        # Generate a unique document ID\n",
    "        document_id = str(uuid.uuid4())\n",
    "\n",
    "        # Create unique IDs for each chunk with the document ID as a prefix\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "\n",
    "        split_docs.extend(curr_split_docs)\n",
    "        chunk_ids.extend(curr_chunk_ids)\n",
    "\n",
    "        upload_documents[document_id] = {\"source\": doc.metadata[\"source\"], \"id\": document_id, \"tenantId\": tenantId}\n",
    "\n",
    "    # Add documents to vector store with unique chunk IDs\n",
    "    vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "\n",
    "    return {\"status\": \"success\", \"message\": \"URLs processed and stored successfully.\"}\n",
    "\n",
    "############################# Add Q&A #############################\n",
    "\n",
    "############################# Add Q&A #############################\n",
    "\n",
    "############################# Add Q&A #############################\n",
    "@app.post(\"/api/artificial-intelligence/qa\")\n",
    "async def upload_question_answer(\n",
    "    tenantId: str = Form(...),\n",
    "    question: List[str] = Form(...),\n",
    "    answer: List[str] = Form(...),\n",
    "    background_task: BackgroundTasks = BackgroundTasks(),\n",
    "):\n",
    "    \"\"\"\n",
    "    Upload and process a question and answer pair and store it in Pinecone.\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "    chunk_size = 1000\n",
    "    chunk_overlap = 20\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    try:\n",
    "        document_id = str(uuid.uuid4())\n",
    "\n",
    "        combined_text = f\"Question: {question}\\nAnswer: {answer}\"\n",
    "\n",
    "        document_id = str(uuid.uuid4())\n",
    "\n",
    "        metadata = {\n",
    "            \"tenantId\": tenantId,\n",
    "            \"fileName\": f\"Q&A_input_{document_id}.txt\",\n",
    "            \"id\": document_id,\n",
    "            \"question\": question[0],  \n",
    "            \"answer\": answer[0], \n",
    "\n",
    "        }\n",
    "\n",
    "        document = Document(page_content=combined_text, metadata=metadata)\n",
    "\n",
    "        curr_split_docs = text_splitter.split_documents([document])\n",
    "\n",
    "\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "\n",
    "        split_docs += curr_split_docs\n",
    "        chunk_ids += curr_chunk_ids\n",
    "\n",
    "        upload_documents[document_id] = metadata\n",
    "        \n",
    "\n",
    "        # Add to Pinecone vector store\n",
    "        vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump(upload_documents, f, indent=4)\n",
    "\n",
    "        #return {\"status\": \"success\", \"message\": \"Question and Answer processed and stored successfully.\"}\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"Question and Answer processed  successfully.\",\n",
    "            \"id\": document_id,\n",
    "            \"question\": question[0],  \n",
    "            \"answer\": answer[0]       \n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error processing the question and answer: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "@app.get(\"/api/artificial-intelligence/tenant_files\")\n",
    "async def retrieve_files(tenantId: str = Query(...)):\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    tenantFiles = []\n",
    "    for item in list(upload_documents.values()):\n",
    "        if item.get('tenantId') == tenantId:\n",
    "            tenantFiles.append(item)\n",
    "\n",
    "    return {\"data\": tenantFiles}\n",
    "\n",
    "\n",
    "\n",
    "def initializeVectorStore():\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",                             #response time is 9s  #infloat/e5-base-V2 has 3.53sec response time.\n",
    "    )\n",
    "    vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    #model=\"gpt-4o\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "@app.get(\"/api/artificial-intelligence/prompts\")\n",
    "async def prompts_keyword(tenantId: str = Query(...), keyword: str = Query(...)):\n",
    "    try:\n",
    "        print(tenantId, keyword)\n",
    "        vectorStore = initializeVectorStore()\n",
    "        #retriever = vectorStore.as_retriever()\n",
    "        retriever = vectorStore.as_retriever(\n",
    "    \n",
    "    #search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 1,\n",
    "            \"filter\" : {\n",
    "        'tenantId': {'$eq': tenantId}  \n",
    "    \n",
    "    },\n",
    "            }\n",
    ")\n",
    "        newQa = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            #retriever=vectorStore.as_retriever(),\n",
    "            return_source_documents=True,\n",
    "        )\n",
    "\n",
    "        #answer = newQa.invoke({\"query\": keyword})\n",
    "        answer = newQa({\"query\": keyword})\n",
    "\n",
    "\n",
    "        return {\"answer\": answer}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DeleteRequest(BaseModel):\n",
    "    prefix: str\n",
    "@app.delete(\"/api/artificial-intelligence/delete\", summary=\"Delete documents by prefix\", description=\"Delete all documents in the Pinecone index that match the given prefix.\")\n",
    "async def delete_documents(request: DeleteRequest):\n",
    "    \"\"\"\n",
    "    Deletes all document IDs in the Pinecone index that start with the given prefix.\n",
    "\n",
    "    Parameters:\n",
    "    - **prefix**: The prefix string to filter and delete documents by.\n",
    "\n",
    "    Returns:\n",
    "    - A JSON response with a success message or an error if no IDs were found with the given prefix.\n",
    "    \"\"\"\n",
    "    prefix = request.prefix\n",
    "    # List all IDs with the given prefix\n",
    "    ids_to_delete = [id for id in index.list(prefix=prefix)]\n",
    "    \n",
    "    print(ids_to_delete)\n",
    "\n",
    "    if not ids_to_delete:\n",
    "        raise HTTPException(status_code=404, detail=\"No documents found with the given prefix.\")\n",
    "\n",
    "    # Delete the IDs from the index\n",
    "    index.delete(ids=ids_to_delete)\n",
    "\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    if prefix in upload_documents:\n",
    "        del upload_documents[prefix]\n",
    "    \n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "\n",
    "    return {\"message\": f\"Deleted {len(ids_to_delete)} documents with prefix '{prefix}'.\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DeleteRequest(BaseModel):\n",
    "    prefix: str\n",
    "@app.delete(\"/api/artificial-intelligence/qa\", summary=\"Delete QA by prefix\", description=\"Delete all documents in the Pinecone index that match the given prefix.\")\n",
    "async def delete_documents(request: DeleteRequest):\n",
    "    \"\"\"\n",
    "    Deletes all QA IDs in the Pinecone index that start with the given prefix.\n",
    "\n",
    "    Parameters:\n",
    "    - **prefix**: The prefix string to filter and delete QA by.\n",
    "\n",
    "    Returns:\n",
    "    - A JSON response with a success message or an error if no IDs were found with the given prefix.\n",
    "    \"\"\"\n",
    "    prefix = request.prefix\n",
    "    # List all IDs with the given prefix\n",
    "    ids_to_delete = [id for id in index.list(prefix=prefix)]\n",
    "    \n",
    "    print(ids_to_delete)\n",
    "\n",
    "    if not ids_to_delete:\n",
    "        raise HTTPException(status_code=404, detail=\"No QA found with the given prefix.\")\n",
    "\n",
    "    # Delete the IDs from the index\n",
    "    index.delete(ids=ids_to_delete)\n",
    "\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "\n",
    "    if prefix in upload_documents:\n",
    "        del upload_documents[prefix]\n",
    "    \n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "\n",
    "    return {\"message\": f\"Deleted {len(ids_to_delete)} QA with prefix '{prefix}'.\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initializeVectorStore():\n",
    "    \"\"\"Initialize and configure vector store with embeddings\n",
    "    Returns:\n",
    "        Configured PineconeVectorStore instance\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "@app.get(\"/api/artificial-intelligence/prompts\")\n",
    "async def prompts_keyword(tenantId: str = Query(...), keyword: str = Query(...)):\n",
    "    \"\"\"Process keyword prompts and retrieve answers using improved RAG chain\n",
    "    Args:\n",
    "        tenantId: Identifier for the tenant\n",
    "        keyword: Search keyword/prompt  \n",
    "    Returns:\n",
    "        PlainText response with answer or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(tenantId, keyword)\n",
    "        vectorstore = initializeVectorStore()\n",
    "        \n",
    "        # Define the template for more structured responses\n",
    "        template = \"\"\"Answer the question based only on the following context:\n",
    "        {context}\n",
    "        Question: {question}\n",
    "        Answer: Let me help you with that based on the available information.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Configure retriever with tenant filter\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_kwargs={\n",
    "                \"k\": 1,\n",
    "                \"filter\": {\n",
    "                    'tenantId': {'$eq': tenantId}\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Create prompt template\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        \n",
    "        # Build the RAG chain\n",
    "        rag_chain = (\n",
    "            {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        # Execute the chain\n",
    "        answer = rag_chain.invoke(keyword)\n",
    "        \n",
    "        return PlainTextResponse(answer)\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing your request: {str(e)}. Please contact support for assistance.\"\n",
    "        print(f\"Error in prompts_keyword: {str(e)}\")  # For logging\n",
    "        return PlainTextResponse(error_msg, status_code=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def initializeVectorStore():\n",
    "#     \"\"\"Initialize and configure vector store with embeddings\n",
    "#     Returns:\n",
    "#         Configured PineconeVectorStore instance\n",
    "#     \"\"\"\n",
    "#     embeddings = OpenAIEmbeddings(\n",
    "#     model=\"text-embedding-ada-002\")\n",
    "#     vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "#     return vectorstore\n",
    "# llm = ChatOpenAI(\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "#     temperature=0.0)\n",
    "# @app.get(\"/api/artificial-intelligence/prompts\")\n",
    "# async def prompts_keyword(tenantId: str = Query(...), keyword: str = Query(...)):\n",
    "#     \"\"\"Process keyword prompts and retrieve answers\n",
    "#     Args:\n",
    "#         tenantId: Identifier for the tenant\n",
    "#         keyword: Search keyword/prompt  \n",
    "#     Returns:\n",
    "#         PlainText response with answer or error message\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         print(tenantId, keyword)\n",
    "#         vectorStore = initializeVectorStore()\n",
    "#         retriever = vectorStore.as_retriever(\n",
    "#         search_kwargs={\n",
    "#         \"k\": 1,\n",
    "#             \"filter\" : {\n",
    "#         'tenantId': {'$eq': tenantId} }} )\n",
    "#         newQa = RetrievalQA.from_chain_type(\n",
    "#             llm=llm,\n",
    "#             chain_type=\"stuff\",\n",
    "#             retriever=retriever,\n",
    "#             return_source_documents=False)\n",
    "#         answer = newQa({\"query\": keyword})\n",
    "#     #     return PlainTextResponse(answer[\"result\"]) \n",
    "#     # except Exception as e:\n",
    "#     #     return PlainTextResponse(\n",
    "#     #         f\"Error: {str(e)}. Please contact the support team for assistance.\")\n",
    "#         if not answer[\"result\"] or \"I don't know\" in answer[\"result\"]:\n",
    "#             return PlainTextResponse(\n",
    "#                 \"It might be helpful to consult a subject-matter expert  or refer to specific documentation or references for more detailed information.\"\n",
    "#             )\n",
    "\n",
    "#         return PlainTextResponse(answer[\"result\"])\n",
    "\n",
    "#     except Exception as e:\n",
    "#         return PlainTextResponse(\n",
    "#             f\"Error: {str(e)}. Please contact the support team for assistance.\")\n",
    "\n",
    "def initializeVectorStore():\n",
    "    \"\"\"Initialize and configure vector store with embeddings\n",
    "    Returns:\n",
    "        Configured PineconeVectorStore instance\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0\n",
    ")\n",
    "@app.get(\"/api/artificial-intelligence/prompts\")\n",
    "async def prompts_keyword(tenantId: str = Query(...), keyword: str = Query(...)):\n",
    "    \"\"\"Process keyword prompts and retrieve answers using improved RAG chain\n",
    "    Args:\n",
    "        tenantId: Identifier for the tenant\n",
    "        keyword: Search keyword/prompt  \n",
    "    Returns:\n",
    "        PlainText response with answer or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(tenantId, keyword)\n",
    "        vectorstore = initializeVectorStore()\n",
    "        template = \"\"\"Answer the question based only on the following context:\n",
    "        {context}\n",
    "        Question: {question}\n",
    "        Answer: Answer the same question given by the user during input in /api/artificial-intelligence/qa\n",
    "        If answer is not available then say: I don't have the information you're looking for, please provide additional details.\n",
    "        In response give only answer but not question.\n",
    "        \"\"\"\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_kwargs={\n",
    "                \"k\": 1,\n",
    "                \"filter\": {\n",
    "                    'tenantId': {'$eq': tenantId}\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        rag_chain = (\n",
    "            {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser())\n",
    "        answer = rag_chain.invoke(keyword)\n",
    "        return PlainTextResponse(answer)\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing your request: {str(e)}. Please contact support for assistance.\"\n",
    "        print(f\"Error in prompts_keyword: {str(e)}\") \n",
    "        return PlainTextResponse(error_msg, status_code=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def initializeVectorStore():\n",
    "#     \"\"\"Initialize and configure vector store with embeddings\n",
    "#     Returns:\n",
    "#         Configured PineconeVectorStore instance\n",
    "#     \"\"\"\n",
    "#     embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "#     vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "#     return vectorstore\n",
    "\n",
    "# # Contextualization \n",
    "# contextualize_q_system_prompt = (\n",
    "#     \"Given a chat history and the latest user question \"\n",
    "#     \"which might reference context in the chat history, \"\n",
    "#     \"formulate a standalone question which can be understood \"\n",
    "#     \"without the chat history. Do NOT answer the question, \"\n",
    "#     \"just reformulate it if needed and otherwise return it as is.\"\n",
    "# )\n",
    "\n",
    "# contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", contextualize_q_system_prompt),\n",
    "#     MessagesPlaceholder(\"chat_history\"),\n",
    "#     (\"human\", \"{input}\"),\n",
    "# ])\n",
    "\n",
    "# qa_prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a helpful Vitafy AI assistant. Use the following context to answer the user's question.\"),\n",
    "#     (\"system\", \"Context: {context}\"),\n",
    "#     MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "#     (\"human\", \"{input}\")\n",
    "# ])\n",
    "\n",
    "# # Session Mgmt\n",
    "# class ChatSessionManager:\n",
    "#     _sessions: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "#     @classmethod\n",
    "#     def get_or_create_session(cls, tenantId: str, session_id: str = None):\n",
    "#         \"\"\"\n",
    "#         Get or create a conversation session\n",
    "        \n",
    "#         Args:\n",
    "#             tenantId (str): Tenant identifier\n",
    "#             session_id (str, optional): Existing session ID\n",
    "        \n",
    "#         Returns:\n",
    "#             dict: Session state\n",
    "#         \"\"\"\n",
    "#         if not session_id:\n",
    "#             session_id = str(uuid.uuid4())\n",
    "        \n",
    "#         if session_id not in cls._sessions:\n",
    "#             cls._sessions[session_id] = {\n",
    "#                 \"tenantId\": tenantId,\n",
    "#                 \"chat_history\": [],\n",
    "#                 \"context\": \"\"\n",
    "#             }\n",
    "        \n",
    "#         return session_id, cls._sessions[session_id]\n",
    "\n",
    "#     @classmethod\n",
    "#     def update_session(cls, session_id: str, user_input: str, ai_response: str):\n",
    "#         \"\"\"\n",
    "#         Update session with new interaction\n",
    "        \n",
    "#         Args:\n",
    "#             session_id (str): Session identifier\n",
    "#             user_input (str): User's message\n",
    "#             ai_response (str): AI's response\n",
    "#         \"\"\"\n",
    "#         if session_id in cls._sessions:\n",
    "#             cls._sessions[session_id][\"chat_history\"].extend([\n",
    "#                 {\"role\": \"human\", \"content\": user_input},\n",
    "#                 {\"role\": \"ai\", \"content\": ai_response}\n",
    "#             ])\n",
    "\n",
    "# @app.get(\"/api/artificial-intelligence/prompts\")\n",
    "# async def prompts_keyword(\n",
    "#     tenantId: str = Query(...), \n",
    "#     keyword: str = Query(...),\n",
    "#     session_id: str = Query(default=None)\n",
    "# ):\n",
    "#     \"\"\"Process keyword prompts with enhanced contextual awareness\"\"\"\n",
    "#     try:\n",
    "#         #  sessions mgmt\n",
    "#         session_id, session_state = ChatSessionManager.get_or_create_session(\n",
    "#             tenantId=tenantId, \n",
    "#             session_id=session_id\n",
    "#         )\n",
    "        \n",
    "#         vectorstore = initializeVectorStore()\n",
    "#         retriever = vectorstore.as_retriever(\n",
    "#             search_kwargs={\n",
    "#                 \"k\": 3,  \n",
    "#                 \"filter\": {\n",
    "#                     'tenantId': {'$eq': tenantId}\n",
    "#                 }\n",
    "#             }\n",
    "#         )\n",
    "        \n",
    "#         # chat history for context\n",
    "#         chat_history = [\n",
    "#             HumanMessage(content=msg['content']) if msg['role'] == 'human' \n",
    "#             else AIMessage(content=msg['content']) \n",
    "#             for msg in session_state.get('chat_history', [])\n",
    "#         ]\n",
    "        \n",
    "#         # LLM \n",
    "#         llm = ChatOpenAI(\n",
    "#             model=\"gpt-3.5-turbo\",\n",
    "#             temperature=0.0,\n",
    "#             #max_tokens=150,  # Limit response length\n",
    "#             top_p=1.0,  # Nucleus sampling\n",
    "#             frequency_penalty=0.5,  # Reduce repetition\n",
    "#             stop=[\"\\n\"]) #stop sequence\n",
    "        \n",
    "#         # history-aware retriever\n",
    "#         history_aware_retriever = create_history_aware_retriever(\n",
    "#             llm, \n",
    "#             retriever, \n",
    "#             contextualize_q_prompt\n",
    "#         )\n",
    "        \n",
    "#         # document chain\n",
    "#         question_answer_chain = create_stuff_documents_chain(\n",
    "#             llm, \n",
    "#             qa_prompt\n",
    "#         )\n",
    "        \n",
    "#         # full RAG chain\n",
    "#         rag_chain = create_retrieval_chain(\n",
    "#             history_aware_retriever, \n",
    "#             question_answer_chain\n",
    "#         )\n",
    "        \n",
    "#         result = rag_chain.invoke({\n",
    "#             \"input\": keyword,\n",
    "#             \"chat_history\": chat_history\n",
    "#         })\n",
    "        \n",
    "#         answer = result['answer']\n",
    "        \n",
    "#         # Update session with new interaction\n",
    "#         ChatSessionManager.update_session(\n",
    "#             session_id, \n",
    "#             keyword, \n",
    "#             answer\n",
    "#         )\n",
    "        \n",
    "#         return PlainTextResponse(answer)\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         error_msg = f\"Error processing your request: {str(e)}. Please contact support for assistance.\"\n",
    "#         print(f\"Error in prompts_keyword: {str(e)}\")\n",
    "#         return PlainTextResponse(error_msg, status_code=500)\n",
    "\n",
    "# @app.get(\"/api/artificial-intelligence/chathistory\")\n",
    "# async def get_session_history(session_id: str):\n",
    "#     \"\"\"Retrieve session history\"\"\"\n",
    "#     try:\n",
    "#         session = ChatSessionManager._sessions.get(session_id)\n",
    "#         if not session:\n",
    "#             raise HTTPException(status_code=404, detail=\"Session not found\")\n",
    "        \n",
    "#         return {\n",
    "#             \"chat_history\": session.get('chat_history', []),\n",
    "#             \"tenantId\": session.get('tenantId')\n",
    "#         }\n",
    "#     except Exception as e:\n",
    "#         raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# @app.delete(\"/api/artificial-intelligence/reset-session\")\n",
    "# async def reset_session(session_id: str):\n",
    "#     \"\"\"Reset a specific conversation session\"\"\"\n",
    "#     try:\n",
    "#         if session_id in ChatSessionManager._sessions:\n",
    "#             del ChatSessionManager._sessions[session_id]\n",
    "#         return {\"status\": \"success\", \"message\": \"Session reset successfully\"}\n",
    "#     except Exception as e:\n",
    "#         raise HTTPException(status_code=500, detail=str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/api/artificial-intelligence/links\")\n",
    "async def upload_web_urls(\n",
    "    tenantId: str,\n",
    "    urls: List[str] = Form(...)):\n",
    "    \"\"\"\n",
    "    Process and store web URL content\n",
    "    Args:\n",
    "        tenantId: Identifier for the tenant\n",
    "        urls: List of URLs to process   \n",
    "    Returns:\n",
    "        JSON response indicating processing status   \n",
    "    Raises:\n",
    "        HTTPException: When no valid URLs are provided\n",
    "    \"\"\"\n",
    "    all_docs = []  \n",
    "    for url in urls:\n",
    "        try:\n",
    "            loader = WebBaseLoader(url)\n",
    "            docs = loader.load()  \n",
    "            for doc in docs:\n",
    "                doc.metadata[\"tenantId\"] = tenantId\n",
    "                doc.metadata[\"source\"] = url\n",
    "            all_docs.extend(docs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading URL {url}: {e}\")\n",
    "            continue\n",
    "    if not all_docs:\n",
    "        raise HTTPException(status_code=400, detail=\"No valid URLs provided or failed to fetch content.\")\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo-preview\", temperature=0.0)\n",
    "    chunk_size = 500\n",
    "    chunk_overlap = 50\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    for doc in all_docs:\n",
    "        curr_split_docs = text_splitter.split_documents([doc])\n",
    "        document_id = str(uuid.uuid4())\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "        split_docs.extend(curr_split_docs)\n",
    "        chunk_ids.extend(curr_chunk_ids)\n",
    "        upload_documents[document_id] = {\"source\": doc.metadata[\"source\"], \"id\": document_id, \"tenantId\": tenantId}\n",
    "    vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    return {\"status\": \"success\", \"message\": \"URLs processed and stored successfully.\"}\n",
    "############################# links@edit #############################\n",
    "@app.put(\"/api/artificial-intelligence/links\")\n",
    "async def upload_web_urls(\n",
    "    tenantId: str,\n",
    "    embeddedId: str,\n",
    "    urls: List[str] = Form(...)):\n",
    "    \"\"\"\n",
    "    Update web URL content for a specific embeddedId.\n",
    "    \n",
    "    Args:\n",
    "        tenantId: Identifier for the tenant\n",
    "        embeddedId: The ID of the document to update\n",
    "        urls: List of URLs to be updated\n",
    "    \n",
    "    Returns:\n",
    "        JSON response indicating processing status\n",
    "    \n",
    "    Raises:\n",
    "        HTTPException: When no valid URLs are provided or if the embeddedId does not exist.\n",
    "    \"\"\"\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    \n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    \n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    \n",
    "    if embeddedId not in upload_documents:\n",
    "        raise HTTPException(status_code=404, detail=\"The embeddedId does not exist.\")\n",
    "    \n",
    "    try:\n",
    "        all_docs = []\n",
    "        for url in urls:\n",
    "            try:\n",
    "                loader = WebBaseLoader(url)\n",
    "                docs = loader.load()  \n",
    "                for doc in docs:\n",
    "                    doc.metadata[\"tenantId\"] = tenantId\n",
    "                    doc.metadata[\"source\"] = url\n",
    "                all_docs.extend(docs)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading URL {url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not all_docs:\n",
    "            raise HTTPException(status_code=400, detail=\"No valid URLs provided or failed to fetch content.\")\n",
    "        \n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "        vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "        llm = ChatOpenAI(model=\"gpt-4-turbo-preview\", temperature=0.0)\n",
    "        chunk_size = 500\n",
    "        chunk_overlap = 50\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        split_docs = []\n",
    "        chunk_ids = []\n",
    "        \n",
    "        existing_chunk_ids = [key for key in upload_documents if upload_documents[key]['id'] == embeddedId]\n",
    "        \n",
    "        for chunk_id in existing_chunk_ids:\n",
    "            del upload_documents[chunk_id]  \n",
    "        \n",
    "        for doc in all_docs:\n",
    "            curr_split_docs = text_splitter.split_documents([doc])\n",
    "            document_id = embeddedId \n",
    "            curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "            split_docs.extend(curr_split_docs)\n",
    "            chunk_ids.extend(curr_chunk_ids)\n",
    "            upload_documents[document_id] = {\"source\": doc.metadata[\"source\"], \"id\": document_id, \"tenantId\": tenantId}\n",
    "        \n",
    "        vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump(upload_documents, f, indent=4)\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"URLs updated successfully.\",\n",
    "            \"embeddedId\": embeddedId,\n",
    "            \"updated_urls\": urls\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error updating URLs: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tempfile\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Function to summarize a document\n",
    "def summarize_document(openai_api_key, pinecone_api_key, pinecone_index, source_doc):\n",
    "    try:\n",
    "        # Validate inputs\n",
    "        if not openai_api_key or not pinecone_api_key or not pinecone_index or not source_doc:\n",
    "            raise ValueError(\"Missing required fields or file.\")\n",
    "\n",
    "        # Set environment variables for API keys\n",
    "        os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "        os.environ['PINECONE_API_KEY'] = pinecone_api_key\n",
    "\n",
    "        # Save uploaded file temporarily to disk, load and split the file into pages, delete temp file\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "            tmp_file.write(source_doc.read())\n",
    "        loader = PyPDFLoader(tmp_file.name)\n",
    "        pages = loader.load_and_split()\n",
    "        os.remove(tmp_file.name)\n",
    "\n",
    "        # Create embeddings for the pages and insert into Pinecone vector database\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "        vectorstore = PineconeVectorStore.from_documents(pages, embeddings, index_name=pinecone_index)\n",
    "\n",
    "        # Initialize the ChatOpenAI module, load and run the summarize chain\n",
    "        llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "        chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "        search = vectorstore.similarity_search(\" \")\n",
    "        summary = chain.run(input_documents=search, question=\"Write a concise summary within 200 words.\")\n",
    "\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################ Summarizer #################################################\n",
    "# class SummarizationRequest(BaseModel):\n",
    "#     caseId: str\n",
    "#     onBehalfOf: str\n",
    "#     notes: List[Any]\n",
    "\n",
    "# @app.post(\"/api/artificial-intelligence/summarize\")\n",
    "# async def summarize(request: SummarizationRequest):\n",
    "#     \"\"\"Combine JSON upload and summary generation into one endpoint.\"\"\"\n",
    "#     try:\n",
    "#         json_data = {\"caseId\": request.caseId, \"notes\": request.notes}\n",
    "#         text = \" \".join([str(note) if isinstance(note, str) else json.dumps(note) for note in request.notes])\n",
    "#         word_count = len(text.split())\n",
    "#         if word_count <= 200:\n",
    "#             max_length = 50  \n",
    "#         elif word_count <= 1000:\n",
    "#             max_length = 150  \n",
    "#         else:\n",
    "#             max_length = 300 \n",
    "        \n",
    "#         json_doc = Document(page_content=json.dumps(json_data), metadata={'fileName': 'query_json'})\n",
    "#         split_docs = [json_doc]\n",
    "#         print(f\"Processing {len(split_docs)} documents.\")\n",
    "        \n",
    "#         llm = ChatOpenAI(\n",
    "#             model=\"gpt-3.5-turbo\",\n",
    "#             #model='gpt-4o-mini',\n",
    "#             temperature=0.0,\n",
    "#             max_tokens=max_length,\n",
    "#             top_p=1.0,\n",
    "#         )\n",
    "        \n",
    "#         patientPrompt = \"\"\" Write a detailed and structured summary of the following text:\n",
    "#         \"{split_docs}\"\n",
    "#          These are the conversations that might have happened between the advocates or patients. These notes are not directly sent to me.\n",
    "#         summarize all the notes as if im a patient and address patient as me and include minor details also.\n",
    "#         Be sure to highlight important note, medical details and Recommendations.\n",
    "#         Include the major events in a bullet points if available.\n",
    "\n",
    "#         Complete the sentence at the end with a full stop.\n",
    "\n",
    "#                 \"\"\"\n",
    "\n",
    "#         prompt_template = \"\"\"Write a detailed and structured summary of the following text:\n",
    "#         \"{split_docs}\"\n",
    "#         Please summarize the content above by addressing the following aspects in an integrated narrative format:\n",
    "#         - Introduce the patient, including their full name (if mentioned), age, location, relevant medical history, and current condition at the time of documentation.\n",
    "#         - Document the involvement of family members and advocates in the patient's care, specifying their names, roles, and any significant communications or actions they have taken.\n",
    "#         - Provide a comprehensive account of the patient's treatment history, detailing specific challenges (e.g., mental status changes, medication side effects), responses from the medical team, and any milestones or setbacks in the patient's progress.\n",
    "#         - Highlight the collaboration between medical professionals, family members, and advocates, noting specific meetings, consultations, and shared decisions made to manage the patient's care.\n",
    "#         - Include updates on the case status (e.g., \"IN-PROGRESS,\" \"DISCHARGED\") as well as any recommendations or plans for ongoing care, discharge planning, or long-term health considerations.\n",
    "\n",
    "#         Ensure the summary is:\n",
    "#         - **Comprehensive**: Cover all necessary details to serve as a formal record of the patient's care.\n",
    "#         - **Well-Organized**: Present the information in a logical flow without using section headers, maintaining clarity and readability.\n",
    "#         - **Professional in Tone**: Write in a factual, formal tone suitable for medical or administrative reporting.\n",
    "#         - **Clear and Concise**: Use straightforward language that is easy to understand.\n",
    "#         - **Structured**: Format the summary in paragraph form for readability, ensuring logical flow between points.\n",
    "\n",
    "#         Additionally, please conclude with any recommendations for next steps or ongoing care considerations that were mentioned in the notes.\n",
    "#         The tone should be professional yet empathetic, reflecting the seriousness of the patient's condition while acknowledging the efforts of all parties involved in their care.\n",
    "#         Conclude the summary with a recap of any critical points or follow-up actions necessary to ensure continuity of care.\n",
    "\n",
    "#         Complete the sentence at the end with a full stop.\n",
    "\n",
    "#         \"\"\"\n",
    "        \n",
    "#         if(request.onBehalfOf == \"patient\"):\n",
    "#             prompt_template = patientPrompt\n",
    "        \n",
    "#         prompt = PromptTemplate.from_template(prompt_template)\n",
    "#         llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "#         stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"split_docs\")\n",
    "#         summary = stuff_chain.run(split_docs)\n",
    "#         if not summary.strip().endswith('.'):\n",
    "#             summary += '.'\n",
    "#         summary = summary.replace(\"\\n\", \" \")\n",
    "#         return JSONResponse(content={\n",
    "#             \"summary\": summary\n",
    "#         })\n",
    "#     except Exception as e:\n",
    "#         raise HTTPException(status_code=500, detail=f\"Error during summarizing the documents: {str(e)}\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class UploadLinkRequest(BaseModel):\n",
    "#     tenantId: str\n",
    "#     url: str \n",
    "# @app.post(\"/api/artificial-intelligence/links\")\n",
    "# async def upload_web_url(request: UploadLinkRequest, background_tasks: BackgroundTasks):\n",
    "\n",
    "#     \"\"\"\n",
    "#     Process and store web URL content.\n",
    "    \n",
    "#     Args:\n",
    "#         request: UploadLinkRequest containing tenantId and url.\n",
    "    \n",
    "#     Returns:\n",
    "#         JSON response indicating processing status.\n",
    "    \n",
    "#     Raises:\n",
    "#         HTTPException: When no valid url are provided.\n",
    "#     \"\"\"\n",
    "#     all_docs = []  \n",
    "#     url = request.url.split(\",\")  \n",
    "#     for url in url:\n",
    "#         try:\n",
    "#             loader = WebBaseLoader(url.strip())  \n",
    "#             docs = loader.load()  \n",
    "#             for doc in docs:\n",
    "#                 doc.metadata[\"tenantId\"] = request.tenantId\n",
    "#                 doc.metadata[\"source\"] = url.strip()\n",
    "#             all_docs.extend(docs)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading URL {url}: {e}\")\n",
    "#             continue\n",
    "#     if not all_docs:\n",
    "#         raise HTTPException(status_code=400, detail=\"No valid url provided or failed to fetch content.\")\n",
    "    \n",
    "#     embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "#     vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "#     llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "#     chunk_size = 2000\n",
    "#     chunk_overlap = 500\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "#     split_docs = []\n",
    "#     chunk_ids = []\n",
    "#     uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    \n",
    "#     if not os.path.exists(uploaded_documents_path):\n",
    "#         with open(uploaded_documents_path, \"w\") as f:\n",
    "#             json.dump({}, f)\n",
    "    \n",
    "#     with open(uploaded_documents_path, \"r\") as f:\n",
    "#         upload_documents = json.load(f)\n",
    "    \n",
    "#     document_id =  str(uuid.uuid4())\n",
    "#     for doc in all_docs:\n",
    "#         curr_split_docs = text_splitter.split_documents([doc])\n",
    "#         document_id = document_id\n",
    "#         curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "#         split_docs.extend(curr_split_docs)\n",
    "#         chunk_ids.extend(curr_chunk_ids)\n",
    "#         upload_documents[document_id] = {\"source\": doc.metadata[\"source\"], \"id\": document_id, \"tenantId\": request.tenantId}\n",
    "    \n",
    "#     vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "    \n",
    "#     with open(uploaded_documents_path, \"w\") as f:\n",
    "#         json.dump(upload_documents, f, indent=4)\n",
    "    \n",
    "#     return {\"status\": \"success\", \"message\": \"url processed and stored successfully.\", \"data\": {\"documentId\": document_id}}\n",
    "\n",
    "\n",
    "\n",
    "# import os\n",
    "# import json\n",
    "# import uuid\n",
    "# from fastapi import FastAPI, HTTPException, BackgroundTasks\n",
    "# from pydantic import BaseModel\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "# from langchain_pinecone import PineconeVectorStore\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from PyPDF2 import PdfReader\n",
    "# import pdfkit  \n",
    "# import requests  \n",
    "\n",
    "\n",
    "# class UploadLinkRequest(BaseModel):\n",
    "#     tenantId: str\n",
    "#     url: str \n",
    "\n",
    "# @app.post(\"/api/artificial-intelligence/links\")\n",
    "# async def upload_web_url(request: UploadLinkRequest, background_tasks: BackgroundTasks):\n",
    "#     \"\"\"\n",
    "#     Process and store web URL content.\n",
    "    \n",
    "#     Args:\n",
    "#         request: UploadLinkRequest containing tenantId and url.\n",
    "    \n",
    "#     Returns:\n",
    "#         JSON response indicating processing status.\n",
    "    \n",
    "#     Raises:\n",
    "#         HTTPException: When no valid url are provided.\n",
    "#     \"\"\"\n",
    "#     all_docs = []  \n",
    "#     urls = request.url.split(\",\")  # Split the input into a list of URLs\n",
    "    \n",
    "#     for url in urls:\n",
    "#         url = url.strip()\n",
    "#         try:\n",
    "#             if url.lower().endswith('.pdf'):\n",
    "#                 # If the URL is a PDF, extract text from the PDF\n",
    "#                 response = requests.get(url)\n",
    "#                 pdf_reader = PdfReader(response.content)\n",
    "#                 extracted_text = \"\"\n",
    "#                 for page in pdf_reader.pages:\n",
    "#                     extracted_text += page.extract_text() + \"\\n\"\n",
    "                \n",
    "#                 # Create a Document object with the extracted text\n",
    "#                 doc = Document(page_content=extracted_text, metadata={\"source\": url, \"tenantId\": request.tenantId})\n",
    "#                 all_docs.append(doc)\n",
    "#             else:\n",
    "#                 # If the URL is not a PDF, load it normally\n",
    "#                 loader = WebBaseLoader(url)\n",
    "#                 docs = loader.load()\n",
    "#                 for doc in docs:\n",
    "#                     doc.metadata[\"tenantId\"] = request.tenantId\n",
    "#                     doc.metadata[\"source\"] = url\n",
    "#                     all_docs.append(doc)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading URL {url}: {e}\")\n",
    "#             continue\n",
    "\n",
    "#     if not all_docs:\n",
    "#         raise HTTPException(status_code=400, detail=\"No valid URL provided or failed to fetch content.\")\n",
    "    \n",
    "#     embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "#     vectorstore = PineconeVectorStore(index_name=os.getenv(\"PINECONE_INDEX_NAME\"), embedding=embeddings)\n",
    "#     chunk_size = 800\n",
    "#     chunk_overlap = 150\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "#     split_docs = []\n",
    "#     chunk_ids = []\n",
    "#     uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    \n",
    "#     if not os.path.exists(uploaded_documents_path):\n",
    "#         with open(uploaded_documents_path, \"w\") as f:\n",
    "#             json.dump({}, f)\n",
    "    \n",
    "#     with open(uploaded_documents_path, \"r\") as f:\n",
    "#         upload_documents = json.load(f)\n",
    "    \n",
    "#     document_id = str(uuid.uuid4())\n",
    "#     for doc in all_docs:\n",
    "#         curr_split_docs = text_splitter.split_documents([doc])\n",
    "#         curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "#         split_docs.extend(curr_split_docs)\n",
    "#         chunk_ids.extend(curr_chunk_ids)\n",
    "#         upload_documents[document_id] = {\"source\": doc.metadata[\"source\"], \"id\": document_id, \"tenantId\": request.tenantId}\n",
    "    \n",
    "#     vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "    \n",
    "#     with open(uploaded_documents_path, \"w\") as f:\n",
    "#         json.dump(upload_documents, f, indent=4)\n",
    "    \n",
    "#     return {\"status\": \"success\", \"message\": \"URL processed and stored successfully.\", \"data\": {\"documentId\": document_id}}\n",
    "\n",
    "\n",
    "# class UploadLinkRequest(BaseModel):\n",
    "#     tenantId: str\n",
    "#     url: str \n",
    "# @app.post(\"/api/artificial-intelligence/links\")\n",
    "# async def upload_web_url(request: UploadLinkRequest, background_tasks: BackgroundTasks):\n",
    "\n",
    "#     \"\"\"\n",
    "#     Process and store web URL content.\n",
    "    \n",
    "#     Args:\n",
    "#         request: UploadLinkRequest containing tenantId and url.\n",
    "    \n",
    "#     Returns:\n",
    "#         JSON response indicating processing status.\n",
    "    \n",
    "#     Raises:\n",
    "#         HTTPException: When no valid url are provided.\n",
    "#     \"\"\"\n",
    "#     all_docs = []  \n",
    "#     url = request.url.split(\",\")  \n",
    "#     for url in url:\n",
    "#         try:\n",
    "#             loader = WebBaseLoader(url.strip())  \n",
    "#             docs = loader.load()  \n",
    "#             for doc in docs:\n",
    "#                 doc.metadata[\"tenantId\"] = request.tenantId\n",
    "#                 doc.metadata[\"source\"] = url.strip()\n",
    "#             all_docs.extend(docs)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading URL {url}: {e}\")\n",
    "#             continue\n",
    "#     if not all_docs:\n",
    "#         raise HTTPException(status_code=400, detail=\"No valid url provided or failed to fetch content.\")\n",
    "    \n",
    "#     embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "#     vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "#     llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "#     chunk_size = 300\n",
    "#     chunk_overlap = 50\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "#     split_docs = []\n",
    "#     chunk_ids = []\n",
    "#     uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    \n",
    "#     if not os.path.exists(uploaded_documents_path):\n",
    "#         with open(uploaded_documents_path, \"w\") as f:\n",
    "#             json.dump({}, f)\n",
    "    \n",
    "#     with open(uploaded_documents_path, \"r\") as f:\n",
    "#         upload_documents = json.load(f)\n",
    "    \n",
    "#     document_id =  str(uuid.uuid4())\n",
    "#     for doc in all_docs:\n",
    "#         curr_split_docs = text_splitter.split_documents([doc])\n",
    "#         document_id = document_id\n",
    "#         curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "#         split_docs.extend(curr_split_docs)\n",
    "#         chunk_ids.extend(curr_chunk_ids)\n",
    "#         upload_documents[document_id] = {\"source\": doc.metadata[\"source\"], \"id\": document_id, \"tenantId\": request.tenantId}\n",
    "    \n",
    "#     vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "    \n",
    "#     with open(uploaded_documents_path, \"w\") as f:\n",
    "#         json.dump(upload_documents, f, indent=4)\n",
    "    \n",
    "#     return {\"status\": \"success\", \"message\": \"url processed and stored successfully.\", \"data\": {\"documentId\": document_id}}\n",
    "\n",
    "\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "#from langserve import add_routes\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "from fastapi import UploadFile, File, HTTPException\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from typing import Any\n",
    "import uuid\n",
    "import pylibmagic\n",
    "from fastapi import Query\n",
    "import io\n",
    "import docx\n",
    "import shutil\n",
    "import uvicorn\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import os\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import asyncio\n",
    "from fastapi import FastAPI, HTTPException, BackgroundTasks\n",
    "from pydantic import BaseModel\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from crawl4ai import AsyncWebCrawler \n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.retrievers import PineconeHybridSearchRetriever\n",
    "from fastapi.responses import PlainTextResponse\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import Runnable\n",
    "from pydantic import BaseModel, Field, validator, ValidationError\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from tqdm.auto import tqdm\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv\n",
    "from fastapi.responses import RedirectResponse\n",
    "import glob\n",
    "from fastapi import  UploadFile\n",
    "from fastapi.responses import JSONResponse\n",
    "from uuid import uuid4\n",
    "import time\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi import  Request\n",
    "from fastapi import  BackgroundTasks, UploadFile, Form\n",
    "import getpass\n",
    "import os\n",
    "import concurrent.futures\n",
    "from fastapi import  UploadFile\n",
    "from PyPDF2 import PdfReader  \n",
    "import docx\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " patientPrompt = \"\"\" Write a detailed and structured summary of the following text:\n",
    "        \"{split_docs}\"\n",
    "        These are the conversations that might have happened between the advocates or patients. These notes are not directly sent to me.\n",
    "        Summarize all the notes as if you are addressing me, the patient, and include minor details as well.\n",
    "        Be sure to highlight important notes, medical details, and recommendations.\n",
    "        Include the major events in bullet points if available.\n",
    "\n",
    "        Complete the sentence at the end with a full stop.\n",
    "        \"\"\"\n",
    "\n",
    "        prompt_template = f\"\"\"Write a detailed and structured summary of the following text:\n",
    "        \"{{split_docs}}\"\n",
    "\n",
    "        Please summarize the content above by addressing the following aspects in an integrated narrative format:\n",
    "        - Introduce the patient, age, location, relevant medical history, and current condition.\n",
    "\n",
    "        - No need to mention case ID\n",
    "        - No need to talk about note accessType\n",
    "        \n",
    "        Ensure the summary is:\n",
    "        - **Comprehensive**: Cover all necessary details to serve as a formal record of the patient's care.\n",
    "        - **Well-Organized**: Present the information in a logical flow without using section headers, maintaining clarity and readability.\n",
    "        - **Professional in Tone**: Write in a factual, formal tone suitable for medical or administrative reporting.\n",
    "        - **Clear and Concise**: Use straightforward language that is easy to understand.\n",
    "        - **Structured**: Format the summary in paragraph form for readability, ensuring logical flow between points.\n",
    "\n",
    "        Additionally, please conclude with any recommendations for next steps or ongoing care considerations that were mentioned in the notes.\n",
    "        The tone should be professional yet empathetic, reflecting the seriousness of the patient's condition while acknowledging the efforts of all parties involved in their care.\n",
    "        Conclude the summary with a recap of any critical points or follow-up actions necessary to ensure continuity of care.\n",
    "\n",
    "        Complete the sentence at the end with a full stop.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebaseLoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain_community beautifulsoup4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://www.noveltytechnology.com/about-us\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://www.noveltytechnology.com/about-us', 'title': 'About Us | Novelty Technology', 'description': 'Discover Novelty Technology, a leader in innovative IT solutions. We are dedicated to delivering tailored services that enhance efficiency and empower your business.', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout Us | Novelty Technology\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ntop of page\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSolutionsProcessProjectsAbout UsAbout UsDriven by Excellence,\\nFueled by PassionYour Success, Our PurposeWeâ€™re a group of professionals who thrive on designing and building solutions that make a real impact. Our collective expertise and commitment to excellence ensure that every project we undertake is a perfect blend of creativity, precision, and purpose. Your success is at the heart of everything we do because when you succeed, we succeed.\\xa0Book a Consultation\\n\\n\\n\\n\\nOur PhilosophySimplicityWe design software with a clean, intuitive interface, making complex tasks feel effortless. Simplicity drives efficiency, helping your team work smarter, not harder.SecuritySecurity is non-negotiable in our software. We implement advanced protections to keep your data secure, giving you confidence and peace of mind.ScalibilityOur software scales with your business, adapting to increased demand and evolving needs, ensuring you're always ready for future opportunities.Jon W. Hopkins brings a wealth of expertise in architecting and designing complex, secure, and high-performance software systems. As a recognized leader in the field, Jon co-founded Blueprint Technologies and served as President and CEO of Palladio Software prior to its acquisition by Rational. He has also acted as a fractional CTO for several companies under Archiblox, offering strategic technology leadership. With decades of experience, Jon has consistently delivered solutions that exceed client expectations. He holds both a Masterâ€™s and Bachelorâ€™s degree in Computer Science from the University of Wisconsin-Madison.Jon HopkinsCEOMeet Our TeamOjash ShresthaPresident and ArchitectBryan BottProgram ManagerPrija ShresthaDirector of Finance & AccountKrishna ShresthaManaging Director, NepalRamesh PrajapatiSolutions ArchitectOur Nepal Team\\nEmpowering Innovations Locally & GloballyBook a consultation today to discuss\\nhow we can help you succeed!Book a Consulation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNovelty Technology is a strategic partner in custom software development, offering comprehensive and tailored solutions that drive your success. With a unique blend of creativity and cutting-edge technology, we turn ideas into reality through a seamless, end-to-end process.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCharlotte, NCMilwaukee, WIKathmandu, Nepal\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJon Hopkins\\nOjash Shrestha\\n(866) 666-8832\\n\\n\\n\\n\\n\\ninfo@noveltytechnology.comQuick LinksSolutions\\n\\n\\n\\n\\nProcess\\n\\n\\n\\n\\nProjects\\n\\n\\n\\n\\nAbout Us\\n\\n\\n\\n\\nHome\\n\\n\\n\\n\\nBook Consultation\\n\\n\\n\\n\\nCopyright Â© 2o24. Novelty Technology, LLC. All rights reserved. PrivacyPolicybottom of page\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://www.noveltytechnology.com/about-us', 'title': 'About Us | Novelty Technology', 'description': 'Discover Novelty Technology, a leader in innovative IT solutions. We are dedicated to delivering tailored services that enhance efficiency and empower your business.', 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU  nest_asyncio\n",
    "\n",
    "# fixes a bug with asyncio and jupyter\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 1/1 [00:03<00:00,  3.07s/it]\n",
      "/Users/ujjwal/Library/Python/3.9/lib/python/site-packages/bs4/builder/__init__.py:314: RuntimeWarning: coroutine 'AsyncChromiumLoader.ascrape_playwright' was never awaited\n",
      "  for attr in list(attrs.keys()):\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.noveltytechnology.com/about-us', 'title': 'About Us | Novelty Technology', 'description': 'Discover Novelty Technology, a leader in innovative IT solutions. We are dedicated to delivering tailored services that enhance efficiency and empower your business.', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout Us | Novelty Technology\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ntop of page\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSolutionsProcessProjectsAbout UsAbout UsDriven by Excellence,\\nFueled by PassionYour Success, Our PurposeWeâ€™re a group of professionals who thrive on designing and building solutions that make a real impact. Our collective expertise and commitment to excellence ensure that every project we undertake is a perfect blend of creativity, precision, and purpose. Your success is at the heart of everything we do because when you succeed, we succeed.\\xa0Book a Consultation\\n\\n\\n\\n\\nOur PhilosophySimplicityWe design software with a clean, intuitive interface, making complex tasks feel effortless. Simplicity drives efficiency, helping your team work smarter, not harder.SecuritySecurity is non-negotiable in our software. We implement advanced protections to keep your data secure, giving you confidence and peace of mind.ScalibilityOur software scales with your business, adapting to increased demand and evolving needs, ensuring you're always ready for future opportunities.Jon W. Hopkins brings a wealth of expertise in architecting and designing complex, secure, and high-performance software systems. As a recognized leader in the field, Jon co-founded Blueprint Technologies and served as President and CEO of Palladio Software prior to its acquisition by Rational. He has also acted as a fractional CTO for several companies under Archiblox, offering strategic technology leadership. With decades of experience, Jon has consistently delivered solutions that exceed client expectations. He holds both a Masterâ€™s and Bachelorâ€™s degree in Computer Science from the University of Wisconsin-Madison.Jon HopkinsCEOMeet Our TeamOjash ShresthaPresident and ArchitectBryan BottProgram ManagerPrija ShresthaDirector of Finance & AccountKrishna ShresthaManaging Director, NepalRamesh PrajapatiSolutions ArchitectOur Nepal Team\\nEmpowering Innovations Locally & GloballyBook a consultation today to discuss\\nhow we can help you succeed!Book a Consulation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNovelty Technology is a strategic partner in custom software development, offering comprehensive and tailored solutions that drive your success. With a unique blend of creativity and cutting-edge technology, we turn ideas into reality through a seamless, end-to-end process.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCharlotte, NCMilwaukee, WIKathmandu, Nepal\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJon Hopkins\\nOjash Shrestha\\n(866) 666-8832\\n\\n\\n\\n\\n\\ninfo@noveltytechnology.comQuick LinksSolutions\\n\\n\\n\\n\\nProcess\\n\\n\\n\\n\\nProjects\\n\\n\\n\\n\\nAbout Us\\n\\n\\n\\n\\nHome\\n\\n\\n\\n\\nBook Consultation\\n\\n\\n\\n\\nCopyright Â© 2o24. Novelty Technology, LLC. All rights reserved. PrivacyPolicybottom of page\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://www.noveltytechnology.com/about-us\")\n",
    "loader.requests_per_second = 1\n",
    "docs = loader.aload()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstructured URL Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet unstructured\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ujjwal/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://www.noveltytechnology.com/about-us\",\n",
    "    \"https://www.noveltytechnology.com/solutions\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://www.noveltytechnology.com/about-us'}, page_content=\"About Us\\n\\nDriven by Excellence, Fueled by Passion\\n\\nYour Success, Our Purpose\\n\\nWeâ€™re a group of professionals who thrive on designing and building solutions that make a real impact. Our collective expertise and commitment to excellence ensure that every project we undertake is a perfect blend of creativity, precision, and purpose. Your success is at the heart of everything we do because when you succeed, we succeed.\\n\\nBook a Consultation\\n\\nOur Philosophy\\n\\nSimplicity\\n\\nWe design software with a clean, intuitive interface, making complex tasks feel effortless. Simplicity drives efficiency, helping your team work smarter, not harder.\\n\\nSecurity\\n\\nSecurity is non-negotiable in our software. We implement advanced protections to keep your data secure, giving you confidence and peace of mind.\\n\\nScalibility\\n\\nOur software scales with your business, adapting to increased demand and evolving needs, ensuring you're always ready for future opportunities.\\n\\nJon W. Hopkins brings a wealth of expertise in architecting and designing complex, secure, and high-performance software systems. As a recognized leader in the field, Jon co-founded Blueprint Technologies and served as President and CEO of Palladio Software prior to its acquisition by Rational. He has also acted as a fractional CTO for several companies under Archiblox, offering strategic technology leadership. With decades of experience, Jon has consistently delivered solutions that exceed client expectations. He holds both a Masterâ€™s and Bachelorâ€™s degree in Computer Science from the University of Wisconsin-Madison.\\n\\nJon Hopkins\\n\\nCEO\\n\\nMeet Our Team\\n\\nOjash Shrestha\\n\\nPresident and Architect\\n\\nBryan Bott\\n\\nProgram Manager\\n\\nPrija Shrestha\\n\\nDirector of Finance & Account\\n\\nKrishna Shrestha\\n\\nManaging Director, Nepal\\n\\nRamesh Prajapati\\n\\nSolutions Architect\\n\\nOur Nepal Team Empowering Innovations Locally & Globally\\n\\nBook a consultation today to discuss how we can help you succeed!\\n\\nBook a Consulation\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = UnstructuredURLLoader(urls=urls)\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet selenium unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://www.noveltytechnology.com/about-us', 'title': 'About Us | Novelty Technology', 'description': 'Discover Novelty Technology, a leader in innovative IT solutions. We are dedicated to delivering tailored services that enhance efficiency and empower your business.', 'language': 'en'}, page_content=\"About Us\\n\\nDriven by Excellence, Fueled by Passion\\n\\nYour Success, Our Purpose\\n\\nWeâ€™re a group of professionals who thrive on designing and building solutions that make a real impact. Our collective expertise and commitment to excellence ensure that every project we undertake is a perfect blend of creativity, precision, and purpose. Your success is at the heart of everything we do because when you succeed, we succeed.\\n\\nBook a Consultation\\n\\nOur Philosophy\\n\\nSimplicity\\n\\nWe design software with a clean, intuitive interface, making complex tasks feel effortless. Simplicity drives efficiency, helping your team work smarter, not harder.\\n\\nSecurity\\n\\nSecurity is non-negotiable in our software. We implement advanced protections to keep your data secure, giving you confidence and peace of mind.\\n\\nScalibility\\n\\nOur software scales with your business, adapting to increased demand and evolving needs, ensuring you're always ready for future opportunities.\\n\\nJon W. Hopkins brings a wealth of expertise in architecting and designing complex, secure, and high-performance software systems. As a recognized leader in the field, Jon co-founded Blueprint Technologies and served as President and CEO of Palladio Software prior to its acquisition by Rational. He has also acted as a fractional CTO for several companies under Archiblox, offering strategic technology leadership. With decades of experience, Jon has consistently delivered solutions that exceed client expectations. He holds both a Masterâ€™s and Bachelorâ€™s degree in Computer Science from the University of Wisconsin-Madison.\\n\\nJon Hopkins\\n\\nCEO\\n\\nMeet Our Team\\n\\nOjash Shrestha\\n\\nPresident and Architect\\n\\nBryan Bott\\n\\nProgram Manager\\n\\nPrija Shrestha\\n\\nDirector of Finance & Account\\n\\nKrishna Shrestha\\n\\nManaging Director, Nepal\\n\\nRamesh Prajapati\\n\\nSolutions Architect\\n\\nOur Nepal Team Empowering Innovations Locally & Globally\\n\\nBook a consultation today to discuss how we can help you succeed!\\n\\nBook a Consulation\")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import SeleniumURLLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://www.noveltytechnology.com/about-us\",\n",
    "    \"https://www.noveltytechnology.com/solutions\",\n",
    "]\n",
    "\n",
    "loader = SeleniumURLLoader(urls=urls)\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl4AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UploadLinkRequest(BaseModel):\n",
    "    tenantId: str\n",
    "    url: str \n",
    "\n",
    "@app.post(\"/api/artificial-intelligence/links\")\n",
    "async def upload_web_url(request: UploadLinkRequest, background_tasks: BackgroundTasks):\n",
    "    \"\"\"\n",
    "    Process and store web URL content.\n",
    "    \n",
    "    Args:\n",
    "        request: UploadLinkRequest containing tenantId and url.\n",
    "    \n",
    "    Returns:\n",
    "        JSON response indicating processing status.\n",
    "    \n",
    "    Raises:\n",
    "        HTTPException: When no valid url are provided.\n",
    "    \"\"\"\n",
    "    all_docs = []  \n",
    "    urls = request.url.split(\",\")  \n",
    "\n",
    "    for url in urls:\n",
    "        url = url.strip()\n",
    "        try:\n",
    "            async with AsyncWebCrawler(verbose=True) as crawler:\n",
    "                result = await crawler.arun(url=url)  # Running the crawler on the URL\n",
    "                extracted_content = result.markdown  # Extracted content in markdown format\n",
    "                doc = Document(page_content=extracted_content, metadata={\"source\": url, \"tenantId\": request.tenantId})\n",
    "                all_docs.append(doc)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading URL {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not all_docs:\n",
    "        raise HTTPException(status_code=400, detail=\"No valid URL provided or failed to fetch content.\")\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=os.getenv(\"PINECONE_INDEX_NAME\"), embedding=embeddings)\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "    chunk_size = 3500\n",
    "    chunk_overlap = 1000\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_docs = []\n",
    "    chunk_ids = []\n",
    "    uploaded_documents_path = \"do_not_delete_uploaded_documents.json\"\n",
    "    \n",
    "    if not os.path.exists(uploaded_documents_path):\n",
    "        with open(uploaded_documents_path, \"w\") as f:\n",
    "            json.dump({}, f)\n",
    "    \n",
    "    with open(uploaded_documents_path, \"r\") as f:\n",
    "        upload_documents = json.load(f)\n",
    "    \n",
    "    document_id = str(uuid.uuid4())\n",
    "    for doc in all_docs:\n",
    "        curr_split_docs = text_splitter.split_documents([doc])\n",
    "        curr_chunk_ids = [f\"{document_id}_chunk_{i+1}\" for i in range(len(curr_split_docs))]\n",
    "        split_docs.extend(curr_split_docs)\n",
    "        chunk_ids.extend(curr_chunk_ids)\n",
    "        upload_documents[document_id] = {\"source\": doc.metadata[\"source\"], \"id\": document_id, \"tenantId\": request.tenantId}\n",
    "    \n",
    "    vectorstore.add_documents(documents=split_docs, ids=chunk_ids)\n",
    "    \n",
    "    with open(uploaded_documents_path, \"w\") as f:\n",
    "        json.dump(upload_documents, f, indent=4)\n",
    "    \n",
    "    return {\"status\": \"success\", \"message\": \"URL processed and stored successfully.\", \"data\": {\"documentId\": document_id}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting crawl4ai\n",
      "  Downloading Crawl4AI-0.4.246-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting aiosqlite~=0.20 (from crawl4ai)\n",
      "  Downloading aiosqlite-0.20.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: lxml~=5.3 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from crawl4ai) (5.3.0)\n",
      "Collecting litellm>=1.53.1 (from crawl4ai)\n",
      "  Downloading litellm-1.56.9-py3-none-any.whl.metadata (36 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.26.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from crawl4ai) (1.26.4)\n",
      "Collecting pillow~=10.4 (from crawl4ai)\n",
      "  Downloading pillow-10.4.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: playwright>=1.49.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from crawl4ai) (1.49.1)\n",
      "Requirement already satisfied: python-dotenv~=1.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from crawl4ai) (1.0.1)\n",
      "Requirement already satisfied: requests~=2.26 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from crawl4ai) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4~=4.12 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from crawl4ai) (4.12.3)\n",
      "Collecting tf-playwright-stealth>=1.1.0 (from crawl4ai)\n",
      "  Downloading tf_playwright_stealth-1.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting xxhash~=3.4 (from crawl4ai)\n",
      "  Downloading xxhash-3.5.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting rank-bm25~=0.2 (from crawl4ai)\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from crawl4ai) (24.1.0)\n",
      "Collecting colorama~=0.4 (from crawl4ai)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting snowballstemmer~=2.2 (from crawl4ai)\n",
      "  Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pydantic>=2.10 (from crawl4ai)\n",
      "  Downloading pydantic-2.10.4-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting pyOpenSSL>=24.3.0 (from crawl4ai)\n",
      "  Downloading pyOpenSSL-24.3.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting psutil>=6.1.1 (from crawl4ai)\n",
      "  Downloading psutil-6.1.1-cp36-abi3-macosx_11_0_arm64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: nltk>=3.9.1 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from crawl4ai) (3.9.1)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from aiosqlite~=0.20->crawl4ai) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4~=4.12->crawl4ai) (2.6)\n",
      "Requirement already satisfied: aiohttp in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from litellm>=1.53.1->crawl4ai) (3.9.5)\n",
      "Requirement already satisfied: click in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from litellm>=1.53.1->crawl4ai) (8.1.7)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.23.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from litellm>=1.53.1->crawl4ai) (0.27.2)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from litellm>=1.53.1->crawl4ai) (8.5.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from litellm>=1.53.1->crawl4ai) (3.1.4)\n",
      "Collecting jsonschema<5.0.0,>=4.22.0 (from litellm>=1.53.1->crawl4ai)\n",
      "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting openai>=1.55.3 (from litellm>=1.53.1->crawl4ai)\n",
      "  Downloading openai-1.59.3-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from litellm>=1.53.1->crawl4ai) (0.8.0)\n",
      "Requirement already satisfied: tokenizers in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from litellm>=1.53.1->crawl4ai) (0.20.1)\n",
      "Requirement already satisfied: joblib in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from nltk>=3.9.1->crawl4ai) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from nltk>=3.9.1->crawl4ai) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from nltk>=3.9.1->crawl4ai) (4.66.6)\n",
      "Requirement already satisfied: greenlet==3.1.1 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from playwright>=1.49.0->crawl4ai) (3.1.1)\n",
      "Requirement already satisfied: pyee==12.0.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from playwright>=1.49.0->crawl4ai) (12.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from pydantic>=2.10->crawl4ai) (0.7.0)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic>=2.10->crawl4ai)\n",
      "  Downloading pydantic_core-2.27.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: cryptography<45,>=41.0.5 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from pyOpenSSL>=24.3.0->crawl4ai) (43.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from requests~=2.26->crawl4ai) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from requests~=2.26->crawl4ai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from requests~=2.26->crawl4ai) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from requests~=2.26->crawl4ai) (2024.8.30)\n",
      "Collecting fake-http-header<0.4.0,>=0.3.5 (from tf-playwright-stealth>=1.1.0->crawl4ai)\n",
      "  Downloading fake_http_header-0.3.5-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting pytest-mockito<0.0.5,>=0.0.4 (from tf-playwright-stealth>=1.1.0->crawl4ai)\n",
      "  Downloading pytest-mockito-0.0.4.tar.gz (3.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from cryptography<45,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (1.17.1)\n",
      "Requirement already satisfied: anyio in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=6.8.0->litellm>=1.53.1->crawl4ai) (3.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.53.1->crawl4ai) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (24.2.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai)\n",
      "  Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai)\n",
      "  Downloading referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai)\n",
      "  Downloading rpds_py-0.22.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from openai>=1.55.3->litellm>=1.53.1->crawl4ai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from openai>=1.55.3->litellm>=1.53.1->crawl4ai) (0.6.1)\n",
      "Collecting pytest>=3 (from pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai)\n",
      "  Downloading pytest-8.3.4-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting mockito>=1.0.6 (from pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai)\n",
      "  Downloading mockito-1.5.3-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (4.0.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from tokenizers->litellm>=1.53.1->crawl4ai) (0.26.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (1.2.2)\n",
      "Requirement already satisfied: pycparser in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from cffi>=1.12->cryptography<45,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (2.22)\n",
      "Requirement already satisfied: filelock in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (6.0.2)\n",
      "Collecting iniconfig (from pytest>=3->pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai)\n",
      "  Downloading iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting pluggy<2,>=1.5 (from pytest>=3->pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai)\n",
      "  Downloading pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: tomli>=1 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from pytest>=3->pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai) (2.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/ujjwal/Library/Python/3.9/lib/python/site-packages (from yarl<2.0,>=1.0->aiohttp->litellm>=1.53.1->crawl4ai) (0.2.0)\n",
      "Downloading Crawl4AI-0.4.246-py3-none-any.whl (166 kB)\n",
      "Downloading aiosqlite-0.20.0-py3-none-any.whl (15 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading litellm-1.56.9-py3-none-any.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.4.0-cp39-cp39-macosx_11_0_arm64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading psutil-6.1.1-cp36-abi3-macosx_11_0_arm64.whl (248 kB)\n",
      "Downloading pydantic-2.10.4-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp39-cp39-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyOpenSSL-24.3.0-py3-none-any.whl (56 kB)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "Downloading tf_playwright_stealth-1.1.0-py3-none-any.whl (33 kB)\n",
      "Downloading xxhash-3.5.0-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading fake_http_header-0.3.5-py3-none-any.whl (14 kB)\n",
      "Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Downloading openai-1.59.3-py3-none-any.whl (454 kB)\n",
      "Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Downloading mockito-1.5.3-py3-none-any.whl (30 kB)\n",
      "Downloading pytest-8.3.4-py3-none-any.whl (343 kB)\n",
      "Downloading referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.22.3-cp39-cp39-macosx_11_0_arm64.whl (348 kB)\n",
      "Downloading pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
      "Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Building wheels for collected packages: pytest-mockito\n",
      "  Building wheel for pytest-mockito (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pytest-mockito: filename=pytest_mockito-0.0.4-py3-none-any.whl size=3703 sha256=99d564fc4150974112f8155792c46db269de9ab0a10fcf2b07da7fbf6ae069ad\n",
      "  Stored in directory: /Users/ujjwal/Library/Caches/pip/wheels/37/ef/57/49fa3483387a8257932046830ec1cbfd46744531882bf2f6f6\n",
      "Successfully built pytest-mockito\n",
      "Installing collected packages: snowballstemmer, xxhash, rpds-py, rank-bm25, pydantic-core, psutil, pluggy, pillow, mockito, iniconfig, fake-http-header, colorama, aiosqlite, referencing, pytest, pydantic, pytest-mockito, pyOpenSSL, openai, jsonschema-specifications, tf-playwright-stealth, jsonschema, litellm, crawl4ai\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.23.4\n",
      "    Uninstalling pydantic_core-2.23.4:\n",
      "      Successfully uninstalled pydantic_core-2.23.4\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 6.1.0\n",
      "    Uninstalling psutil-6.1.0:\n",
      "      Successfully uninstalled psutil-6.1.0\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 11.0.0\n",
      "    Uninstalling pillow-11.0.0:\n",
      "      Successfully uninstalled pillow-11.0.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.9.2\n",
      "    Uninstalling pydantic-2.9.2:\n",
      "      Successfully uninstalled pydantic-2.9.2\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.54.4\n",
      "    Uninstalling openai-1.54.4:\n",
      "      Successfully uninstalled openai-1.54.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyaudioprocessing 1.1.13 requires ffmpeg==1.4, which is not installed.\n",
      "pyaudioprocessing 1.1.13 requires pydub==0.25.1, which is not installed.\n",
      "pyaudioprocessing 1.1.13 requires matplotlib==3.3.4, but you have matplotlib 3.9.2 which is incompatible.\n",
      "pyaudioprocessing 1.1.13 requires numpy==1.22.0; python_version >= \"3.8\", but you have numpy 1.26.4 which is incompatible.\n",
      "pyaudioprocessing 1.1.13 requires pytest==6.2.3, but you have pytest 8.3.4 which is incompatible.\n",
      "pyaudioprocessing 1.1.13 requires scikit-learn==1.1.1; python_version >= \"3.8\", but you have scikit-learn 1.5.2 which is incompatible.\n",
      "pyaudioprocessing 1.1.13 requires scipy==1.5.4, but you have scipy 1.13.1 which is incompatible.\n",
      "unstructured-client 0.26.1 requires pydantic<2.10.0,>=2.9.0, but you have pydantic 2.10.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiosqlite-0.20.0 colorama-0.4.6 crawl4ai-0.4.246 fake-http-header-0.3.5 iniconfig-2.0.0 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 litellm-1.56.9 mockito-1.5.3 openai-1.59.3 pillow-10.4.0 pluggy-1.5.0 psutil-6.1.1 pyOpenSSL-24.3.0 pydantic-2.10.4 pydantic-core-2.27.2 pytest-8.3.4 pytest-mockito-0.0.4 rank-bm25-0.2.2 referencing-0.35.1 rpds-py-0.22.3 snowballstemmer-2.2.0 tf-playwright-stealth-1.1.0 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U crawl4ai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ujjwal/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "\u001b[36m[INIT].... â†’ Running post-installation setup...\u001b[0m\n",
      "\u001b[36m[INIT].... â†’ Installing Playwright browsers...\u001b[0m\n",
      "\u001b[32m[COMPLETE] â— Playwright installation completed successfully.\u001b[0m\n",
      "\u001b[36m[INIT].... â†’ Starting database initialization...\u001b[0m\n",
      "\u001b[36m[COMPLETE] â— Database backup created at: /Users/ujjwal/.crawl4ai/crawl4ai.db.backup_20250104_203017\u001b[0m\n",
      "\u001b[36m[INIT].... â†’ Starting database migration...\u001b[0m\n",
      "\u001b[32m[COMPLETE] â— Migration completed. 0 records processed.\u001b[0m\n",
      "\u001b[32m[COMPLETE] â— Database initialization completed successfully.\u001b[0m\n",
      "\u001b[32m[COMPLETE] â— Post-installation setup completed!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!crawl4ai-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ujjwal/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "\u001b[36m[INIT].... â†’ Running Crawl4AI health check...\u001b[0m\n",
      "\u001b[36m[INIT].... â†’ Crawl4AI 0.4.246\u001b[0m\n",
      "\u001b[36m[TEST].... â„¹ Testing crawling capabilities...\u001b[0m\n",
      "\u001b[36m[EXPORT].. â„¹ Exporting PDF and taking screenshot took 0.34s\u001b[0m\n",
      "\u001b[32m[FETCH]... â†“ https://crawl4ai.com... | Status: \u001b[32mTrue\u001b[0m | Time: 5.57s\u001b[0m\n",
      "\u001b[36m[SCRAPE].. â—† Processed https://crawl4ai.com... | Time: 34ms\u001b[0m\n",
      "\u001b[32m[COMPLETE] â— https://crawl4ai.com... | Status: \u001b[32mTrue\u001b[0m | Total: \u001b[33m5.61s\u001b[0m\u001b[0m\n",
      "\u001b[32m[COMPLETE] â— âœ… Crawling test passed!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!crawl4ai-doctor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing event loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... â†’ Crawl4AI 0.4.246\n",
      "[FETCH]... â†“ https://www.noveltytechnology.com/about-us... | Status: True | Time: 1.89s\n",
      "[SCRAPE].. â—† Processed https://www.noveltytechnology.com/about-us... | Time: 44ms\n",
      "[COMPLETE] â— https://www.noveltytechnology.com/about-us... | Status: True | Total: 1.98s\n",
      "top of page\n",
      "[](https://www.noveltytechnology.com/<https:/www.noveltytechnology.com>)\n",
      "  * [Solutions](https://www.noveltytechnology.com/<https:/www.noveltytechnology.com/solutions>)\n",
      "  * [Process](https://www.noveltytechnology.com/<https:/www.noveltytechnology.com/process>)\n",
      "  * [Projects](https://www.noveltytechnology.com/<https:/www.noveltytechnology.com/projects>)\n",
      "  * [About Us](https://www.noveltytechnology.com/<https:/www.noveltytechnology.com/about-us>)\n",
      "\n",
      "\n",
      "About Us\n",
      "## Driven by Excellence, Fueled by Passion\n",
      "### Your Success, Our Purpose\n",
      "Weâ€™re a group of professionals who thrive on designing and building solutions that make a real impact. Our collective expertise and commitment to excellence ensure that every project we undertake is a perfect blend of creativity, precision, and purpose. Your success is at the heart of everything we do because when you succeed, we succeed. \n",
      "[Book a Consultation](https://www.noveltytechnology.com/<https:/calendly.com/jonwhopkins>)\n",
      "## Our Philosophy\n",
      "![Novelty Group Photo.jpg](https://static.wixstatic.com/media/f9bf74_14a267d354774ef7954c28a0d6bd86d0~mv2.jpg/v1/crop/x_0,y_42,w_2048,h_1346/fill/w_656,h_431,al_c,q_80,usm_0.66_1.00_0.01,enc_avif,quality_auto/Novelty%20Group%20Photo.jpg)\n",
      "### Simplicity\n",
      "We design software with a clean, intuitive interface, making complex tasks feel effortless. Simplicity drives efficiency, helping your team work smarter, not harder.\n",
      "### Security\n",
      "Security is non-negotiable in our software. We implement advanced protections to keep your data secure, giving you confidence and peace of mind.\n",
      "### Scalibility\n",
      "Our software scales with your business, adapting to increased demand and evolving needs, ensuring you're always ready for future opportunities.\n",
      "Jon W. Hopkins brings a wealth of expertise in architecting and designing complex, secure, and high-performance software systems. As a recognized leader in the field, Jon co-founded Blueprint Technologies and served as President and CEO of Palladio Software prior to its acquisition by Rational. He has also acted as a fractional CTO for several companies under Archiblox, offering strategic technology leadership. With decades of experience, Jon has consistently delivered solutions that exceed client expectations. He holds both a Masterâ€™s and Bachelorâ€™s degree in Computer Science from the University of Wisconsin-Madison.\n",
      "![JWH No Tie Headshot sm_edited.jpg](https://static.wixstatic.com/media/f9bf74_a5387259430c4f93aeded53e4ec5e753~mv2.jpg/v1/fill/w_82,h_92,al_c,q_80,usm_0.66_1.00_0.01,blur_2,enc_avif,quality_auto/JWH%20No%20Tie%20Headshot%20sm_edited.jpg)\n",
      "###### Jon Hopkins\n",
      "###### CEO\n",
      "# Meet Our Team\n",
      "![Ojash Shreatha.jpg](https://static.wixstatic.com/media/f9bf74_ff34968868484ffb9e88eb2bcde6f3db~mv2.jpg/v1/fill/w_80,h_91,al_c,q_80,usm_0.66_1.00_0.01,blur_2,enc_avif,quality_auto/Ojash%20Shreatha.jpg)\n",
      "###### Ojash Shrestha\n",
      "###### President and Architect\n",
      "![Bryan Bott.jpg](https://static.wixstatic.com/media/f9bf74_7a9990dbe22b42a5a51718b463c52da4~mv2.jpg/v1/fill/w_80,h_90,al_c,q_80,usm_0.66_1.00_0.01,blur_2,enc_avif,quality_auto/Bryan%20Bott.jpg)\n",
      "###### Bryan Bott\n",
      "###### Program Manager\n",
      "![Prija Shrestha_edited_edited_edited.jpg](https://static.wixstatic.com/media/f9bf74_1bf4535171d14f898bcd6c459bdb9e6c~mv2.jpg/v1/fill/w_80,h_89,al_c,q_80,usm_0.66_1.00_0.01,blur_2,enc_avif,quality_auto/Prija%20Shrestha_edited_edited_edited.jpg)\n",
      "###### Prija Shrestha\n",
      "###### Director of Finance & Account\n",
      "![Kirshna Shrestha_edited_edited.jpg](https://static.wixstatic.com/media/f9bf74_67e40368ee6b44b7ac5c921151a2c656~mv2.jpg/v1/fill/w_80,h_88,al_c,q_80,usm_0.66_1.00_0.01,blur_2,enc_avif,quality_auto/Kirshna%20Shrestha_edited_edited.jpg)\n",
      "###### Krishna Shrestha\n",
      "###### Managing Director, Nepal\n",
      "![Ramesh Prajapati .png](https://static.wixstatic.com/media/f9bf74_e798ef80aa3747d0bf5ab93d5e6194e6~mv2.png/v1/fill/w_80,h_90,al_c,q_85,usm_0.66_1.00_0.01,blur_2,enc_avif,quality_auto/Ramesh%20Prajapati%20.png)\n",
      "###### Ramesh Prajapati\n",
      "###### Solutions Architect\n",
      "![](https://static.wixstatic.com/media/f9bf74_a070c042e2bc44a9b98d6932cf85c6a8~mv2.png/v1/fill/w_49,h_20,al_c,q_85,usm_0.66_1.00_0.01,blur_2,enc_avif,quality_auto/f9bf74_a070c042e2bc44a9b98d6932cf85c6a8~mv2.png)\n",
      "## Our Nepal Team Empowering Innovations Locally & Globally\n",
      "#### Book a consultation today to discuss how we can help you succeed!\n",
      "[Book a Consulation](https://www.noveltytechnology.com/<https:/calendly.com/jonwhopkins>)\n",
      "Novelty Technology is a strategic partner in custom software development, offering comprehensive and tailored solutions that drive your success. With a unique blend of creativity and cutting-edge technology, we turn ideas into reality through a seamless, end-to-end process.\n",
      "Charlotte, NC\n",
      "Milwaukee, WI\n",
      "Kathmandu, Nepal\n",
      "Jon Hopkins\n",
      "Ojash Shrestha\n",
      "(866) 666-8832\n",
      "info@noveltytechnology.com\n",
      "###### Quick Links\n",
      "[Solutions](https://www.noveltytechnology.com/<https:/www.noveltytechnology.com/solutions>)\n",
      "[Process](https://www.noveltytechnology.com/<https:/www.noveltytechnology.com/process>)\n",
      "[Projects](https://www.noveltytechnology.com/<https:/www.noveltytechnology.com/projects>)\n",
      "[About Us](https://www.noveltytechnology.com/<https:/www.noveltytechnology.com/about-us>)\n",
      "[Home](https://www.noveltytechnology.com/<https:/www.noveltytechnology.com>)\n",
      "[Book Consultation](https://www.noveltytechnology.com/<https:/calendly.com/jonwhopkins>)\n",
      "Copyright Â© 2o24. Novelty Technology, LLC. All rights reserved. [PrivacyPolicy](https://www.noveltytechnology.com/<https:/www.noveltytechnology.com/privacy-policy>)\n",
      "bottom of page\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "\n",
    "async def main():\n",
    "    # Create an instance of AsyncWebCrawler\n",
    "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
    "        # Run the crawler on a URL\n",
    "        result = await crawler.arun(url=\"https://www.noveltytechnology.com/about-us\")\n",
    "\n",
    "        # Print the extracted content\n",
    "        print(result.markdown)\n",
    "\n",
    "# Check if there's an existing event loop\n",
    "try:\n",
    "    # For Python 3.7 and later\n",
    "    loop = asyncio.get_running_loop()\n",
    "except RuntimeError:\n",
    "    # No event loop is running\n",
    "    loop = None\n",
    "\n",
    "if loop and loop.is_running():\n",
    "    # If an event loop is already running, use it to run the main coroutine\n",
    "    print(\"Using existing event loop.\")\n",
    "    task = loop.create_task(main())\n",
    "else:\n",
    "    # If no event loop is running, create a new one\n",
    "    print(\"Creating a new event loop.\")\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Config",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Agent, Task, Crew\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ScrapeWebsiteTool\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/crewai/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrew\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Crew\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocess\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Process\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/crewai/agent.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01muuid\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, List, Optional\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunnableAgent\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformat_scratchpad\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m format_log_to_str\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConversationSummaryMemory\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/agents/__init__.py:34\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_toolkits\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     35\u001b[0m     create_json_agent,\n\u001b[1;32m     36\u001b[0m     create_openapi_agent,\n\u001b[1;32m     37\u001b[0m     create_pbi_agent,\n\u001b[1;32m     38\u001b[0m     create_pbi_chat_agent,\n\u001b[1;32m     39\u001b[0m     create_spark_sql_agent,\n\u001b[1;32m     40\u001b[0m     create_sql_agent,\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m as_import_path\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     45\u001b[0m     Agent,\n\u001b[1;32m     46\u001b[0m     AgentExecutor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     LLMSingleActionAgent,\n\u001b[1;32m     51\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_community/agent_toolkits/__init__.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Agent toolkits contain integrations with various resources and services.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mLangChain has a large ecosystem of integrations with various external resources\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03mSee [Security](https://python.langchain.com/docs/security) for more information.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mainetwork\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtoolkit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AINetworkToolkit\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamadeus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtoolkit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AmadeusToolkit\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mazure_cognitive_services\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     AzureCognitiveServicesToolkit,\n\u001b[1;32m     20\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_community/agent_toolkits/ainetwork/toolkit.py:7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, List, Literal, Optional\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic_v1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m root_validator\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseToolkit\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseTool\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mainetwork\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AINAppOps\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_community/agent_toolkits/base.py:7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic_v1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseTool\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseToolkit\u001b[39;00m(BaseModel, ABC):\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Base Toolkit representing a collection of related tools.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_community/tools/__init__.py:21\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"**Tools** are classes that an Agent uses to interact with the world.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mEach tool has a **description**. Agent uses the description to choose the right\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    CallbackManagerForToolRun, AsyncCallbackManagerForToolRun\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseTool, StructuredTool, Tool, tool\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Used for internal purposes\u001b[39;00m\n\u001b[1;32m     24\u001b[0m _DEPRECATED_TOOLS \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPythonAstREPLTool\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPythonREPLTool\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/tools.py:113\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optional exception that tool throws when execution error occurs.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    When this exception is thrown, the agent will not stop working,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    to the agent as observation, and printed in red on the console.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseTool\u001b[39;00m(RunnableSerializable[Union[\u001b[38;5;28mstr\u001b[39m, Dict], Any]):\n\u001b[1;32m    114\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Interface LangChain tools must implement.\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init_subclass__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/tools.py:185\u001b[0m, in \u001b[0;36mBaseTool\u001b[0;34m()\u001b[0m\n\u001b[1;32m    180\u001b[0m handle_validation_error: Optional[\n\u001b[1;32m    181\u001b[0m     Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m, Callable[[ValidationError], \u001b[38;5;28mstr\u001b[39m]]\n\u001b[1;32m    182\u001b[0m ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Handle the content of the ValidationError thrown.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mConfig\u001b[39;00m(\u001b[43mSerializable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConfig\u001b[49m):\n\u001b[1;32m    186\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Configuration for this pydantic object.\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     arbitrary_types_allowed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pydantic/_internal/_model_construction.py:262\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    259\u001b[0m original_mro \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mmro()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__bases__\u001b[39m \u001b[38;5;241m==\u001b[39m (\u001b[38;5;28mobject\u001b[39m,):\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_mro\n\u001b[1;32m    264\u001b[0m generic_metadata: PydanticGenericMetadata \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__pydantic_generic_metadata__\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m generic_metadata:\n",
      "\u001b[0;31mAttributeError\u001b[0m: Config"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "from langchain_openai import ChatOpenAI\n",
    "from crewai_tools import ScrapeWebsiteTool\n",
    "import os \n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################# prompts #############################\n",
    "def initializeVectorStore():\n",
    "    \"\"\"Initialize and configure vector store with embeddings\n",
    "    Returns:\n",
    "        Configured PineconeVectorStore instance\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    temperature=0.0\n",
    ")\n",
    "@app.get(\"/api/artificial-intelligence/prompts\")\n",
    "async def prompts_keyword(tenantId: str = Query(...), keyword: str = Query(...)):\n",
    "    \"\"\"Process keyword prompts and retrieve answers using improved RAG chain\n",
    "    Args:\n",
    "        tenantId: Identifier for the tenant\n",
    "        keyword: Search keyword/prompt  \n",
    "    Returns:\n",
    "        PlainText response with answer or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(tenantId, keyword)\n",
    "        vectorstore = initializeVectorStore()\n",
    "        template = \"\"\"Answer the question based only on the following context:\n",
    "        {context}\n",
    "        User Question:\n",
    "        {question}\n",
    "         If the user greets you, respond with one of the following without searching information from knowledge base:\n",
    "        - \"Hello! How can I assist you today?\"\n",
    "        - \"Hi there! What can I do for you?\"\n",
    "        - \"Good day! How can I assist you?\"\n",
    "        - \"Hey! What can I help you with?\"\n",
    "        - \"Hi! How can I support you today?\"\n",
    "        - \"Welcome! How may I help you?\"\n",
    "        - \"Salutations! How can I assist you?\"\n",
    "        - \"What's up? How can I help?\"\n",
    "        - \"Good to see you! How can I assist?\"\n",
    "\n",
    "        If the question asked by user is not exact match to the knowledge base question then say: \"I don't have the information you're looking for, please provide additional details.\" And if it's greetings then give greetings response.    \n",
    "        Only give answer if you're 100 per cent sure. If you're not sure then say: \"I don't have the information you're looking for, please provide additional details.\"      \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\n",
    "                \"k\": 1,\n",
    "                \"filter\": {\n",
    "                    'tenantId': {'$eq': tenantId}\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        rag_chain = (\n",
    "            {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser())\n",
    "        answer = rag_chain.invoke(keyword)\n",
    "        return PlainTextResponse(answer)\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing your request: {str(e)}. Please contact support for assistance.\"\n",
    "        print(f\"Error in prompts_keyword: {str(e)}\") \n",
    "        return PlainTextResponse(error_msg, status_code=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
